{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSdOmADbH2eA",
        "outputId": "972af6cf-260f-4ea2-90df-1178defbbf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install kaggle CLI if not already\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Step 2: Upload the correct kaggle.json file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # ← make sure you upload your real kaggle.json here"
      ],
      "metadata": {
        "id": "TQ1KRMSDH6ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "dL7AwD64H8iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Replace with correct path after using os.listdir()\n",
        "source_path = \"/content/kaggle.json\"\n",
        "target_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "\n",
        "os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "shutil.copy(source_path, target_path)\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "p75c97uzH-55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tanlikesmath/diabetic-retinopathy-resized\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "kSbGzV3kIBTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# === Black Border Remover ===\n",
        "def remove_black_border(img, threshold=5):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not contours:\n",
        "        return img\n",
        "    x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
        "    return img[y:y+h, x:x+w]\n",
        "\n",
        "# === Auto Flip if Left Eye ===\n",
        "def auto_flip_if_left_eye(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    h, w = gray.shape\n",
        "    left = np.mean(gray[:, :w//2])\n",
        "    right = np.mean(gray[:, w//2:])\n",
        "    if right > left:\n",
        "        return cv2.flip(img, 1)  # horizontal flip\n",
        "    return img\n",
        "\n",
        "# === Full Preprocessing Pipeline (Resizing moved to end) ===\n",
        "def preprocess_image_full(img, target_size=(512, 512), blur_thresh=20.0, brightness_thresh=25):\n",
        "    try:\n",
        "        if img is None or len(img.shape) != 3 or img.shape[2] != 3:\n",
        "            return None\n",
        "\n",
        "        # Blur and brightness checks\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        if cv2.Laplacian(gray, cv2.CV_64F).var() < blur_thresh:\n",
        "            return None\n",
        "        if np.mean(gray) < brightness_thresh:\n",
        "            return None\n",
        "\n",
        "        # Remove black borders\n",
        "        img = remove_black_border(img)\n",
        "\n",
        "        # Auto flip if left eye\n",
        "        img = auto_flip_if_left_eye(img)\n",
        "\n",
        "        # CLAHE enhancement\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        l = clahe.apply(l)\n",
        "        img = cv2.cvtColor(cv2.merge((l, a, b)), cv2.COLOR_LAB2BGR)\n",
        "\n",
        "        # Glare detection\n",
        "        glare_mask = cv2.inRange(img, (240, 240, 240), (255, 255, 255))\n",
        "        contours, _ = cv2.findContours(glare_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if any(cv2.contourArea(c) > 500 for c in contours):\n",
        "            return None\n",
        "\n",
        "        # Text/annotation filter\n",
        "        mask_text = cv2.inRange(img, (220, 220, 220), (255, 255, 255))\n",
        "        if cv2.countNonZero(mask_text) > 3000:\n",
        "            return None\n",
        "\n",
        "        # Dark region filter\n",
        "        if np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) > 10) < 0.2:\n",
        "            return None\n",
        "\n",
        "        # ✅ Resize (critical fix)\n",
        "        img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # ✅ Validate shape before return\n",
        "        if img.shape != (512, 512, 3):\n",
        "            raise ValueError(f\"Resized image has invalid shape: {img.shape}\")\n",
        "\n",
        "        # Normalize to [-1, 1]\n",
        "        return img.astype(np.float32) / 127.5 - 1.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Exception during processing: {e}\")\n",
        "        return None\n",
        "\n",
        "# === Set Input/Output Folders ===\n",
        "# Modify the path as needed for your local or Colab environment\n",
        "base_path = \"/content/drive/MyDrive/EECE 490 Project/EyePac\"\n",
        "input_folder = os.path.join(path, \"resized_train_cropped\", \"resized_train_cropped\")\n",
        "output_folder = os.path.join(base_path, \"Cleaned_images\")\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# === Run Preprocessing ===\n",
        "skipped = []\n",
        "image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.jpeg', '.jpg', '.png'))]\n",
        "sample_images = []\n",
        "\n",
        "for fname in tqdm(image_files):\n",
        "    in_path = os.path.join(input_folder, fname)\n",
        "    out_path = os.path.join(output_folder, fname)\n",
        "\n",
        "    img = cv2.imread(in_path)\n",
        "    if img is None:\n",
        "        skipped.append(fname)\n",
        "        continue\n",
        "\n",
        "    processed = preprocess_image_full(img)\n",
        "    if processed is None:\n",
        "        skipped.append(fname)\n",
        "        continue\n",
        "    if processed.shape != (512, 512, 3):\n",
        "        print(f\"⚠️ Unexpected size in {fname}: {processed.shape}\")\n",
        "        skipped.append(fname)\n",
        "        continue\n",
        "\n",
        "    out_img = ((processed + 1.0) * 127.5).astype(np.uint8)\n",
        "    cv2.imwrite(out_path, out_img)\n",
        "\n",
        "    if len(sample_images) < 10 and random.random() < 0.1:\n",
        "        sample_images.append(cv2.cvtColor(out_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# === Show Random Previews ===\n",
        "if sample_images:\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    for i, img in enumerate(sample_images):\n",
        "        plt.subplot(1, len(sample_images), i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ No sample images to display.\")\n",
        "\n",
        "print(f\"\\n✅ Done! Preprocessed images saved to: {output_folder} | Skipped {len(skipped)} bad images.\")\n"
      ],
      "metadata": {
        "id": "u7RNXEGMtJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# === Folder containing images ===\n",
        "folder_path = \"/content/drive/MyDrive/EECE 490 Project/EyePac/Cleaned_images\"\n",
        "\n",
        "# === Loop through all files and check dimensions ===\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"❌ Could not read {filename}\")\n",
        "            continue\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h != 512 or w != 512:\n",
        "            print(f\"⚠️ {filename} has size {w}x{h}\")\n"
      ],
      "metadata": {
        "id": "FU2DhhTm9Ma4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_images = [f for f in os.listdir(output_folder) if f.lower().endswith(('.jpeg', '.jpg', '.png'))]\n",
        "print(f\"🧼 Number of cleaned images: {len(cleaned_images)}\")\n"
      ],
      "metadata": {
        "id": "UbhQFl3HeT07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# === Paths ===\n",
        "path = kagglehub.dataset_download(\"tanlikesmath/diabetic-retinopathy-resized\")\n",
        "labels_path = os.path.join(path, \"trainLabels_cropped.csv\")\n",
        "df = pd.read_csv(labels_path)\n",
        "\n",
        "# === Load CSV and add filename column ===\n",
        "df['filename'] = df['image'].astype(str) + \".jpeg\"\n",
        "\n",
        "# === Get filenames of kept images ===\n",
        "kept_images = set(os.listdir(output_folder))\n",
        "\n",
        "# === Filter CSV based on kept images ===\n",
        "df_filtered = df[df['filename'].isin(kept_images)]\n",
        "\n",
        "# === Count and display label distribution ===\n",
        "label_counts = df_filtered['level'].value_counts().sort_index()\n",
        "count_0 = label_counts.get(0, 0)\n",
        "count_1 = label_counts.get(1, 0)\n",
        "count_2 = label_counts.get(2, 0)\n",
        "count_3 = label_counts.get(3, 0)\n",
        "count_4 = label_counts.get(4, 0)\n",
        "\n",
        "print(f\"✅ Remaining samples:\\n  No_DR (0): {count_0}\\n  DR (1): {count_1}\\n  DR (2): {count_2}\\n  DR (3): {count_3}\\n  DR (4): {count_4}\")\n",
        "\n",
        "# === Save the filtered CSV ===\n",
        "output_csv_path = \"/content/drive/MyDrive/EECE 490 Project/EyePac/trainLabels_filtered.csv\"\n",
        "df_filtered.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"\\n📁 Filtered CSV saved to: {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "cItBPGayL65n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# === Paths ===\n",
        "base_dir = \"/content/drive/MyDrive/EECE 490 Project/EyePac\"\n",
        "input_csv = os.path.join(base_dir, \"trainLabels_filtered.csv\")\n",
        "input_images_folder = os.path.join(base_dir, \"Cleaned_images\")\n",
        "output_csv = os.path.join(base_dir, \"trainLabels_filtered_unbalanced.csv\")\n",
        "output_split_folder = os.path.join(base_dir, \"Unbalanced_Split\")\n",
        "no_dr_folder = os.path.join(output_split_folder, \"NoDR\")\n",
        "dr_folder = os.path.join(output_split_folder, \"DR\")\n",
        "\n",
        "# === Step 1: Load CSV and add binary_level column ===\n",
        "df = pd.read_csv(input_csv)\n",
        "df[\"binary_level\"] = df[\"level\"].apply(lambda x: 0 if x == 0 else 1)\n",
        "\n",
        "# === Step 2: Save new CSV file ===\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"✅ New CSV file saved to: {output_csv}\")\n",
        "\n",
        "# === Step 3: Create output folders ===\n",
        "os.makedirs(no_dr_folder, exist_ok=True)\n",
        "os.makedirs(dr_folder, exist_ok=True)\n",
        "\n",
        "# === Step 4: Copy images based on binary_level ===\n",
        "for _, row in df.iterrows():\n",
        "    fname = row[\"filename\"]\n",
        "    label = row[\"binary_level\"]\n",
        "\n",
        "    src_path = os.path.join(input_images_folder, fname)\n",
        "    dst_path = os.path.join(no_dr_folder if label == 0 else dr_folder, fname)\n",
        "\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(f\"✅ Images split into {no_dr_folder} and {dr_folder} based on binary labels.\")\n"
      ],
      "metadata": {
        "id": "J3oSVLrELvyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# === Paths ===\n",
        "filtered_csv_path = \"/content/drive/MyDrive/EECE 490 Project/EyePac/trainLabels_filtered.csv\"\n",
        "output_folder = \"/content/drive/MyDrive/EECE 490 Project/EyePac/Cleaned_images\"\n",
        "balanced_csv_path = \"/content/drive/MyDrive/EECE 490 Project/EyePac/trainLabels_balanced.csv\"\n",
        "balanced_images_folder = \"/content/drive/MyDrive/EECE 490 Project/EyePac/Balanced_Images\"\n",
        "os.makedirs(balanced_images_folder, exist_ok=True)\n",
        "\n",
        "# === Load filtered CSV ===\n",
        "df_filtered = pd.read_csv(filtered_csv_path)\n",
        "\n",
        "# === Add binary label column ===\n",
        "df_filtered['binary_level'] = df_filtered['level'].apply(lambda x: 0 if x == 0 else 1)\n",
        "\n",
        "# === Balance the dataset ===\n",
        "min_count = df_filtered['binary_level'].value_counts().min()\n",
        "df_balanced = pd.concat([\n",
        "    df_filtered[df_filtered['binary_level'] == 0].sample(min_count, random_state=42),\n",
        "    df_filtered[df_filtered['binary_level'] == 1].sample(min_count, random_state=42)\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# === Save balanced CSV ===\n",
        "df_balanced.to_csv(balanced_csv_path, index=False)\n",
        "print(f\"✅ Balanced CSV saved to: {balanced_csv_path}\")\n",
        "\n",
        "# === Copy images to Balanced_Images folder ===\n",
        "for fname in df_balanced['filename']:\n",
        "    src = os.path.join(output_folder, fname)\n",
        "    dst = os.path.join(balanced_images_folder, fname)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "print(f\"✅ {len(df_balanced)} images copied to: {balanced_images_folder}\")\n"
      ],
      "metadata": {
        "id": "qY0jSa6icrq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Count binary label distribution in balanced dataset ===\n",
        "binary_counts = df_balanced['binary_level'].value_counts().sort_index()\n",
        "count_0 = binary_counts.get(0, 0)\n",
        "count_1 = binary_counts.get(1, 0)\n",
        "\n",
        "print(f\"📊 Class Distribution in Balanced CSV:\\n  No_DR (0): {count_0}\\n  DR (1): {count_1}\")\n"
      ],
      "metadata": {
        "id": "HTmCFAUvgUyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# === Paths ===\n",
        "balanced_csv_path = \"/content/drive/MyDrive/EECE 490 Project/EyePac/trainLabels_balanced.csv\"\n",
        "balanced_images_folder = \"/content/drive/MyDrive/EECE 490 Project/EyePac/Balanced_Images\"\n",
        "split_folder = \"/content/drive/MyDrive/EECE 490 Project/EyePac/Balanced_Split\"\n",
        "dr_folder = os.path.join(split_folder, \"DR\")\n",
        "no_dr_folder = os.path.join(split_folder, \"NoDR\")\n",
        "\n",
        "# === Create target folders ===\n",
        "os.makedirs(dr_folder, exist_ok=True)\n",
        "os.makedirs(no_dr_folder, exist_ok=True)\n",
        "\n",
        "# === Load balanced CSV ===\n",
        "df_balanced = pd.read_csv(balanced_csv_path)\n",
        "\n",
        "# === Copy images to corresponding folders ===\n",
        "for _, row in df_balanced.iterrows():\n",
        "    fname = row['filename']\n",
        "    label = row['binary_level']\n",
        "\n",
        "    src = os.path.join(balanced_images_folder, fname)\n",
        "    dst = os.path.join(dr_folder if label == 1 else no_dr_folder, fname)\n",
        "\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "print(\"✅ Images split into DR and NoDR folders successfully!\")\n"
      ],
      "metadata": {
        "id": "xx20bluqgWMz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}