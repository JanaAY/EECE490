{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9UKygqnGYJy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2EW-XHzBe7A"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Updated paths (binary dataset)\n",
        "train_dir = \"/content/drive/MyDrive/EECE 490 Project/Classification_Set/augmented_final_split_binary/train\"\n",
        "val_dir   = \"/content/drive/MyDrive/EECE 490 Project/Classification_Set/augmented_final_split_binary/val\"\n",
        "test_dir  = \"/content/drive/MyDrive/EECE 490 Project/Classification_Set/augmented_final_split_binary/test\"\n",
        "\n",
        "img_size = (200, 200)\n",
        "batch_size = 32\n",
        "\n",
        "# No augmentation, just rescaling\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\"  # Changed to binary\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\"  # Changed to binary\n",
        ")\n",
        "\n",
        "test_gen = datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",  # Changed to binary\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQKAp1pjUHwU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "def build_advanced_rsg_net(input_shape=(200, 200, 3), num_classes=5):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # === RSG Block 1 ===\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # === RSG Block 2 ===\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # === Global Average Pooling + Dense Layers ===\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # === Output Layer ===\n",
        "    if num_classes == 2:\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "    else:\n",
        "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIsj7KKYCS7F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "\n",
        "# === ENSURE CHECKPOINT DIR EXISTS\n",
        "os.makedirs(\"/content/drive/MyDrive/EECE 490 Project/rsgnet_checkpoints_binary\", exist_ok=True)\n",
        "\n",
        "# === OUTPUT PATHS\n",
        "checkpoint_path = \"/content/drive/MyDrive/EECE 490 Project/rsgnet_checkpoints_binary/best_model.h5\"\n",
        "log_path = \"/content/drive/MyDrive/EECE 490 Project/rsgnet_checkpoints_binary/training_log.csv\"\n",
        "\n",
        "# === CALLBACKS\n",
        "callbacks = [\n",
        "    ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),\n",
        "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1),\n",
        "    CSVLogger(log_path)\n",
        "]\n",
        "\n",
        "# === BUILD + COMPILE (FOR BINARY)\n",
        "model = build_advanced_rsg_net(input_shape=(200, 200, 3), num_classes=2)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',  # ðŸ”¥ Binary Loss\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]  # ðŸ”¥ Binary Metrics\n",
        ")\n",
        "\n",
        "# === TRAIN\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=30,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}