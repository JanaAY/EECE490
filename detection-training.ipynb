{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11551778,"sourceType":"datasetVersion","datasetId":7244043}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/rmaphoh/RETFound_MAE.git\n%cd RETFound_MAE\n","metadata":{"id":"RLXZGGsmUlfg","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T20:49:39.513689Z","iopub.execute_input":"2025-04-24T20:49:39.514075Z","iopub.status.idle":"2025-04-24T20:49:39.649393Z","shell.execute_reply.started":"2025-04-24T20:49:39.514049Z","shell.execute_reply":"2025-04-24T20:49:39.648655Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'RETFound_MAE' already exists and is not an empty directory.\n/kaggle/working/RETFound_MAE\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -r requirements.txt\n","metadata":{"id":"tnc3U5bcXMYH","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T20:49:43.576018Z","iopub.execute_input":"2025-04-24T20:49:43.576332Z","iopub.status.idle":"2025-04-24T20:49:46.758966Z","shell.execute_reply.started":"2025-04-24T20:49:43.576306Z","shell.execute_reply":"2025-04-24T20:49:46.758276Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python~=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (4.9.0.80)\nRequirement already satisfied: Pillow~=10.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (10.2.0)\nRequirement already satisfied: pycm~=4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.3)\nRequirement already satisfied: scikit-learn~=1.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.4.2)\nRequirement already satisfied: timm~=0.9.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.9.16)\nRequirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\nRequirement already satisfied: matplotlib~=3.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.8.4)\nRequirement already satisfied: scikit-multilearn~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.2.0)\nRequirement already satisfied: huggingface-hub~=0.23.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.23.5)\nRequirement already satisfied: tensorboard~=2.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2.17.1)\nRequirement already satisfied: art>=1.8 in /usr/local/lib/python3.11/dist-packages (from pycm~=4.0->-r requirements.txt (line 3)) (6.5)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.4.2->-r requirements.txt (line 4)) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.4.2->-r requirements.txt (line 4)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.4.2->-r requirements.txt (line 4)) (3.6.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm~=0.9.2->-r requirements.txt (line 5)) (2.7.0+cu118)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm~=0.9.2->-r requirements.txt (line 5)) (0.22.0+cu118)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm~=0.9.2->-r requirements.txt (line 5)) (6.0.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm~=0.9.2->-r requirements.txt (line 5)) (0.5.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r requirements.txt (line 7)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r requirements.txt (line 7)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r requirements.txt (line 7)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r requirements.txt (line 7)) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r requirements.txt (line 7)) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r requirements.txt (line 7)) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (2.9.0.post0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (2025.3.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (4.13.1)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.17.0->-r requirements.txt (line 11)) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.17.0->-r requirements.txt (line 11)) (1.70.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.17.0->-r requirements.txt (line 11)) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.17.0->-r requirements.txt (line 11)) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.17.0->-r requirements.txt (line 11)) (75.1.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.17.0->-r requirements.txt (line 11)) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.17.0->-r requirements.txt (line 11)) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.17.0->-r requirements.txt (line 11)) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.17.0->-r requirements.txt (line 11)) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy~=1.26.4->-r requirements.txt (line 7)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy~=1.26.4->-r requirements.txt (line 7)) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy~=1.26.4->-r requirements.txt (line 7)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy~=1.26.4->-r requirements.txt (line 7)) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (2025.1.31)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.8.86)\nRequirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (3.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy~=1.26.4->-r requirements.txt (line 7)) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->timm~=0.9.2->-r requirements.txt (line 5)) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install huggingface_hub\n\n","metadata":{"id":"DLeMF7e6XOjg","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T20:52:17.618044Z","iopub.execute_input":"2025-04-24T20:52:17.618296Z","iopub.status.idle":"2025-04-24T20:52:20.541326Z","shell.execute_reply.started":"2025-04-24T20:52:17.618279Z","shell.execute_reply":"2025-04-24T20:52:20.540478Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.23.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T20:52:23.675081Z","iopub.execute_input":"2025-04-24T20:52:23.675387Z","iopub.status.idle":"2025-04-24T20:52:23.738644Z","shell.execute_reply.started":"2025-04-24T20:52:23.675363Z","shell.execute_reply":"2025-04-24T20:52:23.737921Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"finetune_path = \"/kaggle/working/RETFound_MAE/main_finetune.py\"\n\nwith open(finetune_path, \"r\") as f:\n    lines = f.readlines()\n\nwith open(finetune_path, \"w\") as f:\n    for line in lines:\n        if \"hf_hub_download\" in line and \"repo_id=args.finetune\" in line:\n            # Overwrite line with fixed repo and filename\n            f.write('    checkpoint_path = hf_hub_download(repo_id=\"open-eye/RETFound_MAE\", filename=\"RETFound_cfp.pth\")\\n')\n        else:\n            f.write(line)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T21:14:42.699944Z","iopub.execute_input":"2025-04-24T21:14:42.700792Z","iopub.status.idle":"2025-04-24T21:14:42.706221Z","shell.execute_reply.started":"2025-04-24T21:14:42.700766Z","shell.execute_reply":"2025-04-24T21:14:42.705624Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"!CUDA_VISIBLE_DEVICES=0 python /kaggle/working/RETFound_MAE/main_finetune.py \\\n    --batch_size 16 \\\n    --world_size 1 \\\n    --model RETFound_mae \\\n    --epochs 80 \\\n    --blr 5e-3 \\\n    --layer_decay 0.65 \\\n    --weight_decay 0.05 \\\n    --drop_path 0.2 \\\n    --nb_classes 2 \\\n    --data_path \"/kaggle/input/dataset/RETFound_Split_Augmented\"\\\n    --output_dir \"/kaggle/working/DR_finetune\" \\\n    --input_size 224\n","metadata":{"id":"hkGj2rCmXRjM","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T21:17:02.607271Z","iopub.execute_input":"2025-04-24T21:17:02.607633Z","iopub.status.idle":"2025-04-25T05:39:10.833865Z","shell.execute_reply.started":"2025-04-24T21:17:02.607607Z","shell.execute_reply":"2025-04-25T05:39:10.832793Z"}},"outputs":[{"name":"stdout","text":"2025-04-24 21:17:04.867545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745529424.889324     454 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745529424.895726     454 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nNot using distributed mode\n[21:17:11.242759] job dir: /kaggle/working/RETFound_MAE\n[21:17:11.242826] Namespace(batch_size=16,\nepochs=80,\naccum_iter=1,\nmodel='RETFound_mae',\ninput_size=224,\ndrop_path=0.2,\nclip_grad=None,\nweight_decay=0.05,\nlr=None,\nblr=0.005,\nlayer_decay=0.65,\nmin_lr=1e-06,\nwarmup_epochs=10,\ncolor_jitter=None,\naa='rand-m9-mstd0.5-inc1',\nsmoothing=0.1,\nreprob=0.25,\nremode='pixel',\nrecount=1,\nresplit=False,\nmixup=0,\ncutmix=0,\ncutmix_minmax=None,\nmixup_prob=1.0,\nmixup_switch_prob=0.5,\nmixup_mode='batch',\nfinetune='',\ntask='',\nglobal_pool=True,\ndata_path='/kaggle/input/dataset/RETFound_Split_Augmented',\nnb_classes=2,\noutput_dir='/kaggle/working/DR_finetune',\nlog_dir='./output_logs',\ndevice='cuda',\nseed=0,\nresume='',\nstart_epoch=0,\neval=False,\ndist_eval=False,\nnum_workers=10,\npin_mem=True,\nworld_size=1,\nlocal_rank=-1,\ndist_on_itp=False,\ndist_url='env://',\nsavemodel=True,\nnorm='IMAGENET',\nenhance=False,\ndatasets_seed=2026,\ndistributed=False)\n[21:17:46.405229] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7e6363728d50>\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[21:17:46.407571] len of train_set: 15504\n[21:17:47.024664] number of model params (M): 303.30\n[21:17:47.024730] base lr: 5.00e-03\n[21:17:47.024757] actual lr: 3.13e-04\n[21:17:47.024781] accumulate grad iterations: 1\n[21:17:47.024800] effective batch size: 16\n[21:17:47.028751] criterion = CrossEntropyLoss()\n[21:17:47.028795] Start training for 80 epochs\n[21:17:47.030739] log_dir: ./output_logs/\n[21:17:49.811957] Epoch: [0]  [  0/969]  eta: 0:44:53  lr: 0.000000  loss: 0.7973 (0.7973)  time: 2.7801  data: 1.0180  max mem: 4705\n[21:17:59.489727] Epoch: [0]  [ 20/969]  eta: 0:09:22  lr: 0.000001  loss: 0.7301 (0.7354)  time: 0.4838  data: 0.0002  max mem: 6820\n[21:18:09.475344] Epoch: [0]  [ 40/969]  eta: 0:08:28  lr: 0.000001  loss: 0.7223 (0.7340)  time: 0.4992  data: 0.0002  max mem: 6820\n[21:18:19.736860] Epoch: [0]  [ 60/969]  eta: 0:08:07  lr: 0.000002  loss: 0.7403 (0.7349)  time: 0.5130  data: 0.0002  max mem: 6820\n[21:18:30.201624] Epoch: [0]  [ 80/969]  eta: 0:07:53  lr: 0.000003  loss: 0.7313 (0.7286)  time: 0.5232  data: 0.0002  max mem: 6820\n[21:18:41.027004] Epoch: [0]  [100/969]  eta: 0:07:44  lr: 0.000003  loss: 0.7334 (0.7298)  time: 0.5412  data: 0.0002  max mem: 6820\n[21:18:52.154093] Epoch: [0]  [120/969]  eta: 0:07:36  lr: 0.000004  loss: 0.7319 (0.7312)  time: 0.5563  data: 0.0002  max mem: 6820\n[21:19:03.641124] Epoch: [0]  [140/969]  eta: 0:07:30  lr: 0.000005  loss: 0.7086 (0.7284)  time: 0.5743  data: 0.0002  max mem: 6820\n[21:19:15.579104] Epoch: [0]  [160/969]  eta: 0:07:24  lr: 0.000005  loss: 0.7112 (0.7279)  time: 0.5969  data: 0.0002  max mem: 6820\n[21:19:27.672624] Epoch: [0]  [180/969]  eta: 0:07:18  lr: 0.000006  loss: 0.6932 (0.7256)  time: 0.6046  data: 0.0002  max mem: 6820\n[21:19:39.363388] Epoch: [0]  [200/969]  eta: 0:07:09  lr: 0.000006  loss: 0.7181 (0.7254)  time: 0.5845  data: 0.0002  max mem: 6820\n[21:19:50.709232] Epoch: [0]  [220/969]  eta: 0:06:59  lr: 0.000007  loss: 0.6938 (0.7232)  time: 0.5672  data: 0.0002  max mem: 6820\n[21:20:01.942820] Epoch: [0]  [240/969]  eta: 0:06:48  lr: 0.000008  loss: 0.7097 (0.7225)  time: 0.5616  data: 0.0002  max mem: 6820\n[21:20:13.195428] Epoch: [0]  [260/969]  eta: 0:06:37  lr: 0.000008  loss: 0.6802 (0.7195)  time: 0.5626  data: 0.0002  max mem: 6820\n[21:20:24.570367] Epoch: [0]  [280/969]  eta: 0:06:26  lr: 0.000009  loss: 0.6979 (0.7187)  time: 0.5687  data: 0.0002  max mem: 6820\n[21:20:36.108101] Epoch: [0]  [300/969]  eta: 0:06:15  lr: 0.000010  loss: 0.7015 (0.7184)  time: 0.5768  data: 0.0002  max mem: 6820\n[21:20:47.769916] Epoch: [0]  [320/969]  eta: 0:06:05  lr: 0.000010  loss: 0.6797 (0.7172)  time: 0.5830  data: 0.0002  max mem: 6820\n[21:20:59.441524] Epoch: [0]  [340/969]  eta: 0:05:54  lr: 0.000011  loss: 0.6925 (0.7164)  time: 0.5835  data: 0.0002  max mem: 6820\n[21:21:11.029737] Epoch: [0]  [360/969]  eta: 0:05:44  lr: 0.000012  loss: 0.7066 (0.7168)  time: 0.5794  data: 0.0002  max mem: 6820\n[21:21:22.520442] Epoch: [0]  [380/969]  eta: 0:05:33  lr: 0.000012  loss: 0.7034 (0.7173)  time: 0.5745  data: 0.0002  max mem: 6820\n[21:21:33.969825] Epoch: [0]  [400/969]  eta: 0:05:21  lr: 0.000013  loss: 0.7295 (0.7174)  time: 0.5724  data: 0.0002  max mem: 6820\n[21:21:45.413765] Epoch: [0]  [420/969]  eta: 0:05:10  lr: 0.000014  loss: 0.7138 (0.7176)  time: 0.5722  data: 0.0002  max mem: 6820\n[21:21:56.864111] Epoch: [0]  [440/969]  eta: 0:04:59  lr: 0.000014  loss: 0.7079 (0.7179)  time: 0.5725  data: 0.0002  max mem: 6820\n[21:22:08.343827] Epoch: [0]  [460/969]  eta: 0:04:48  lr: 0.000015  loss: 0.6940 (0.7181)  time: 0.5739  data: 0.0002  max mem: 6820\n[21:22:19.838545] Epoch: [0]  [480/969]  eta: 0:04:37  lr: 0.000015  loss: 0.6999 (0.7173)  time: 0.5747  data: 0.0002  max mem: 6820\n[21:22:31.357053] Epoch: [0]  [500/969]  eta: 0:04:26  lr: 0.000016  loss: 0.6801 (0.7163)  time: 0.5759  data: 0.0002  max mem: 6820\n[21:22:42.894272] Epoch: [0]  [520/969]  eta: 0:04:14  lr: 0.000017  loss: 0.7136 (0.7162)  time: 0.5768  data: 0.0002  max mem: 6820\n[21:22:54.419301] Epoch: [0]  [540/969]  eta: 0:04:03  lr: 0.000017  loss: 0.7212 (0.7164)  time: 0.5762  data: 0.0002  max mem: 6820\n[21:23:05.971404] Epoch: [0]  [560/969]  eta: 0:03:52  lr: 0.000018  loss: 0.6772 (0.7151)  time: 0.5776  data: 0.0002  max mem: 6820\n[21:23:17.515342] Epoch: [0]  [580/969]  eta: 0:03:41  lr: 0.000019  loss: 0.7277 (0.7156)  time: 0.5771  data: 0.0002  max mem: 6820\n[21:23:29.075359] Epoch: [0]  [600/969]  eta: 0:03:29  lr: 0.000019  loss: 0.7074 (0.7160)  time: 0.5780  data: 0.0002  max mem: 6820\n[21:23:40.634382] Epoch: [0]  [620/969]  eta: 0:03:18  lr: 0.000020  loss: 0.7309 (0.7166)  time: 0.5779  data: 0.0002  max mem: 6820\n[21:23:52.198171] Epoch: [0]  [640/969]  eta: 0:03:07  lr: 0.000021  loss: 0.7014 (0.7167)  time: 0.5781  data: 0.0002  max mem: 6820\n[21:24:03.734934] Epoch: [0]  [660/969]  eta: 0:02:56  lr: 0.000021  loss: 0.6930 (0.7161)  time: 0.5768  data: 0.0002  max mem: 6820\n[21:24:15.270788] Epoch: [0]  [680/969]  eta: 0:02:44  lr: 0.000022  loss: 0.6563 (0.7156)  time: 0.5767  data: 0.0002  max mem: 6820\n[21:24:26.799141] Epoch: [0]  [700/969]  eta: 0:02:33  lr: 0.000023  loss: 0.7258 (0.7158)  time: 0.5764  data: 0.0002  max mem: 6820\n[21:24:38.325219] Epoch: [0]  [720/969]  eta: 0:02:22  lr: 0.000023  loss: 0.6913 (0.7158)  time: 0.5763  data: 0.0002  max mem: 6820\n[21:24:49.846662] Epoch: [0]  [740/969]  eta: 0:02:10  lr: 0.000024  loss: 0.7052 (0.7156)  time: 0.5760  data: 0.0002  max mem: 6820\n[21:25:01.341207] Epoch: [0]  [760/969]  eta: 0:01:59  lr: 0.000025  loss: 0.7425 (0.7160)  time: 0.5747  data: 0.0002  max mem: 6820\n[21:25:12.815762] Epoch: [0]  [780/969]  eta: 0:01:47  lr: 0.000025  loss: 0.6715 (0.7152)  time: 0.5737  data: 0.0002  max mem: 6820\n[21:25:24.296893] Epoch: [0]  [800/969]  eta: 0:01:36  lr: 0.000026  loss: 0.7057 (0.7151)  time: 0.5740  data: 0.0002  max mem: 6820\n[21:25:35.775110] Epoch: [0]  [820/969]  eta: 0:01:25  lr: 0.000026  loss: 0.6911 (0.7149)  time: 0.5739  data: 0.0002  max mem: 6820\n[21:25:47.246493] Epoch: [0]  [840/969]  eta: 0:01:13  lr: 0.000027  loss: 0.6824 (0.7147)  time: 0.5735  data: 0.0002  max mem: 6820\n[21:25:58.705468] Epoch: [0]  [860/969]  eta: 0:01:02  lr: 0.000028  loss: 0.6935 (0.7146)  time: 0.5729  data: 0.0002  max mem: 6820\n[21:26:10.185524] Epoch: [0]  [880/969]  eta: 0:00:50  lr: 0.000028  loss: 0.7048 (0.7143)  time: 0.5740  data: 0.0002  max mem: 6820\n[21:26:21.646509] Epoch: [0]  [900/969]  eta: 0:00:39  lr: 0.000029  loss: 0.7387 (0.7148)  time: 0.5730  data: 0.0002  max mem: 6820\n[21:26:33.140349] Epoch: [0]  [920/969]  eta: 0:00:27  lr: 0.000030  loss: 0.7354 (0.7155)  time: 0.5746  data: 0.0002  max mem: 6820\n[21:26:44.646069] Epoch: [0]  [940/969]  eta: 0:00:16  lr: 0.000030  loss: 0.6989 (0.7155)  time: 0.5752  data: 0.0002  max mem: 6820\n[21:26:56.144583] Epoch: [0]  [960/969]  eta: 0:00:05  lr: 0.000031  loss: 0.7301 (0.7163)  time: 0.5749  data: 0.0002  max mem: 6820\n[21:27:00.742148] Epoch: [0]  [968/969]  eta: 0:00:00  lr: 0.000031  loss: 0.7128 (0.7163)  time: 0.5751  data: 0.0002  max mem: 6820\n[21:27:00.847445] Epoch: [0] Total time: 0:09:13 (0.5715 s / it)\n[21:27:00.847541] Averaged stats: lr: 0.000031  loss: 0.7128 (0.7163)\n[21:27:01.733106] val:  [  0/139]  eta: 0:02:02  loss: 0.7295 (0.7295)  time: 0.8814  data: 0.7321  max mem: 6820\n[21:27:03.238139] val:  [ 10/139]  eta: 0:00:27  loss: 0.7129 (0.7137)  time: 0.2169  data: 0.0668  max mem: 6820\n[21:27:04.741154] val:  [ 20/139]  eta: 0:00:22  loss: 0.6467 (0.7074)  time: 0.1503  data: 0.0002  max mem: 6820\n[21:27:06.253286] val:  [ 30/139]  eta: 0:00:18  loss: 0.6525 (0.7086)  time: 0.1507  data: 0.0002  max mem: 6820\n[21:27:07.764745] val:  [ 40/139]  eta: 0:00:16  loss: 0.7435 (0.7296)  time: 0.1511  data: 0.0002  max mem: 6820\n[21:27:09.276748] val:  [ 50/139]  eta: 0:00:14  loss: 0.7922 (0.7408)  time: 0.1511  data: 0.0002  max mem: 6820\n[21:27:10.779523] val:  [ 60/139]  eta: 0:00:12  loss: 0.7922 (0.7494)  time: 0.1507  data: 0.0002  max mem: 6820\n[21:27:12.290903] val:  [ 70/139]  eta: 0:00:11  loss: 0.8056 (0.7551)  time: 0.1506  data: 0.0002  max mem: 6820\n[21:27:13.803813] val:  [ 80/139]  eta: 0:00:09  loss: 0.7336 (0.7465)  time: 0.1511  data: 0.0002  max mem: 6820\n[21:27:15.318497] val:  [ 90/139]  eta: 0:00:07  loss: 0.6466 (0.7340)  time: 0.1513  data: 0.0002  max mem: 6820\n[21:27:16.840792] val:  [100/139]  eta: 0:00:06  loss: 0.6076 (0.7210)  time: 0.1518  data: 0.0002  max mem: 6820\n[21:27:18.358159] val:  [110/139]  eta: 0:00:04  loss: 0.5857 (0.7090)  time: 0.1519  data: 0.0002  max mem: 6820\n[21:27:19.867857] val:  [120/139]  eta: 0:00:02  loss: 0.5805 (0.6994)  time: 0.1513  data: 0.0002  max mem: 6820\n[21:27:21.387285] val:  [130/139]  eta: 0:00:01  loss: 0.5944 (0.6935)  time: 0.1514  data: 0.0002  max mem: 6820\n[21:27:22.652252] val:  [138/139]  eta: 0:00:00  loss: 0.5923 (0.6866)  time: 0.1542  data: 0.0001  max mem: 6820\n[21:27:22.748888] val: Total time: 0:00:21 (0.1575 s / it)\n[21:27:22.824831] val loss: 0.6866474837707959\n[21:27:22.824901] Accuracy: 0.5660, F1 Score: 0.5640, ROC AUC: 0.5933, Hamming Loss: 0.4340,\n Jaccard Score: 0.3933, Precision: 0.5672, Recall: 0.5660,\n Average Precision: 0.5822, Kappa: 0.1320, Score: 0.4298\n[21:27:24.284415] Best epoch = 0, Best score = 0.4298\n[21:27:24.287290] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[21:27:25.876732] Epoch: [1]  [  0/969]  eta: 0:25:38  lr: 0.000031  loss: 0.6237 (0.6237)  time: 1.5882  data: 0.8228  max mem: 6820\n[21:27:37.332140] Epoch: [1]  [ 20/969]  eta: 0:09:49  lr: 0.000032  loss: 0.6822 (0.6842)  time: 0.5727  data: 0.0002  max mem: 6820\n[21:27:48.868000] Epoch: [1]  [ 40/969]  eta: 0:09:16  lr: 0.000033  loss: 0.7371 (0.7112)  time: 0.5767  data: 0.0002  max mem: 6820\n[21:28:00.448330] Epoch: [1]  [ 60/969]  eta: 0:08:58  lr: 0.000033  loss: 0.6897 (0.7068)  time: 0.5790  data: 0.0002  max mem: 6820\n[21:28:12.014214] Epoch: [1]  [ 80/969]  eta: 0:08:43  lr: 0.000034  loss: 0.6703 (0.7060)  time: 0.5782  data: 0.0002  max mem: 6820\n[21:28:23.517821] Epoch: [1]  [100/969]  eta: 0:08:29  lr: 0.000034  loss: 0.6754 (0.7018)  time: 0.5751  data: 0.0002  max mem: 6820\n[21:28:35.036891] Epoch: [1]  [120/969]  eta: 0:08:16  lr: 0.000035  loss: 0.7117 (0.7075)  time: 0.5759  data: 0.0002  max mem: 6820\n[21:28:46.536167] Epoch: [1]  [140/969]  eta: 0:08:03  lr: 0.000036  loss: 0.7348 (0.7110)  time: 0.5749  data: 0.0003  max mem: 6820\n[21:28:58.044112] Epoch: [1]  [160/969]  eta: 0:07:51  lr: 0.000036  loss: 0.6786 (0.7093)  time: 0.5753  data: 0.0002  max mem: 6820\n[21:29:09.525456] Epoch: [1]  [180/969]  eta: 0:07:38  lr: 0.000037  loss: 0.6797 (0.7086)  time: 0.5740  data: 0.0002  max mem: 6820\n[21:29:21.005833] Epoch: [1]  [200/969]  eta: 0:07:26  lr: 0.000038  loss: 0.7252 (0.7106)  time: 0.5740  data: 0.0002  max mem: 6820\n[21:29:32.451829] Epoch: [1]  [220/969]  eta: 0:07:14  lr: 0.000038  loss: 0.6885 (0.7106)  time: 0.5723  data: 0.0002  max mem: 6820\n[21:29:43.930263] Epoch: [1]  [240/969]  eta: 0:07:02  lr: 0.000039  loss: 0.7202 (0.7115)  time: 0.5739  data: 0.0002  max mem: 6820\n[21:29:55.389108] Epoch: [1]  [260/969]  eta: 0:06:50  lr: 0.000040  loss: 0.6685 (0.7110)  time: 0.5729  data: 0.0002  max mem: 6820\n[21:30:06.862146] Epoch: [1]  [280/969]  eta: 0:06:38  lr: 0.000040  loss: 0.7246 (0.7128)  time: 0.5736  data: 0.0002  max mem: 6820\n[21:30:18.329562] Epoch: [1]  [300/969]  eta: 0:06:26  lr: 0.000041  loss: 0.7340 (0.7144)  time: 0.5733  data: 0.0002  max mem: 6820\n[21:30:29.803214] Epoch: [1]  [320/969]  eta: 0:06:15  lr: 0.000042  loss: 0.7142 (0.7148)  time: 0.5736  data: 0.0002  max mem: 6820\n[21:30:41.268430] Epoch: [1]  [340/969]  eta: 0:06:03  lr: 0.000042  loss: 0.7105 (0.7150)  time: 0.5732  data: 0.0002  max mem: 6820\n[21:30:52.746002] Epoch: [1]  [360/969]  eta: 0:05:51  lr: 0.000043  loss: 0.6766 (0.7139)  time: 0.5738  data: 0.0002  max mem: 6820\n[21:31:04.225076] Epoch: [1]  [380/969]  eta: 0:05:39  lr: 0.000044  loss: 0.7000 (0.7148)  time: 0.5739  data: 0.0002  max mem: 6820\n[21:31:15.715329] Epoch: [1]  [400/969]  eta: 0:05:28  lr: 0.000044  loss: 0.7127 (0.7144)  time: 0.5745  data: 0.0002  max mem: 6820\n[21:31:27.188740] Epoch: [1]  [420/969]  eta: 0:05:16  lr: 0.000045  loss: 0.7052 (0.7143)  time: 0.5736  data: 0.0002  max mem: 6820\n[21:31:38.639593] Epoch: [1]  [440/969]  eta: 0:05:05  lr: 0.000045  loss: 0.7083 (0.7141)  time: 0.5725  data: 0.0002  max mem: 6820\n[21:31:50.101422] Epoch: [1]  [460/969]  eta: 0:04:53  lr: 0.000046  loss: 0.6810 (0.7134)  time: 0.5730  data: 0.0002  max mem: 6820\n[21:32:01.570192] Epoch: [1]  [480/969]  eta: 0:04:41  lr: 0.000047  loss: 0.7150 (0.7139)  time: 0.5734  data: 0.0002  max mem: 6820\n[21:32:13.046268] Epoch: [1]  [500/969]  eta: 0:04:30  lr: 0.000047  loss: 0.7160 (0.7141)  time: 0.5737  data: 0.0002  max mem: 6820\n[21:32:24.518719] Epoch: [1]  [520/969]  eta: 0:04:18  lr: 0.000048  loss: 0.7131 (0.7147)  time: 0.5736  data: 0.0002  max mem: 6820\n[21:32:35.990901] Epoch: [1]  [540/969]  eta: 0:04:07  lr: 0.000049  loss: 0.6555 (0.7132)  time: 0.5736  data: 0.0002  max mem: 6820\n[21:32:47.481549] Epoch: [1]  [560/969]  eta: 0:03:55  lr: 0.000049  loss: 0.6782 (0.7123)  time: 0.5745  data: 0.0002  max mem: 6820\n[21:32:58.977532] Epoch: [1]  [580/969]  eta: 0:03:44  lr: 0.000050  loss: 0.7150 (0.7126)  time: 0.5747  data: 0.0002  max mem: 6820\n[21:33:10.466807] Epoch: [1]  [600/969]  eta: 0:03:32  lr: 0.000051  loss: 0.7117 (0.7125)  time: 0.5744  data: 0.0002  max mem: 6820\n[21:33:21.866013] Epoch: [1]  [620/969]  eta: 0:03:20  lr: 0.000051  loss: 0.7167 (0.7123)  time: 0.5699  data: 0.0002  max mem: 6820\n[21:33:33.362152] Epoch: [1]  [640/969]  eta: 0:03:09  lr: 0.000052  loss: 0.7119 (0.7127)  time: 0.5748  data: 0.0002  max mem: 6820\n[21:33:44.853273] Epoch: [1]  [660/969]  eta: 0:02:57  lr: 0.000053  loss: 0.6850 (0.7120)  time: 0.5745  data: 0.0002  max mem: 6820\n[21:33:56.340440] Epoch: [1]  [680/969]  eta: 0:02:46  lr: 0.000053  loss: 0.7098 (0.7122)  time: 0.5743  data: 0.0002  max mem: 6820\n[21:34:07.833809] Epoch: [1]  [700/969]  eta: 0:02:34  lr: 0.000054  loss: 0.7076 (0.7128)  time: 0.5746  data: 0.0002  max mem: 6820\n[21:34:19.311404] Epoch: [1]  [720/969]  eta: 0:02:23  lr: 0.000054  loss: 0.7285 (0.7132)  time: 0.5738  data: 0.0002  max mem: 6820\n[21:34:30.824875] Epoch: [1]  [740/969]  eta: 0:02:11  lr: 0.000055  loss: 0.6950 (0.7131)  time: 0.5756  data: 0.0002  max mem: 6820\n[21:34:42.325481] Epoch: [1]  [760/969]  eta: 0:02:00  lr: 0.000056  loss: 0.7052 (0.7128)  time: 0.5750  data: 0.0002  max mem: 6820\n[21:34:53.837654] Epoch: [1]  [780/969]  eta: 0:01:48  lr: 0.000056  loss: 0.6745 (0.7121)  time: 0.5756  data: 0.0002  max mem: 6820\n[21:35:05.346640] Epoch: [1]  [800/969]  eta: 0:01:37  lr: 0.000057  loss: 0.7182 (0.7123)  time: 0.5754  data: 0.0002  max mem: 6820\n[21:35:16.857047] Epoch: [1]  [820/969]  eta: 0:01:25  lr: 0.000058  loss: 0.6794 (0.7120)  time: 0.5755  data: 0.0002  max mem: 6820\n[21:35:28.378576] Epoch: [1]  [840/969]  eta: 0:01:14  lr: 0.000058  loss: 0.7066 (0.7118)  time: 0.5760  data: 0.0002  max mem: 6820\n[21:35:39.890411] Epoch: [1]  [860/969]  eta: 0:01:02  lr: 0.000059  loss: 0.7102 (0.7118)  time: 0.5755  data: 0.0002  max mem: 6820\n[21:35:51.418515] Epoch: [1]  [880/969]  eta: 0:00:51  lr: 0.000060  loss: 0.6736 (0.7112)  time: 0.5764  data: 0.0002  max mem: 6820\n[21:36:02.953439] Epoch: [1]  [900/969]  eta: 0:00:39  lr: 0.000060  loss: 0.7197 (0.7111)  time: 0.5767  data: 0.0002  max mem: 6820\n[21:36:14.476855] Epoch: [1]  [920/969]  eta: 0:00:28  lr: 0.000061  loss: 0.6995 (0.7110)  time: 0.5761  data: 0.0002  max mem: 6820\n[21:36:25.998903] Epoch: [1]  [940/969]  eta: 0:00:16  lr: 0.000062  loss: 0.6808 (0.7106)  time: 0.5761  data: 0.0002  max mem: 6820\n[21:36:37.510257] Epoch: [1]  [960/969]  eta: 0:00:05  lr: 0.000062  loss: 0.7066 (0.7105)  time: 0.5755  data: 0.0002  max mem: 6820\n[21:36:42.111919] Epoch: [1]  [968/969]  eta: 0:00:00  lr: 0.000062  loss: 0.6906 (0.7103)  time: 0.5753  data: 0.0001  max mem: 6820\n[21:36:42.219575] Epoch: [1] Total time: 0:09:17 (0.5758 s / it)\n[21:36:42.219693] Averaged stats: lr: 0.000062  loss: 0.6906 (0.7103)\n[21:36:43.027417] val:  [  0/139]  eta: 0:01:51  loss: 0.6272 (0.6272)  time: 0.8029  data: 0.6386  max mem: 6820\n[21:36:44.521401] val:  [ 10/139]  eta: 0:00:26  loss: 0.5944 (0.5894)  time: 0.2086  data: 0.0582  max mem: 6820\n[21:36:46.028464] val:  [ 20/139]  eta: 0:00:21  loss: 0.5715 (0.5851)  time: 0.1499  data: 0.0002  max mem: 6820\n[21:36:47.541051] val:  [ 30/139]  eta: 0:00:18  loss: 0.5776 (0.5885)  time: 0.1509  data: 0.0002  max mem: 6820\n[21:36:49.057682] val:  [ 40/139]  eta: 0:00:16  loss: 0.6081 (0.5969)  time: 0.1514  data: 0.0002  max mem: 6820\n[21:36:50.578323] val:  [ 50/139]  eta: 0:00:14  loss: 0.6143 (0.6037)  time: 0.1518  data: 0.0002  max mem: 6820\n[21:36:52.098085] val:  [ 60/139]  eta: 0:00:12  loss: 0.6530 (0.6145)  time: 0.1519  data: 0.0002  max mem: 6820\n[21:36:53.617398] val:  [ 70/139]  eta: 0:00:11  loss: 0.6636 (0.6249)  time: 0.1519  data: 0.0002  max mem: 6820\n[21:36:55.131716] val:  [ 80/139]  eta: 0:00:09  loss: 0.7328 (0.6408)  time: 0.1516  data: 0.0002  max mem: 6820\n[21:36:56.650773] val:  [ 90/139]  eta: 0:00:07  loss: 0.7319 (0.6488)  time: 0.1516  data: 0.0002  max mem: 6820\n[21:36:58.173818] val:  [100/139]  eta: 0:00:06  loss: 0.7173 (0.6560)  time: 0.1520  data: 0.0002  max mem: 6820\n[21:36:59.693920] val:  [110/139]  eta: 0:00:04  loss: 0.7303 (0.6635)  time: 0.1521  data: 0.0002  max mem: 6820\n[21:37:01.205115] val:  [120/139]  eta: 0:00:02  loss: 0.7500 (0.6705)  time: 0.1515  data: 0.0002  max mem: 6820\n[21:37:02.724607] val:  [130/139]  eta: 0:00:01  loss: 0.7593 (0.6779)  time: 0.1515  data: 0.0002  max mem: 6820\n[21:37:03.843084] val:  [138/139]  eta: 0:00:00  loss: 0.7717 (0.6839)  time: 0.1469  data: 0.0001  max mem: 6820\n[21:37:03.941873] val: Total time: 0:00:21 (0.1562 s / it)\n[21:37:03.999292] val loss: 0.6838550155968974\n[21:37:03.999342] Accuracy: 0.5457, F1 Score: 0.5367, ROC AUC: 0.5829, Hamming Loss: 0.4543,\n Jaccard Score: 0.3695, Precision: 0.5495, Recall: 0.5457,\n Average Precision: 0.5786, Kappa: 0.0913, Score: 0.4037\n[21:37:04.001154] Best epoch = 0, Best score = 0.4298\n[21:37:04.003647] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[21:37:05.365310] Epoch: [2]  [  0/969]  eta: 0:21:58  lr: 0.000063  loss: 0.6180 (0.6180)  time: 1.3603  data: 0.7586  max mem: 6820\n[21:37:16.817838] Epoch: [2]  [ 20/969]  eta: 0:09:38  lr: 0.000063  loss: 0.7165 (0.7129)  time: 0.5726  data: 0.0002  max mem: 6820\n[21:37:28.292189] Epoch: [2]  [ 40/969]  eta: 0:09:10  lr: 0.000064  loss: 0.6692 (0.7071)  time: 0.5737  data: 0.0002  max mem: 6820\n[21:37:39.792710] Epoch: [2]  [ 60/969]  eta: 0:08:53  lr: 0.000064  loss: 0.6956 (0.7032)  time: 0.5750  data: 0.0003  max mem: 6820\n[21:37:51.304660] Epoch: [2]  [ 80/969]  eta: 0:08:39  lr: 0.000065  loss: 0.7031 (0.7051)  time: 0.5755  data: 0.0002  max mem: 6820\n[21:38:02.799714] Epoch: [2]  [100/969]  eta: 0:08:25  lr: 0.000066  loss: 0.7083 (0.7069)  time: 0.5747  data: 0.0002  max mem: 6820\n[21:38:14.321157] Epoch: [2]  [120/969]  eta: 0:08:13  lr: 0.000066  loss: 0.6815 (0.7025)  time: 0.5760  data: 0.0002  max mem: 6820\n[21:38:25.827170] Epoch: [2]  [140/969]  eta: 0:08:01  lr: 0.000067  loss: 0.7009 (0.7021)  time: 0.5752  data: 0.0002  max mem: 6820\n[21:38:37.334589] Epoch: [2]  [160/969]  eta: 0:07:48  lr: 0.000068  loss: 0.6642 (0.6987)  time: 0.5753  data: 0.0002  max mem: 6820\n[21:38:48.841088] Epoch: [2]  [180/969]  eta: 0:07:36  lr: 0.000068  loss: 0.6739 (0.6982)  time: 0.5753  data: 0.0002  max mem: 6820\n[21:39:00.352137] Epoch: [2]  [200/969]  eta: 0:07:25  lr: 0.000069  loss: 0.6989 (0.6995)  time: 0.5755  data: 0.0002  max mem: 6820\n[21:39:11.877506] Epoch: [2]  [220/969]  eta: 0:07:13  lr: 0.000070  loss: 0.6888 (0.6999)  time: 0.5762  data: 0.0002  max mem: 6820\n[21:39:23.362171] Epoch: [2]  [240/969]  eta: 0:07:01  lr: 0.000070  loss: 0.7047 (0.7010)  time: 0.5742  data: 0.0002  max mem: 6820\n[21:39:34.865806] Epoch: [2]  [260/969]  eta: 0:06:49  lr: 0.000071  loss: 0.6948 (0.7012)  time: 0.5751  data: 0.0002  max mem: 6820\n[21:39:46.375811] Epoch: [2]  [280/969]  eta: 0:06:38  lr: 0.000072  loss: 0.6878 (0.7019)  time: 0.5754  data: 0.0002  max mem: 6820\n[21:39:57.857269] Epoch: [2]  [300/969]  eta: 0:06:26  lr: 0.000072  loss: 0.6908 (0.7022)  time: 0.5740  data: 0.0002  max mem: 6820\n[21:40:09.340162] Epoch: [2]  [320/969]  eta: 0:06:14  lr: 0.000073  loss: 0.6838 (0.7021)  time: 0.5741  data: 0.0002  max mem: 6820\n[21:40:20.800177] Epoch: [2]  [340/969]  eta: 0:06:02  lr: 0.000073  loss: 0.6823 (0.7024)  time: 0.5730  data: 0.0002  max mem: 6820\n[21:40:32.238183] Epoch: [2]  [360/969]  eta: 0:05:51  lr: 0.000074  loss: 0.6887 (0.7024)  time: 0.5719  data: 0.0002  max mem: 6820\n[21:40:43.681240] Epoch: [2]  [380/969]  eta: 0:05:39  lr: 0.000075  loss: 0.6932 (0.7027)  time: 0.5721  data: 0.0002  max mem: 6820\n[21:40:55.108886] Epoch: [2]  [400/969]  eta: 0:05:27  lr: 0.000075  loss: 0.6973 (0.7026)  time: 0.5713  data: 0.0002  max mem: 6820\n[21:41:06.532228] Epoch: [2]  [420/969]  eta: 0:05:16  lr: 0.000076  loss: 0.6932 (0.7029)  time: 0.5711  data: 0.0002  max mem: 6820\n[21:41:17.978096] Epoch: [2]  [440/969]  eta: 0:05:04  lr: 0.000077  loss: 0.6794 (0.7027)  time: 0.5722  data: 0.0002  max mem: 6820\n[21:41:29.420704] Epoch: [2]  [460/969]  eta: 0:04:53  lr: 0.000077  loss: 0.6885 (0.7026)  time: 0.5721  data: 0.0002  max mem: 6820\n[21:41:40.841663] Epoch: [2]  [480/969]  eta: 0:04:41  lr: 0.000078  loss: 0.6972 (0.7022)  time: 0.5710  data: 0.0002  max mem: 6820\n[21:41:52.291960] Epoch: [2]  [500/969]  eta: 0:04:29  lr: 0.000079  loss: 0.6938 (0.7024)  time: 0.5725  data: 0.0002  max mem: 6820\n[21:42:03.757167] Epoch: [2]  [520/969]  eta: 0:04:18  lr: 0.000079  loss: 0.6865 (0.7021)  time: 0.5732  data: 0.0002  max mem: 6820\n[21:42:15.248161] Epoch: [2]  [540/969]  eta: 0:04:06  lr: 0.000080  loss: 0.6752 (0.7015)  time: 0.5745  data: 0.0002  max mem: 6820\n[21:42:26.736668] Epoch: [2]  [560/969]  eta: 0:03:55  lr: 0.000081  loss: 0.7151 (0.7018)  time: 0.5744  data: 0.0002  max mem: 6820\n[21:42:38.215191] Epoch: [2]  [580/969]  eta: 0:03:43  lr: 0.000081  loss: 0.7096 (0.7019)  time: 0.5739  data: 0.0002  max mem: 6820\n[21:42:49.686378] Epoch: [2]  [600/969]  eta: 0:03:32  lr: 0.000082  loss: 0.6735 (0.7015)  time: 0.5735  data: 0.0002  max mem: 6820\n[21:43:01.149529] Epoch: [2]  [620/969]  eta: 0:03:20  lr: 0.000082  loss: 0.6875 (0.7014)  time: 0.5731  data: 0.0002  max mem: 6820\n[21:43:12.608741] Epoch: [2]  [640/969]  eta: 0:03:09  lr: 0.000083  loss: 0.7060 (0.7018)  time: 0.5729  data: 0.0002  max mem: 6820\n[21:43:24.056777] Epoch: [2]  [660/969]  eta: 0:02:57  lr: 0.000084  loss: 0.6888 (0.7016)  time: 0.5724  data: 0.0002  max mem: 6820\n[21:43:35.520364] Epoch: [2]  [680/969]  eta: 0:02:46  lr: 0.000084  loss: 0.6971 (0.7021)  time: 0.5731  data: 0.0002  max mem: 6820\n[21:43:46.987215] Epoch: [2]  [700/969]  eta: 0:02:34  lr: 0.000085  loss: 0.6954 (0.7017)  time: 0.5733  data: 0.0002  max mem: 6820\n[21:43:58.435801] Epoch: [2]  [720/969]  eta: 0:02:23  lr: 0.000086  loss: 0.7099 (0.7023)  time: 0.5724  data: 0.0002  max mem: 6820\n[21:44:09.883595] Epoch: [2]  [740/969]  eta: 0:02:11  lr: 0.000086  loss: 0.6947 (0.7024)  time: 0.5723  data: 0.0002  max mem: 6820\n[21:44:21.333316] Epoch: [2]  [760/969]  eta: 0:02:00  lr: 0.000087  loss: 0.7146 (0.7023)  time: 0.5724  data: 0.0002  max mem: 6820\n[21:44:32.789332] Epoch: [2]  [780/969]  eta: 0:01:48  lr: 0.000088  loss: 0.6766 (0.7020)  time: 0.5728  data: 0.0002  max mem: 6820\n[21:44:44.225592] Epoch: [2]  [800/969]  eta: 0:01:37  lr: 0.000088  loss: 0.6902 (0.7019)  time: 0.5718  data: 0.0002  max mem: 6820\n[21:44:55.676754] Epoch: [2]  [820/969]  eta: 0:01:25  lr: 0.000089  loss: 0.6958 (0.7023)  time: 0.5725  data: 0.0002  max mem: 6820\n[21:45:07.112524] Epoch: [2]  [840/969]  eta: 0:01:14  lr: 0.000090  loss: 0.6743 (0.7021)  time: 0.5717  data: 0.0002  max mem: 6820\n[21:45:18.547395] Epoch: [2]  [860/969]  eta: 0:01:02  lr: 0.000090  loss: 0.6894 (0.7021)  time: 0.5717  data: 0.0002  max mem: 6820\n[21:45:29.977634] Epoch: [2]  [880/969]  eta: 0:00:51  lr: 0.000091  loss: 0.6865 (0.7019)  time: 0.5715  data: 0.0002  max mem: 6820\n[21:45:41.427194] Epoch: [2]  [900/969]  eta: 0:00:39  lr: 0.000092  loss: 0.7045 (0.7019)  time: 0.5724  data: 0.0002  max mem: 6820\n[21:45:52.858718] Epoch: [2]  [920/969]  eta: 0:00:28  lr: 0.000092  loss: 0.7033 (0.7020)  time: 0.5715  data: 0.0002  max mem: 6820\n[21:46:04.303521] Epoch: [2]  [940/969]  eta: 0:00:16  lr: 0.000093  loss: 0.7041 (0.7020)  time: 0.5722  data: 0.0002  max mem: 6820\n[21:46:15.742145] Epoch: [2]  [960/969]  eta: 0:00:05  lr: 0.000093  loss: 0.6964 (0.7021)  time: 0.5719  data: 0.0002  max mem: 6820\n[21:46:20.318761] Epoch: [2]  [968/969]  eta: 0:00:00  lr: 0.000094  loss: 0.6964 (0.7021)  time: 0.5723  data: 0.0001  max mem: 6820\n[21:46:20.432662] Epoch: [2] Total time: 0:09:16 (0.5742 s / it)\n[21:46:20.432787] Averaged stats: lr: 0.000094  loss: 0.6964 (0.7021)\n[21:46:21.269911] val:  [  0/139]  eta: 0:01:55  loss: 0.6790 (0.6790)  time: 0.8323  data: 0.6993  max mem: 6820\n[21:46:22.767962] val:  [ 10/139]  eta: 0:00:27  loss: 0.6618 (0.6415)  time: 0.2118  data: 0.0638  max mem: 6820\n[21:46:24.265335] val:  [ 20/139]  eta: 0:00:21  loss: 0.6249 (0.6378)  time: 0.1497  data: 0.0002  max mem: 6820\n[21:46:25.769845] val:  [ 30/139]  eta: 0:00:18  loss: 0.6412 (0.6390)  time: 0.1500  data: 0.0002  max mem: 6820\n[21:46:27.281076] val:  [ 40/139]  eta: 0:00:16  loss: 0.6549 (0.6442)  time: 0.1507  data: 0.0002  max mem: 6820\n[21:46:28.790593] val:  [ 50/139]  eta: 0:00:14  loss: 0.6597 (0.6480)  time: 0.1510  data: 0.0002  max mem: 6820\n[21:46:30.304228] val:  [ 60/139]  eta: 0:00:12  loss: 0.6704 (0.6536)  time: 0.1511  data: 0.0002  max mem: 6820\n[21:46:31.813764] val:  [ 70/139]  eta: 0:00:11  loss: 0.6748 (0.6588)  time: 0.1511  data: 0.0002  max mem: 6820\n[21:46:33.312945] val:  [ 80/139]  eta: 0:00:09  loss: 0.6957 (0.6652)  time: 0.1504  data: 0.0002  max mem: 6820\n[21:46:34.826907] val:  [ 90/139]  eta: 0:00:07  loss: 0.6987 (0.6685)  time: 0.1506  data: 0.0002  max mem: 6820\n[21:46:36.341189] val:  [100/139]  eta: 0:00:06  loss: 0.6947 (0.6715)  time: 0.1513  data: 0.0002  max mem: 6820\n[21:46:37.845490] val:  [110/139]  eta: 0:00:04  loss: 0.6958 (0.6739)  time: 0.1508  data: 0.0002  max mem: 6820\n[21:46:39.357858] val:  [120/139]  eta: 0:00:02  loss: 0.7007 (0.6763)  time: 0.1508  data: 0.0002  max mem: 6820\n[21:46:40.869391] val:  [130/139]  eta: 0:00:01  loss: 0.7063 (0.6786)  time: 0.1511  data: 0.0002  max mem: 6820\n[21:46:41.983920] val:  [138/139]  eta: 0:00:00  loss: 0.7105 (0.6804)  time: 0.1464  data: 0.0002  max mem: 6820\n[21:46:42.095745] val: Total time: 0:00:21 (0.1558 s / it)\n[21:46:42.151264] val loss: 0.6804120180418165\n[21:46:42.151325] Accuracy: 0.5710, F1 Score: 0.5658, ROC AUC: 0.6150, Hamming Loss: 0.4290,\n Jaccard Score: 0.3960, Precision: 0.5745, Recall: 0.5710,\n Average Precision: 0.6015, Kappa: 0.1420, Score: 0.4409\n[21:46:44.933672] Best epoch = 2, Best score = 0.4409\n[21:46:44.936475] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[21:46:46.524169] Epoch: [3]  [  0/969]  eta: 0:25:37  lr: 0.000094  loss: 0.6868 (0.6868)  time: 1.5864  data: 0.9801  max mem: 6820\n[21:46:57.895262] Epoch: [3]  [ 20/969]  eta: 0:09:45  lr: 0.000094  loss: 0.6926 (0.6916)  time: 0.5685  data: 0.0002  max mem: 6820\n[21:47:09.407049] Epoch: [3]  [ 40/969]  eta: 0:09:14  lr: 0.000095  loss: 0.6944 (0.6940)  time: 0.5755  data: 0.0002  max mem: 6820\n[21:47:20.965545] Epoch: [3]  [ 60/969]  eta: 0:08:56  lr: 0.000096  loss: 0.7012 (0.6968)  time: 0.5779  data: 0.0002  max mem: 6820\n[21:47:32.490286] Epoch: [3]  [ 80/969]  eta: 0:08:41  lr: 0.000096  loss: 0.6808 (0.6951)  time: 0.5762  data: 0.0002  max mem: 6820\n[21:47:43.954904] Epoch: [3]  [100/969]  eta: 0:08:27  lr: 0.000097  loss: 0.7006 (0.6965)  time: 0.5732  data: 0.0002  max mem: 6820\n[21:47:55.376837] Epoch: [3]  [120/969]  eta: 0:08:14  lr: 0.000098  loss: 0.6981 (0.6969)  time: 0.5710  data: 0.0002  max mem: 6820\n[21:48:06.754579] Epoch: [3]  [140/969]  eta: 0:08:00  lr: 0.000098  loss: 0.7155 (0.6990)  time: 0.5688  data: 0.0002  max mem: 6820\n[21:48:18.130381] Epoch: [3]  [160/969]  eta: 0:07:48  lr: 0.000099  loss: 0.6943 (0.6988)  time: 0.5687  data: 0.0002  max mem: 6820\n[21:48:29.543621] Epoch: [3]  [180/969]  eta: 0:07:35  lr: 0.000100  loss: 0.6910 (0.6985)  time: 0.5706  data: 0.0002  max mem: 6820\n[21:48:40.960398] Epoch: [3]  [200/969]  eta: 0:07:23  lr: 0.000100  loss: 0.6977 (0.6984)  time: 0.5708  data: 0.0001  max mem: 6820\n[21:48:52.400037] Epoch: [3]  [220/969]  eta: 0:07:11  lr: 0.000101  loss: 0.6806 (0.6978)  time: 0.5719  data: 0.0003  max mem: 6820\n[21:49:03.864660] Epoch: [3]  [240/969]  eta: 0:07:00  lr: 0.000101  loss: 0.6903 (0.6985)  time: 0.5732  data: 0.0002  max mem: 6820\n[21:49:15.332701] Epoch: [3]  [260/969]  eta: 0:06:48  lr: 0.000102  loss: 0.6876 (0.6978)  time: 0.5733  data: 0.0002  max mem: 6820\n[21:49:26.804397] Epoch: [3]  [280/969]  eta: 0:06:36  lr: 0.000103  loss: 0.7170 (0.6998)  time: 0.5735  data: 0.0002  max mem: 6820\n[21:49:38.265377] Epoch: [3]  [300/969]  eta: 0:06:25  lr: 0.000103  loss: 0.6935 (0.7001)  time: 0.5730  data: 0.0002  max mem: 6820\n[21:49:49.763379] Epoch: [3]  [320/969]  eta: 0:06:13  lr: 0.000104  loss: 0.6723 (0.6992)  time: 0.5748  data: 0.0002  max mem: 6820\n[21:50:01.243998] Epoch: [3]  [340/969]  eta: 0:06:02  lr: 0.000105  loss: 0.6911 (0.6994)  time: 0.5740  data: 0.0002  max mem: 6820\n[21:50:12.744275] Epoch: [3]  [360/969]  eta: 0:05:50  lr: 0.000105  loss: 0.6766 (0.6993)  time: 0.5749  data: 0.0002  max mem: 6820\n[21:50:24.243740] Epoch: [3]  [380/969]  eta: 0:05:38  lr: 0.000106  loss: 0.6908 (0.6992)  time: 0.5749  data: 0.0002  max mem: 6820\n[21:50:35.756302] Epoch: [3]  [400/969]  eta: 0:05:27  lr: 0.000107  loss: 0.6824 (0.6991)  time: 0.5756  data: 0.0002  max mem: 6820\n[21:50:47.261998] Epoch: [3]  [420/969]  eta: 0:05:15  lr: 0.000107  loss: 0.6848 (0.6990)  time: 0.5752  data: 0.0002  max mem: 6820\n[21:50:58.753898] Epoch: [3]  [440/969]  eta: 0:05:04  lr: 0.000108  loss: 0.6979 (0.6989)  time: 0.5745  data: 0.0002  max mem: 6820\n[21:51:10.258550] Epoch: [3]  [460/969]  eta: 0:04:52  lr: 0.000109  loss: 0.6927 (0.6990)  time: 0.5752  data: 0.0002  max mem: 6820\n[21:51:21.739310] Epoch: [3]  [480/969]  eta: 0:04:41  lr: 0.000109  loss: 0.6902 (0.6992)  time: 0.5740  data: 0.0002  max mem: 6820\n[21:51:33.224801] Epoch: [3]  [500/969]  eta: 0:04:29  lr: 0.000110  loss: 0.7009 (0.6991)  time: 0.5742  data: 0.0002  max mem: 6820\n[21:51:44.701369] Epoch: [3]  [520/969]  eta: 0:04:18  lr: 0.000111  loss: 0.6733 (0.6988)  time: 0.5738  data: 0.0002  max mem: 6820\n[21:51:56.207394] Epoch: [3]  [540/969]  eta: 0:04:06  lr: 0.000111  loss: 0.6840 (0.6982)  time: 0.5752  data: 0.0002  max mem: 6820\n[21:52:07.707154] Epoch: [3]  [560/969]  eta: 0:03:55  lr: 0.000112  loss: 0.6799 (0.6981)  time: 0.5749  data: 0.0002  max mem: 6820\n[21:52:19.208211] Epoch: [3]  [580/969]  eta: 0:03:43  lr: 0.000112  loss: 0.7025 (0.6981)  time: 0.5750  data: 0.0002  max mem: 6820\n[21:52:30.693208] Epoch: [3]  [600/969]  eta: 0:03:32  lr: 0.000113  loss: 0.6748 (0.6976)  time: 0.5742  data: 0.0002  max mem: 6820\n[21:52:42.159098] Epoch: [3]  [620/969]  eta: 0:03:20  lr: 0.000114  loss: 0.7081 (0.6982)  time: 0.5732  data: 0.0002  max mem: 6820\n[21:52:53.611478] Epoch: [3]  [640/969]  eta: 0:03:09  lr: 0.000114  loss: 0.7026 (0.6987)  time: 0.5726  data: 0.0002  max mem: 6820\n[21:53:05.060420] Epoch: [3]  [660/969]  eta: 0:02:57  lr: 0.000115  loss: 0.6926 (0.6984)  time: 0.5724  data: 0.0002  max mem: 6820\n[21:53:16.403110] Epoch: [3]  [680/969]  eta: 0:02:46  lr: 0.000116  loss: 0.6884 (0.6983)  time: 0.5671  data: 0.0002  max mem: 6820\n[21:53:27.843770] Epoch: [3]  [700/969]  eta: 0:02:34  lr: 0.000116  loss: 0.6917 (0.6984)  time: 0.5720  data: 0.0002  max mem: 6820\n[21:53:39.283831] Epoch: [3]  [720/969]  eta: 0:02:23  lr: 0.000117  loss: 0.7035 (0.6982)  time: 0.5719  data: 0.0002  max mem: 6820\n[21:53:50.720238] Epoch: [3]  [740/969]  eta: 0:02:11  lr: 0.000118  loss: 0.6881 (0.6981)  time: 0.5718  data: 0.0002  max mem: 6820\n[21:54:02.148318] Epoch: [3]  [760/969]  eta: 0:02:00  lr: 0.000118  loss: 0.6989 (0.6981)  time: 0.5713  data: 0.0002  max mem: 6820\n[21:54:13.557723] Epoch: [3]  [780/969]  eta: 0:01:48  lr: 0.000119  loss: 0.6763 (0.6979)  time: 0.5704  data: 0.0002  max mem: 6820\n[21:54:24.969394] Epoch: [3]  [800/969]  eta: 0:01:37  lr: 0.000120  loss: 0.6958 (0.6980)  time: 0.5705  data: 0.0002  max mem: 6820\n[21:54:36.294151] Epoch: [3]  [820/969]  eta: 0:01:25  lr: 0.000120  loss: 0.6964 (0.6980)  time: 0.5662  data: 0.0002  max mem: 6820\n[21:54:47.691398] Epoch: [3]  [840/969]  eta: 0:01:14  lr: 0.000121  loss: 0.6879 (0.6981)  time: 0.5698  data: 0.0002  max mem: 6820\n[21:54:59.079859] Epoch: [3]  [860/969]  eta: 0:01:02  lr: 0.000121  loss: 0.6925 (0.6979)  time: 0.5694  data: 0.0002  max mem: 6820\n[21:55:10.484752] Epoch: [3]  [880/969]  eta: 0:00:51  lr: 0.000122  loss: 0.6828 (0.6976)  time: 0.5702  data: 0.0002  max mem: 6820\n[21:55:21.902851] Epoch: [3]  [900/969]  eta: 0:00:39  lr: 0.000123  loss: 0.6967 (0.6976)  time: 0.5709  data: 0.0002  max mem: 6820\n[21:55:33.336416] Epoch: [3]  [920/969]  eta: 0:00:28  lr: 0.000123  loss: 0.7069 (0.6978)  time: 0.5716  data: 0.0002  max mem: 6820\n[21:55:44.750191] Epoch: [3]  [940/969]  eta: 0:00:16  lr: 0.000124  loss: 0.6875 (0.6978)  time: 0.5706  data: 0.0002  max mem: 6820\n[21:55:56.194342] Epoch: [3]  [960/969]  eta: 0:00:05  lr: 0.000125  loss: 0.6979 (0.6979)  time: 0.5722  data: 0.0002  max mem: 6820\n[21:56:00.780119] Epoch: [3]  [968/969]  eta: 0:00:00  lr: 0.000125  loss: 0.6979 (0.6980)  time: 0.5729  data: 0.0002  max mem: 6820\n[21:56:00.890939] Epoch: [3] Total time: 0:09:15 (0.5737 s / it)\n[21:56:00.891063] Averaged stats: lr: 0.000125  loss: 0.6979 (0.6980)\n[21:56:01.983591] val:  [  0/139]  eta: 0:02:31  loss: 0.6527 (0.6527)  time: 1.0880  data: 0.8712  max mem: 6820\n[21:56:03.473178] val:  [ 10/139]  eta: 0:00:30  loss: 0.6438 (0.6228)  time: 0.2343  data: 0.0794  max mem: 6820\n[21:56:04.976378] val:  [ 20/139]  eta: 0:00:23  loss: 0.5979 (0.6160)  time: 0.1496  data: 0.0002  max mem: 6820\n[21:56:06.488313] val:  [ 30/139]  eta: 0:00:19  loss: 0.6096 (0.6158)  time: 0.1507  data: 0.0002  max mem: 6820\n[21:56:08.001337] val:  [ 40/139]  eta: 0:00:17  loss: 0.6353 (0.6241)  time: 0.1512  data: 0.0002  max mem: 6820\n[21:56:09.516622] val:  [ 50/139]  eta: 0:00:15  loss: 0.6455 (0.6292)  time: 0.1513  data: 0.0002  max mem: 6820\n[21:56:11.035786] val:  [ 60/139]  eta: 0:00:13  loss: 0.6592 (0.6352)  time: 0.1517  data: 0.0002  max mem: 6820\n[21:56:12.547182] val:  [ 70/139]  eta: 0:00:11  loss: 0.6644 (0.6422)  time: 0.1515  data: 0.0002  max mem: 6820\n[21:56:14.056442] val:  [ 80/139]  eta: 0:00:09  loss: 0.7079 (0.6530)  time: 0.1510  data: 0.0002  max mem: 6820\n[21:56:15.572367] val:  [ 90/139]  eta: 0:00:07  loss: 0.7220 (0.6598)  time: 0.1512  data: 0.0002  max mem: 6820\n[21:56:17.094815] val:  [100/139]  eta: 0:00:06  loss: 0.7108 (0.6649)  time: 0.1518  data: 0.0002  max mem: 6820\n[21:56:18.607754] val:  [110/139]  eta: 0:00:04  loss: 0.7036 (0.6689)  time: 0.1517  data: 0.0002  max mem: 6820\n[21:56:20.120366] val:  [120/139]  eta: 0:00:03  loss: 0.7025 (0.6724)  time: 0.1512  data: 0.0002  max mem: 6820\n[21:56:21.633077] val:  [130/139]  eta: 0:00:01  loss: 0.7252 (0.6759)  time: 0.1512  data: 0.0001  max mem: 6820\n[21:56:22.748604] val:  [138/139]  eta: 0:00:00  loss: 0.7244 (0.6785)  time: 0.1464  data: 0.0001  max mem: 6820\n[21:56:22.851552] val: Total time: 0:00:21 (0.1580 s / it)\n[21:56:22.905111] val loss: 0.6784883128653327\n[21:56:22.905158] Accuracy: 0.5710, F1 Score: 0.5540, ROC AUC: 0.6217, Hamming Loss: 0.4290,\n Jaccard Score: 0.3882, Precision: 0.5837, Recall: 0.5710,\n Average Precision: 0.6080, Kappa: 0.1420, Score: 0.4392\n[21:56:22.906876] Best epoch = 2, Best score = 0.4409\n[21:56:22.909607] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[21:56:24.244128] Epoch: [4]  [  0/969]  eta: 0:21:31  lr: 0.000125  loss: 0.6460 (0.6460)  time: 1.3327  data: 0.7594  max mem: 6820\n[21:56:35.669062] Epoch: [4]  [ 20/969]  eta: 0:09:36  lr: 0.000126  loss: 0.6958 (0.6927)  time: 0.5712  data: 0.0002  max mem: 6820\n[21:56:47.141147] Epoch: [4]  [ 40/969]  eta: 0:09:08  lr: 0.000126  loss: 0.6884 (0.6929)  time: 0.5736  data: 0.0002  max mem: 6820\n[21:56:58.631837] Epoch: [4]  [ 60/969]  eta: 0:08:52  lr: 0.000127  loss: 0.6761 (0.6906)  time: 0.5745  data: 0.0002  max mem: 6820\n[21:57:10.112863] Epoch: [4]  [ 80/969]  eta: 0:08:38  lr: 0.000128  loss: 0.6927 (0.6915)  time: 0.5740  data: 0.0002  max mem: 6820\n[21:57:21.580346] Epoch: [4]  [100/969]  eta: 0:08:24  lr: 0.000128  loss: 0.6999 (0.6928)  time: 0.5733  data: 0.0002  max mem: 6820\n[21:57:33.036168] Epoch: [4]  [120/969]  eta: 0:08:11  lr: 0.000129  loss: 0.6889 (0.6921)  time: 0.5727  data: 0.0002  max mem: 6820\n[21:57:44.454405] Epoch: [4]  [140/969]  eta: 0:07:59  lr: 0.000130  loss: 0.7153 (0.6948)  time: 0.5709  data: 0.0002  max mem: 6820\n[21:57:55.899297] Epoch: [4]  [160/969]  eta: 0:07:47  lr: 0.000130  loss: 0.6806 (0.6936)  time: 0.5722  data: 0.0002  max mem: 6820\n[21:58:07.348683] Epoch: [4]  [180/969]  eta: 0:07:35  lr: 0.000131  loss: 0.6879 (0.6943)  time: 0.5724  data: 0.0002  max mem: 6820\n[21:58:18.793644] Epoch: [4]  [200/969]  eta: 0:07:23  lr: 0.000131  loss: 0.6841 (0.6939)  time: 0.5722  data: 0.0002  max mem: 6820\n[21:58:30.203540] Epoch: [4]  [220/969]  eta: 0:07:11  lr: 0.000132  loss: 0.6874 (0.6946)  time: 0.5704  data: 0.0002  max mem: 6820\n[21:58:41.609931] Epoch: [4]  [240/969]  eta: 0:06:59  lr: 0.000133  loss: 0.6892 (0.6944)  time: 0.5703  data: 0.0002  max mem: 6820\n[21:58:53.016661] Epoch: [4]  [260/969]  eta: 0:06:47  lr: 0.000133  loss: 0.7012 (0.6955)  time: 0.5703  data: 0.0002  max mem: 6820\n[21:59:04.410970] Epoch: [4]  [280/969]  eta: 0:06:35  lr: 0.000134  loss: 0.7059 (0.6969)  time: 0.5697  data: 0.0002  max mem: 6820\n[21:59:15.819699] Epoch: [4]  [300/969]  eta: 0:06:24  lr: 0.000135  loss: 0.6938 (0.6969)  time: 0.5704  data: 0.0002  max mem: 6820\n[21:59:27.221273] Epoch: [4]  [320/969]  eta: 0:06:12  lr: 0.000135  loss: 0.6864 (0.6964)  time: 0.5700  data: 0.0002  max mem: 6820\n[21:59:38.631645] Epoch: [4]  [340/969]  eta: 0:06:00  lr: 0.000136  loss: 0.6893 (0.6963)  time: 0.5705  data: 0.0002  max mem: 6820\n[21:59:50.025109] Epoch: [4]  [360/969]  eta: 0:05:49  lr: 0.000137  loss: 0.6894 (0.6960)  time: 0.5696  data: 0.0002  max mem: 6820\n[22:00:01.440309] Epoch: [4]  [380/969]  eta: 0:05:37  lr: 0.000137  loss: 0.7130 (0.6974)  time: 0.5707  data: 0.0002  max mem: 6820\n[22:00:12.886250] Epoch: [4]  [400/969]  eta: 0:05:26  lr: 0.000138  loss: 0.6914 (0.6972)  time: 0.5722  data: 0.0002  max mem: 6820\n[22:00:24.321738] Epoch: [4]  [420/969]  eta: 0:05:14  lr: 0.000139  loss: 0.6835 (0.6970)  time: 0.5717  data: 0.0002  max mem: 6820\n[22:00:35.762936] Epoch: [4]  [440/969]  eta: 0:05:03  lr: 0.000139  loss: 0.6861 (0.6969)  time: 0.5720  data: 0.0002  max mem: 6820\n[22:00:47.204754] Epoch: [4]  [460/969]  eta: 0:04:51  lr: 0.000140  loss: 0.6911 (0.6968)  time: 0.5720  data: 0.0002  max mem: 6820\n[22:00:58.650699] Epoch: [4]  [480/969]  eta: 0:04:40  lr: 0.000140  loss: 0.6862 (0.6967)  time: 0.5723  data: 0.0002  max mem: 6820\n[22:01:10.120841] Epoch: [4]  [500/969]  eta: 0:04:28  lr: 0.000141  loss: 0.6800 (0.6969)  time: 0.5735  data: 0.0002  max mem: 6820\n[22:01:21.581535] Epoch: [4]  [520/969]  eta: 0:04:17  lr: 0.000142  loss: 0.6967 (0.6970)  time: 0.5730  data: 0.0002  max mem: 6820\n[22:01:33.056476] Epoch: [4]  [540/969]  eta: 0:04:05  lr: 0.000142  loss: 0.6885 (0.6967)  time: 0.5737  data: 0.0002  max mem: 6820\n[22:01:44.529211] Epoch: [4]  [560/969]  eta: 0:03:54  lr: 0.000143  loss: 0.6788 (0.6963)  time: 0.5736  data: 0.0002  max mem: 6820\n[22:01:55.977219] Epoch: [4]  [580/969]  eta: 0:03:42  lr: 0.000144  loss: 0.7018 (0.6967)  time: 0.5723  data: 0.0002  max mem: 6820\n[22:02:07.398391] Epoch: [4]  [600/969]  eta: 0:03:31  lr: 0.000144  loss: 0.6858 (0.6968)  time: 0.5710  data: 0.0002  max mem: 6820\n[22:02:18.852666] Epoch: [4]  [620/969]  eta: 0:03:20  lr: 0.000145  loss: 0.6850 (0.6969)  time: 0.5726  data: 0.0002  max mem: 6820\n[22:02:30.283959] Epoch: [4]  [640/969]  eta: 0:03:08  lr: 0.000146  loss: 0.7021 (0.6970)  time: 0.5715  data: 0.0002  max mem: 6820\n[22:02:41.714043] Epoch: [4]  [660/969]  eta: 0:02:57  lr: 0.000146  loss: 0.6801 (0.6967)  time: 0.5715  data: 0.0002  max mem: 6820\n[22:02:53.132789] Epoch: [4]  [680/969]  eta: 0:02:45  lr: 0.000147  loss: 0.6905 (0.6968)  time: 0.5709  data: 0.0002  max mem: 6820\n[22:03:04.569330] Epoch: [4]  [700/969]  eta: 0:02:34  lr: 0.000148  loss: 0.7007 (0.6968)  time: 0.5718  data: 0.0002  max mem: 6820\n[22:03:15.976269] Epoch: [4]  [720/969]  eta: 0:02:22  lr: 0.000148  loss: 0.6942 (0.6970)  time: 0.5703  data: 0.0002  max mem: 6820\n[22:03:27.408725] Epoch: [4]  [740/969]  eta: 0:02:11  lr: 0.000149  loss: 0.6994 (0.6970)  time: 0.5716  data: 0.0002  max mem: 6820\n[22:03:38.835949] Epoch: [4]  [760/969]  eta: 0:01:59  lr: 0.000150  loss: 0.6856 (0.6969)  time: 0.5713  data: 0.0002  max mem: 6820\n[22:03:50.261164] Epoch: [4]  [780/969]  eta: 0:01:48  lr: 0.000150  loss: 0.6821 (0.6965)  time: 0.5712  data: 0.0001  max mem: 6820\n[22:04:01.672681] Epoch: [4]  [800/969]  eta: 0:01:36  lr: 0.000151  loss: 0.6986 (0.6967)  time: 0.5705  data: 0.0002  max mem: 6820\n[22:04:13.065171] Epoch: [4]  [820/969]  eta: 0:01:25  lr: 0.000151  loss: 0.6801 (0.6968)  time: 0.5696  data: 0.0002  max mem: 6820\n[22:04:24.469511] Epoch: [4]  [840/969]  eta: 0:01:13  lr: 0.000152  loss: 0.6978 (0.6967)  time: 0.5701  data: 0.0002  max mem: 6820\n[22:04:35.867405] Epoch: [4]  [860/969]  eta: 0:01:02  lr: 0.000153  loss: 0.6873 (0.6965)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:04:47.276189] Epoch: [4]  [880/969]  eta: 0:00:50  lr: 0.000153  loss: 0.6868 (0.6965)  time: 0.5704  data: 0.0002  max mem: 6820\n[22:04:58.675083] Epoch: [4]  [900/969]  eta: 0:00:39  lr: 0.000154  loss: 0.6904 (0.6964)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:05:10.079610] Epoch: [4]  [920/969]  eta: 0:00:28  lr: 0.000155  loss: 0.6913 (0.6962)  time: 0.5702  data: 0.0002  max mem: 6820\n[22:05:21.483329] Epoch: [4]  [940/969]  eta: 0:00:16  lr: 0.000155  loss: 0.6968 (0.6962)  time: 0.5701  data: 0.0002  max mem: 6820\n[22:05:32.859379] Epoch: [4]  [960/969]  eta: 0:00:05  lr: 0.000156  loss: 0.6973 (0.6964)  time: 0.5688  data: 0.0002  max mem: 6820\n[22:05:37.428271] Epoch: [4]  [968/969]  eta: 0:00:00  lr: 0.000156  loss: 0.6941 (0.6962)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:05:37.546943] Epoch: [4] Total time: 0:09:14 (0.5724 s / it)\n[22:05:37.547076] Averaged stats: lr: 0.000156  loss: 0.6941 (0.6962)\n[22:05:38.303819] val:  [  0/139]  eta: 0:01:44  loss: 0.6317 (0.6317)  time: 0.7525  data: 0.6106  max mem: 6820\n[22:05:39.812885] val:  [ 10/139]  eta: 0:00:26  loss: 0.6127 (0.5929)  time: 0.2055  data: 0.0557  max mem: 6820\n[22:05:41.317625] val:  [ 20/139]  eta: 0:00:21  loss: 0.5654 (0.5858)  time: 0.1506  data: 0.0002  max mem: 6820\n[22:05:42.821340] val:  [ 30/139]  eta: 0:00:18  loss: 0.5914 (0.5867)  time: 0.1504  data: 0.0002  max mem: 6820\n[22:05:44.325413] val:  [ 40/139]  eta: 0:00:16  loss: 0.6133 (0.5955)  time: 0.1503  data: 0.0002  max mem: 6820\n[22:05:45.836334] val:  [ 50/139]  eta: 0:00:14  loss: 0.6146 (0.6008)  time: 0.1507  data: 0.0002  max mem: 6820\n[22:05:47.347892] val:  [ 60/139]  eta: 0:00:12  loss: 0.6226 (0.6063)  time: 0.1510  data: 0.0002  max mem: 6820\n[22:05:48.854887] val:  [ 70/139]  eta: 0:00:10  loss: 0.6372 (0.6149)  time: 0.1508  data: 0.0002  max mem: 6820\n[22:05:50.362885] val:  [ 80/139]  eta: 0:00:09  loss: 0.7281 (0.6326)  time: 0.1507  data: 0.0002  max mem: 6820\n[22:05:51.881301] val:  [ 90/139]  eta: 0:00:07  loss: 0.7490 (0.6450)  time: 0.1512  data: 0.0002  max mem: 6820\n[22:05:53.400935] val:  [100/139]  eta: 0:00:06  loss: 0.7453 (0.6549)  time: 0.1518  data: 0.0002  max mem: 6820\n[22:05:54.912083] val:  [110/139]  eta: 0:00:04  loss: 0.7359 (0.6621)  time: 0.1515  data: 0.0002  max mem: 6820\n[22:05:56.421678] val:  [120/139]  eta: 0:00:02  loss: 0.7300 (0.6681)  time: 0.1510  data: 0.0002  max mem: 6820\n[22:05:57.934820] val:  [130/139]  eta: 0:00:01  loss: 0.7369 (0.6735)  time: 0.1511  data: 0.0001  max mem: 6820\n[22:05:59.046858] val:  [138/139]  eta: 0:00:00  loss: 0.7369 (0.6775)  time: 0.1463  data: 0.0001  max mem: 6820\n[22:05:59.184041] val: Total time: 0:00:21 (0.1556 s / it)\n[22:05:59.242217] val loss: 0.6775080179996628\n[22:05:59.242281] Accuracy: 0.5592, F1 Score: 0.5303, ROC AUC: 0.6273, Hamming Loss: 0.4408,\n Jaccard Score: 0.3694, Precision: 0.5786, Recall: 0.5592,\n Average Precision: 0.6113, Kappa: 0.1184, Score: 0.4254\n[22:05:59.244220] Best epoch = 2, Best score = 0.4409\n[22:05:59.246913] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[22:06:00.610642] Epoch: [5]  [  0/969]  eta: 0:22:00  lr: 0.000156  loss: 0.6933 (0.6933)  time: 1.3624  data: 0.7727  max mem: 6820\n[22:06:11.998689] Epoch: [5]  [ 20/969]  eta: 0:09:36  lr: 0.000157  loss: 0.6898 (0.6869)  time: 0.5694  data: 0.0002  max mem: 6820\n[22:06:23.419551] Epoch: [5]  [ 40/969]  eta: 0:09:07  lr: 0.000158  loss: 0.6893 (0.6890)  time: 0.5710  data: 0.0002  max mem: 6820\n[22:06:34.858076] Epoch: [5]  [ 60/969]  eta: 0:08:50  lr: 0.000158  loss: 0.6836 (0.6886)  time: 0.5719  data: 0.0002  max mem: 6820\n[22:06:46.317149] Epoch: [5]  [ 80/969]  eta: 0:08:36  lr: 0.000159  loss: 0.6765 (0.6870)  time: 0.5729  data: 0.0002  max mem: 6820\n[22:06:57.793302] Epoch: [5]  [100/969]  eta: 0:08:23  lr: 0.000159  loss: 0.6881 (0.6872)  time: 0.5738  data: 0.0002  max mem: 6820\n[22:07:09.265092] Epoch: [5]  [120/969]  eta: 0:08:11  lr: 0.000160  loss: 0.6803 (0.6879)  time: 0.5735  data: 0.0002  max mem: 6820\n[22:07:20.747857] Epoch: [5]  [140/969]  eta: 0:07:59  lr: 0.000161  loss: 0.7114 (0.6916)  time: 0.5741  data: 0.0002  max mem: 6820\n[22:07:32.222128] Epoch: [5]  [160/969]  eta: 0:07:47  lr: 0.000161  loss: 0.6826 (0.6913)  time: 0.5737  data: 0.0002  max mem: 6820\n[22:07:43.676086] Epoch: [5]  [180/969]  eta: 0:07:35  lr: 0.000162  loss: 0.6943 (0.6914)  time: 0.5726  data: 0.0002  max mem: 6820\n[22:07:55.117363] Epoch: [5]  [200/969]  eta: 0:07:23  lr: 0.000163  loss: 0.6961 (0.6921)  time: 0.5720  data: 0.0002  max mem: 6820\n[22:08:06.554966] Epoch: [5]  [220/969]  eta: 0:07:11  lr: 0.000163  loss: 0.6924 (0.6924)  time: 0.5718  data: 0.0003  max mem: 6820\n[22:08:18.003299] Epoch: [5]  [240/969]  eta: 0:06:59  lr: 0.000164  loss: 0.7023 (0.6931)  time: 0.5724  data: 0.0002  max mem: 6820\n[22:08:29.418180] Epoch: [5]  [260/969]  eta: 0:06:47  lr: 0.000165  loss: 0.6940 (0.6934)  time: 0.5707  data: 0.0002  max mem: 6820\n[22:08:40.824604] Epoch: [5]  [280/969]  eta: 0:06:36  lr: 0.000165  loss: 0.6996 (0.6945)  time: 0.5703  data: 0.0002  max mem: 6820\n[22:08:52.210512] Epoch: [5]  [300/969]  eta: 0:06:24  lr: 0.000166  loss: 0.6900 (0.6945)  time: 0.5692  data: 0.0002  max mem: 6820\n[22:09:03.616097] Epoch: [5]  [320/969]  eta: 0:06:12  lr: 0.000167  loss: 0.6817 (0.6941)  time: 0.5702  data: 0.0002  max mem: 6820\n[22:09:15.023414] Epoch: [5]  [340/969]  eta: 0:06:01  lr: 0.000167  loss: 0.6699 (0.6936)  time: 0.5703  data: 0.0002  max mem: 6820\n[22:09:26.428588] Epoch: [5]  [360/969]  eta: 0:05:49  lr: 0.000168  loss: 0.6927 (0.6940)  time: 0.5702  data: 0.0002  max mem: 6820\n[22:09:37.808266] Epoch: [5]  [380/969]  eta: 0:05:37  lr: 0.000169  loss: 0.7073 (0.6942)  time: 0.5689  data: 0.0002  max mem: 6820\n[22:09:49.214901] Epoch: [5]  [400/969]  eta: 0:05:26  lr: 0.000169  loss: 0.6880 (0.6935)  time: 0.5703  data: 0.0003  max mem: 6820\n[22:10:00.612177] Epoch: [5]  [420/969]  eta: 0:05:14  lr: 0.000170  loss: 0.6932 (0.6935)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:10:12.027188] Epoch: [5]  [440/969]  eta: 0:05:03  lr: 0.000170  loss: 0.6944 (0.6937)  time: 0.5707  data: 0.0002  max mem: 6820\n[22:10:23.425791] Epoch: [5]  [460/969]  eta: 0:04:51  lr: 0.000171  loss: 0.6795 (0.6936)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:10:34.817913] Epoch: [5]  [480/969]  eta: 0:04:40  lr: 0.000172  loss: 0.6895 (0.6937)  time: 0.5696  data: 0.0002  max mem: 6820\n[22:10:46.223961] Epoch: [5]  [500/969]  eta: 0:04:28  lr: 0.000172  loss: 0.6901 (0.6937)  time: 0.5703  data: 0.0003  max mem: 6820\n[22:10:57.614489] Epoch: [5]  [520/969]  eta: 0:04:17  lr: 0.000173  loss: 0.6916 (0.6934)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:11:08.996173] Epoch: [5]  [540/969]  eta: 0:04:05  lr: 0.000174  loss: 0.6821 (0.6930)  time: 0.5690  data: 0.0002  max mem: 6820\n[22:11:20.398021] Epoch: [5]  [560/969]  eta: 0:03:54  lr: 0.000174  loss: 0.6676 (0.6926)  time: 0.5700  data: 0.0002  max mem: 6820\n[22:11:31.806728] Epoch: [5]  [580/969]  eta: 0:03:42  lr: 0.000175  loss: 0.6646 (0.6926)  time: 0.5704  data: 0.0002  max mem: 6820\n[22:11:43.206356] Epoch: [5]  [600/969]  eta: 0:03:31  lr: 0.000176  loss: 0.7100 (0.6931)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:11:54.576681] Epoch: [5]  [620/969]  eta: 0:03:19  lr: 0.000176  loss: 0.6971 (0.6934)  time: 0.5685  data: 0.0002  max mem: 6820\n[22:12:05.941190] Epoch: [5]  [640/969]  eta: 0:03:08  lr: 0.000177  loss: 0.6918 (0.6936)  time: 0.5682  data: 0.0002  max mem: 6820\n[22:12:17.315827] Epoch: [5]  [660/969]  eta: 0:02:56  lr: 0.000178  loss: 0.6858 (0.6932)  time: 0.5687  data: 0.0002  max mem: 6820\n[22:12:28.713265] Epoch: [5]  [680/969]  eta: 0:02:45  lr: 0.000178  loss: 0.7020 (0.6936)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:12:40.121542] Epoch: [5]  [700/969]  eta: 0:02:33  lr: 0.000179  loss: 0.6808 (0.6936)  time: 0.5704  data: 0.0002  max mem: 6820\n[22:12:51.515631] Epoch: [5]  [720/969]  eta: 0:02:22  lr: 0.000179  loss: 0.7124 (0.6937)  time: 0.5697  data: 0.0002  max mem: 6820\n[22:13:02.906055] Epoch: [5]  [740/969]  eta: 0:02:10  lr: 0.000180  loss: 0.6916 (0.6939)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:13:14.277741] Epoch: [5]  [760/969]  eta: 0:01:59  lr: 0.000181  loss: 0.6960 (0.6939)  time: 0.5685  data: 0.0002  max mem: 6820\n[22:13:25.647435] Epoch: [5]  [780/969]  eta: 0:01:48  lr: 0.000181  loss: 0.6941 (0.6940)  time: 0.5684  data: 0.0002  max mem: 6820\n[22:13:37.046476] Epoch: [5]  [800/969]  eta: 0:01:36  lr: 0.000182  loss: 0.6931 (0.6943)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:13:48.437048] Epoch: [5]  [820/969]  eta: 0:01:25  lr: 0.000183  loss: 0.6822 (0.6942)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:13:59.855099] Epoch: [5]  [840/969]  eta: 0:01:13  lr: 0.000183  loss: 0.6932 (0.6940)  time: 0.5709  data: 0.0002  max mem: 6820\n[22:14:11.287658] Epoch: [5]  [860/969]  eta: 0:01:02  lr: 0.000184  loss: 0.7007 (0.6941)  time: 0.5716  data: 0.0002  max mem: 6820\n[22:14:22.729271] Epoch: [5]  [880/969]  eta: 0:00:50  lr: 0.000185  loss: 0.7008 (0.6944)  time: 0.5720  data: 0.0002  max mem: 6820\n[22:14:34.199054] Epoch: [5]  [900/969]  eta: 0:00:39  lr: 0.000185  loss: 0.6909 (0.6943)  time: 0.5734  data: 0.0002  max mem: 6820\n[22:14:45.664308] Epoch: [5]  [920/969]  eta: 0:00:28  lr: 0.000186  loss: 0.6947 (0.6943)  time: 0.5732  data: 0.0002  max mem: 6820\n[22:14:57.115516] Epoch: [5]  [940/969]  eta: 0:00:16  lr: 0.000187  loss: 0.6845 (0.6942)  time: 0.5725  data: 0.0002  max mem: 6820\n[22:15:08.455373] Epoch: [5]  [960/969]  eta: 0:00:05  lr: 0.000187  loss: 0.6918 (0.6942)  time: 0.5669  data: 0.0002  max mem: 6820\n[22:15:13.036872] Epoch: [5]  [968/969]  eta: 0:00:00  lr: 0.000187  loss: 0.6885 (0.6942)  time: 0.5718  data: 0.0002  max mem: 6820\n[22:15:13.155922] Epoch: [5] Total time: 0:09:13 (0.5716 s / it)\n[22:15:13.156132] Averaged stats: lr: 0.000187  loss: 0.6885 (0.6942)\n[22:15:13.994423] val:  [  0/139]  eta: 0:01:55  loss: 0.7234 (0.7234)  time: 0.8339  data: 0.6829  max mem: 6820\n[22:15:15.496330] val:  [ 10/139]  eta: 0:00:27  loss: 0.6888 (0.6807)  time: 0.2123  data: 0.0628  max mem: 6820\n[22:15:17.006715] val:  [ 20/139]  eta: 0:00:21  loss: 0.6682 (0.6759)  time: 0.1505  data: 0.0005  max mem: 6820\n[22:15:18.520451] val:  [ 30/139]  eta: 0:00:18  loss: 0.6871 (0.6784)  time: 0.1511  data: 0.0002  max mem: 6820\n[22:15:20.033999] val:  [ 40/139]  eta: 0:00:16  loss: 0.6996 (0.6865)  time: 0.1513  data: 0.0002  max mem: 6820\n[22:15:21.550933] val:  [ 50/139]  eta: 0:00:14  loss: 0.7085 (0.6915)  time: 0.1514  data: 0.0002  max mem: 6820\n[22:15:23.071909] val:  [ 60/139]  eta: 0:00:12  loss: 0.7176 (0.6967)  time: 0.1518  data: 0.0002  max mem: 6820\n[22:15:24.591690] val:  [ 70/139]  eta: 0:00:11  loss: 0.7169 (0.6994)  time: 0.1520  data: 0.0002  max mem: 6820\n[22:15:26.110286] val:  [ 80/139]  eta: 0:00:09  loss: 0.6960 (0.6955)  time: 0.1518  data: 0.0002  max mem: 6820\n[22:15:27.631442] val:  [ 90/139]  eta: 0:00:07  loss: 0.6542 (0.6909)  time: 0.1519  data: 0.0002  max mem: 6820\n[22:15:29.152199] val:  [100/139]  eta: 0:00:06  loss: 0.6542 (0.6873)  time: 0.1520  data: 0.0002  max mem: 6820\n[22:15:30.671591] val:  [110/139]  eta: 0:00:04  loss: 0.6490 (0.6834)  time: 0.1519  data: 0.0002  max mem: 6820\n[22:15:32.187541] val:  [120/139]  eta: 0:00:02  loss: 0.6475 (0.6802)  time: 0.1517  data: 0.0002  max mem: 6820\n[22:15:33.707354] val:  [130/139]  eta: 0:00:01  loss: 0.6475 (0.6777)  time: 0.1517  data: 0.0001  max mem: 6820\n[22:15:34.829493] val:  [138/139]  eta: 0:00:00  loss: 0.6452 (0.6756)  time: 0.1471  data: 0.0001  max mem: 6820\n[22:15:34.933417] val: Total time: 0:00:21 (0.1566 s / it)\n[22:15:34.987360] val loss: 0.6755691638095773\n[22:15:34.987414] Accuracy: 0.5945, F1 Score: 0.5864, ROC AUC: 0.6257, Hamming Loss: 0.4055,\n Jaccard Score: 0.4172, Precision: 0.6025, Recall: 0.5945,\n Average Precision: 0.6116, Kappa: 0.1890, Score: 0.4670\n[22:15:37.812332] Best epoch = 5, Best score = 0.4670\n[22:15:37.815093] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[22:15:39.303491] Epoch: [6]  [  0/969]  eta: 0:24:00  lr: 0.000188  loss: 0.6541 (0.6541)  time: 1.4868  data: 0.8644  max mem: 6820\n[22:15:50.677316] Epoch: [6]  [ 20/969]  eta: 0:09:41  lr: 0.000188  loss: 0.6894 (0.6868)  time: 0.5686  data: 0.0002  max mem: 6820\n[22:16:02.182598] Epoch: [6]  [ 40/969]  eta: 0:09:12  lr: 0.000189  loss: 0.6780 (0.6874)  time: 0.5752  data: 0.0002  max mem: 6820\n[22:16:13.752485] Epoch: [6]  [ 60/969]  eta: 0:08:55  lr: 0.000189  loss: 0.6844 (0.6885)  time: 0.5784  data: 0.0002  max mem: 6820\n[22:16:25.258913] Epoch: [6]  [ 80/969]  eta: 0:08:40  lr: 0.000190  loss: 0.6829 (0.6873)  time: 0.5753  data: 0.0002  max mem: 6820\n[22:16:36.707622] Epoch: [6]  [100/969]  eta: 0:08:26  lr: 0.000191  loss: 0.6817 (0.6866)  time: 0.5724  data: 0.0002  max mem: 6820\n[22:16:48.080821] Epoch: [6]  [120/969]  eta: 0:08:12  lr: 0.000191  loss: 0.6999 (0.6887)  time: 0.5686  data: 0.0002  max mem: 6820\n[22:16:59.428758] Epoch: [6]  [140/969]  eta: 0:07:59  lr: 0.000192  loss: 0.6840 (0.6903)  time: 0.5673  data: 0.0002  max mem: 6820\n[22:17:10.811663] Epoch: [6]  [160/969]  eta: 0:07:47  lr: 0.000193  loss: 0.6807 (0.6893)  time: 0.5691  data: 0.0002  max mem: 6820\n[22:17:22.228243] Epoch: [6]  [180/969]  eta: 0:07:35  lr: 0.000193  loss: 0.6642 (0.6887)  time: 0.5708  data: 0.0002  max mem: 6820\n[22:17:33.663762] Epoch: [6]  [200/969]  eta: 0:07:23  lr: 0.000194  loss: 0.6961 (0.6898)  time: 0.5717  data: 0.0002  max mem: 6820\n[22:17:45.144411] Epoch: [6]  [220/969]  eta: 0:07:11  lr: 0.000195  loss: 0.6895 (0.6900)  time: 0.5740  data: 0.0002  max mem: 6820\n[22:17:56.616538] Epoch: [6]  [240/969]  eta: 0:06:59  lr: 0.000195  loss: 0.6856 (0.6901)  time: 0.5736  data: 0.0002  max mem: 6820\n[22:18:08.121058] Epoch: [6]  [260/969]  eta: 0:06:48  lr: 0.000196  loss: 0.6968 (0.6908)  time: 0.5752  data: 0.0002  max mem: 6820\n[22:18:19.577754] Epoch: [6]  [280/969]  eta: 0:06:36  lr: 0.000197  loss: 0.6989 (0.6918)  time: 0.5728  data: 0.0002  max mem: 6820\n[22:18:31.032210] Epoch: [6]  [300/969]  eta: 0:06:24  lr: 0.000197  loss: 0.6816 (0.6915)  time: 0.5727  data: 0.0002  max mem: 6820\n[22:18:42.454790] Epoch: [6]  [320/969]  eta: 0:06:13  lr: 0.000198  loss: 0.6755 (0.6907)  time: 0.5711  data: 0.0002  max mem: 6820\n[22:18:53.881674] Epoch: [6]  [340/969]  eta: 0:06:01  lr: 0.000198  loss: 0.6813 (0.6913)  time: 0.5713  data: 0.0002  max mem: 6820\n[22:19:05.273418] Epoch: [6]  [360/969]  eta: 0:05:49  lr: 0.000199  loss: 0.6763 (0.6913)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:19:16.674581] Epoch: [6]  [380/969]  eta: 0:05:38  lr: 0.000200  loss: 0.6956 (0.6915)  time: 0.5700  data: 0.0002  max mem: 6820\n[22:19:28.071452] Epoch: [6]  [400/969]  eta: 0:05:26  lr: 0.000200  loss: 0.6708 (0.6907)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:19:39.473604] Epoch: [6]  [420/969]  eta: 0:05:15  lr: 0.000201  loss: 0.6904 (0.6908)  time: 0.5701  data: 0.0002  max mem: 6820\n[22:19:50.872918] Epoch: [6]  [440/969]  eta: 0:05:03  lr: 0.000202  loss: 0.6770 (0.6909)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:20:02.251913] Epoch: [6]  [460/969]  eta: 0:04:51  lr: 0.000202  loss: 0.6968 (0.6912)  time: 0.5689  data: 0.0002  max mem: 6820\n[22:20:13.653527] Epoch: [6]  [480/969]  eta: 0:04:40  lr: 0.000203  loss: 0.6820 (0.6910)  time: 0.5700  data: 0.0002  max mem: 6820\n[22:20:25.057162] Epoch: [6]  [500/969]  eta: 0:04:28  lr: 0.000204  loss: 0.6804 (0.6911)  time: 0.5701  data: 0.0002  max mem: 6820\n[22:20:36.462197] Epoch: [6]  [520/969]  eta: 0:04:17  lr: 0.000204  loss: 0.6765 (0.6905)  time: 0.5702  data: 0.0002  max mem: 6820\n[22:20:47.857255] Epoch: [6]  [540/969]  eta: 0:04:05  lr: 0.000205  loss: 0.6759 (0.6902)  time: 0.5697  data: 0.0002  max mem: 6820\n[22:20:59.239429] Epoch: [6]  [560/969]  eta: 0:03:54  lr: 0.000206  loss: 0.6765 (0.6896)  time: 0.5691  data: 0.0002  max mem: 6820\n[22:21:10.638239] Epoch: [6]  [580/969]  eta: 0:03:42  lr: 0.000206  loss: 0.6836 (0.6895)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:21:22.010224] Epoch: [6]  [600/969]  eta: 0:03:31  lr: 0.000207  loss: 0.6884 (0.6897)  time: 0.5685  data: 0.0002  max mem: 6820\n[22:21:33.408676] Epoch: [6]  [620/969]  eta: 0:03:19  lr: 0.000207  loss: 0.6972 (0.6900)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:21:44.792140] Epoch: [6]  [640/969]  eta: 0:03:08  lr: 0.000208  loss: 0.6930 (0.6903)  time: 0.5691  data: 0.0003  max mem: 6820\n[22:21:56.181858] Epoch: [6]  [660/969]  eta: 0:02:56  lr: 0.000209  loss: 0.6892 (0.6902)  time: 0.5694  data: 0.0002  max mem: 6820\n[22:22:07.566810] Epoch: [6]  [680/969]  eta: 0:02:45  lr: 0.000209  loss: 0.6892 (0.6902)  time: 0.5692  data: 0.0002  max mem: 6820\n[22:22:18.957457] Epoch: [6]  [700/969]  eta: 0:02:33  lr: 0.000210  loss: 0.7007 (0.6904)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:22:30.324191] Epoch: [6]  [720/969]  eta: 0:02:22  lr: 0.000211  loss: 0.6926 (0.6903)  time: 0.5683  data: 0.0002  max mem: 6820\n[22:22:41.724123] Epoch: [6]  [740/969]  eta: 0:02:10  lr: 0.000211  loss: 0.6807 (0.6900)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:22:53.125001] Epoch: [6]  [760/969]  eta: 0:01:59  lr: 0.000212  loss: 0.7021 (0.6902)  time: 0.5700  data: 0.0002  max mem: 6820\n[22:23:04.508241] Epoch: [6]  [780/969]  eta: 0:01:48  lr: 0.000213  loss: 0.6855 (0.6899)  time: 0.5691  data: 0.0002  max mem: 6820\n[22:23:15.899636] Epoch: [6]  [800/969]  eta: 0:01:36  lr: 0.000213  loss: 0.7111 (0.6902)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:23:27.287304] Epoch: [6]  [820/969]  eta: 0:01:25  lr: 0.000214  loss: 0.6896 (0.6906)  time: 0.5693  data: 0.0002  max mem: 6820\n[22:23:38.678421] Epoch: [6]  [840/969]  eta: 0:01:13  lr: 0.000215  loss: 0.6852 (0.6905)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:23:50.065391] Epoch: [6]  [860/969]  eta: 0:01:02  lr: 0.000215  loss: 0.6786 (0.6904)  time: 0.5693  data: 0.0002  max mem: 6820\n[22:24:01.461544] Epoch: [6]  [880/969]  eta: 0:00:50  lr: 0.000216  loss: 0.6872 (0.6904)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:24:12.851801] Epoch: [6]  [900/969]  eta: 0:00:39  lr: 0.000217  loss: 0.7002 (0.6905)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:24:24.232112] Epoch: [6]  [920/969]  eta: 0:00:28  lr: 0.000217  loss: 0.6818 (0.6905)  time: 0.5690  data: 0.0002  max mem: 6820\n[22:24:35.601687] Epoch: [6]  [940/969]  eta: 0:00:16  lr: 0.000218  loss: 0.6803 (0.6905)  time: 0.5684  data: 0.0002  max mem: 6820\n[22:24:46.985505] Epoch: [6]  [960/969]  eta: 0:00:05  lr: 0.000218  loss: 0.6979 (0.6906)  time: 0.5691  data: 0.0002  max mem: 6820\n[22:24:51.537096] Epoch: [6]  [968/969]  eta: 0:00:00  lr: 0.000219  loss: 0.6707 (0.6904)  time: 0.5690  data: 0.0002  max mem: 6820\n[22:24:51.656638] Epoch: [6] Total time: 0:09:13 (0.5716 s / it)\n[22:24:51.656748] Averaged stats: lr: 0.000219  loss: 0.6707 (0.6904)\n[22:24:52.586676] val:  [  0/139]  eta: 0:02:08  loss: 0.6581 (0.6581)  time: 0.9256  data: 0.7915  max mem: 6820\n[22:24:54.088248] val:  [ 10/139]  eta: 0:00:28  loss: 0.6093 (0.6084)  time: 0.2206  data: 0.0721  max mem: 6820\n[22:24:55.582471] val:  [ 20/139]  eta: 0:00:22  loss: 0.5911 (0.6038)  time: 0.1497  data: 0.0002  max mem: 6820\n[22:24:57.085162] val:  [ 30/139]  eta: 0:00:19  loss: 0.6131 (0.6076)  time: 0.1498  data: 0.0002  max mem: 6820\n[22:24:58.593467] val:  [ 40/139]  eta: 0:00:16  loss: 0.6270 (0.6150)  time: 0.1505  data: 0.0002  max mem: 6820\n[22:25:00.103747] val:  [ 50/139]  eta: 0:00:14  loss: 0.6337 (0.6195)  time: 0.1509  data: 0.0002  max mem: 6820\n[22:25:01.612258] val:  [ 60/139]  eta: 0:00:12  loss: 0.6384 (0.6224)  time: 0.1509  data: 0.0002  max mem: 6820\n[22:25:03.121401] val:  [ 70/139]  eta: 0:00:11  loss: 0.6494 (0.6284)  time: 0.1508  data: 0.0002  max mem: 6820\n[22:25:04.632199] val:  [ 80/139]  eta: 0:00:09  loss: 0.6938 (0.6424)  time: 0.1509  data: 0.0002  max mem: 6820\n[22:25:06.148086] val:  [ 90/139]  eta: 0:00:07  loss: 0.7422 (0.6531)  time: 0.1513  data: 0.0002  max mem: 6820\n[22:25:07.661893] val:  [100/139]  eta: 0:00:06  loss: 0.7460 (0.6623)  time: 0.1514  data: 0.0002  max mem: 6820\n[22:25:09.172425] val:  [110/139]  eta: 0:00:04  loss: 0.7341 (0.6681)  time: 0.1511  data: 0.0002  max mem: 6820\n[22:25:10.676131] val:  [120/139]  eta: 0:00:02  loss: 0.7129 (0.6716)  time: 0.1506  data: 0.0002  max mem: 6820\n[22:25:12.190949] val:  [130/139]  eta: 0:00:01  loss: 0.7137 (0.6752)  time: 0.1509  data: 0.0001  max mem: 6820\n[22:25:13.310561] val:  [138/139]  eta: 0:00:00  loss: 0.7210 (0.6777)  time: 0.1468  data: 0.0001  max mem: 6820\n[22:25:13.417609] val: Total time: 0:00:21 (0.1565 s / it)\n[22:25:13.471227] val loss: 0.6776621427467401\n[22:25:13.471281] Accuracy: 0.5574, F1 Score: 0.5447, ROC AUC: 0.6207, Hamming Loss: 0.4426,\n Jaccard Score: 0.3781, Precision: 0.5646, Recall: 0.5574,\n Average Precision: 0.6029, Kappa: 0.1148, Score: 0.4267\n[22:25:13.473084] Best epoch = 5, Best score = 0.4670\n[22:25:13.475673] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[22:25:14.840164] Epoch: [7]  [  0/969]  eta: 0:22:00  lr: 0.000219  loss: 0.7020 (0.7020)  time: 1.3632  data: 0.7818  max mem: 6820\n[22:25:26.217160] Epoch: [7]  [ 20/969]  eta: 0:09:35  lr: 0.000219  loss: 0.6853 (0.6887)  time: 0.5688  data: 0.0002  max mem: 6820\n[22:25:37.651603] Epoch: [7]  [ 40/969]  eta: 0:09:07  lr: 0.000220  loss: 0.6886 (0.6911)  time: 0.5717  data: 0.0002  max mem: 6820\n[22:25:49.104506] Epoch: [7]  [ 60/969]  eta: 0:08:50  lr: 0.000221  loss: 0.6841 (0.6887)  time: 0.5726  data: 0.0002  max mem: 6820\n[22:26:00.551048] Epoch: [7]  [ 80/969]  eta: 0:08:36  lr: 0.000221  loss: 0.6890 (0.6887)  time: 0.5723  data: 0.0002  max mem: 6820\n[22:26:12.026866] Epoch: [7]  [100/969]  eta: 0:08:23  lr: 0.000222  loss: 0.6791 (0.6881)  time: 0.5737  data: 0.0002  max mem: 6820\n[22:26:23.495143] Epoch: [7]  [120/969]  eta: 0:08:11  lr: 0.000223  loss: 0.7010 (0.6905)  time: 0.5734  data: 0.0002  max mem: 6820\n[22:26:34.977869] Epoch: [7]  [140/969]  eta: 0:07:59  lr: 0.000223  loss: 0.6979 (0.6912)  time: 0.5741  data: 0.0002  max mem: 6820\n[22:26:46.438062] Epoch: [7]  [160/969]  eta: 0:07:47  lr: 0.000224  loss: 0.6807 (0.6895)  time: 0.5730  data: 0.0002  max mem: 6820\n[22:26:57.911211] Epoch: [7]  [180/969]  eta: 0:07:35  lr: 0.000225  loss: 0.6681 (0.6883)  time: 0.5736  data: 0.0002  max mem: 6820\n[22:27:09.373655] Epoch: [7]  [200/969]  eta: 0:07:23  lr: 0.000225  loss: 0.7013 (0.6893)  time: 0.5731  data: 0.0002  max mem: 6820\n[22:27:20.820459] Epoch: [7]  [220/969]  eta: 0:07:11  lr: 0.000226  loss: 0.6768 (0.6892)  time: 0.5723  data: 0.0002  max mem: 6820\n[22:27:32.239753] Epoch: [7]  [240/969]  eta: 0:06:59  lr: 0.000226  loss: 0.6903 (0.6896)  time: 0.5709  data: 0.0002  max mem: 6820\n[22:27:43.655770] Epoch: [7]  [260/969]  eta: 0:06:47  lr: 0.000227  loss: 0.6935 (0.6895)  time: 0.5707  data: 0.0002  max mem: 6820\n[22:27:55.052727] Epoch: [7]  [280/969]  eta: 0:06:36  lr: 0.000228  loss: 0.6973 (0.6899)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:28:06.452689] Epoch: [7]  [300/969]  eta: 0:06:24  lr: 0.000228  loss: 0.6848 (0.6901)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:28:17.845117] Epoch: [7]  [320/969]  eta: 0:06:12  lr: 0.000229  loss: 0.6837 (0.6898)  time: 0.5696  data: 0.0002  max mem: 6820\n[22:28:29.239533] Epoch: [7]  [340/969]  eta: 0:06:01  lr: 0.000230  loss: 0.6906 (0.6897)  time: 0.5697  data: 0.0002  max mem: 6820\n[22:28:40.614013] Epoch: [7]  [360/969]  eta: 0:05:49  lr: 0.000230  loss: 0.6844 (0.6907)  time: 0.5687  data: 0.0002  max mem: 6820\n[22:28:52.002266] Epoch: [7]  [380/969]  eta: 0:05:37  lr: 0.000231  loss: 0.6814 (0.6907)  time: 0.5694  data: 0.0002  max mem: 6820\n[22:29:03.372195] Epoch: [7]  [400/969]  eta: 0:05:26  lr: 0.000232  loss: 0.6830 (0.6905)  time: 0.5684  data: 0.0002  max mem: 6820\n[22:29:14.728847] Epoch: [7]  [420/969]  eta: 0:05:14  lr: 0.000232  loss: 0.6914 (0.6910)  time: 0.5678  data: 0.0002  max mem: 6820\n[22:29:26.117034] Epoch: [7]  [440/969]  eta: 0:05:03  lr: 0.000233  loss: 0.6929 (0.6911)  time: 0.5694  data: 0.0002  max mem: 6820\n[22:29:37.489745] Epoch: [7]  [460/969]  eta: 0:04:51  lr: 0.000234  loss: 0.6723 (0.6902)  time: 0.5686  data: 0.0002  max mem: 6820\n[22:29:48.895999] Epoch: [7]  [480/969]  eta: 0:04:39  lr: 0.000234  loss: 0.6892 (0.6903)  time: 0.5703  data: 0.0002  max mem: 6820\n[22:30:00.287087] Epoch: [7]  [500/969]  eta: 0:04:28  lr: 0.000235  loss: 0.6758 (0.6902)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:30:11.622228] Epoch: [7]  [520/969]  eta: 0:04:16  lr: 0.000236  loss: 0.6733 (0.6895)  time: 0.5667  data: 0.0002  max mem: 6820\n[22:30:22.990234] Epoch: [7]  [540/969]  eta: 0:04:05  lr: 0.000236  loss: 0.6753 (0.6888)  time: 0.5683  data: 0.0002  max mem: 6820\n[22:30:34.365462] Epoch: [7]  [560/969]  eta: 0:03:53  lr: 0.000237  loss: 0.6967 (0.6895)  time: 0.5687  data: 0.0002  max mem: 6820\n[22:30:45.731350] Epoch: [7]  [580/969]  eta: 0:03:42  lr: 0.000237  loss: 0.6821 (0.6893)  time: 0.5682  data: 0.0003  max mem: 6820\n[22:30:57.147236] Epoch: [7]  [600/969]  eta: 0:03:30  lr: 0.000238  loss: 0.6912 (0.6896)  time: 0.5707  data: 0.0002  max mem: 6820\n[22:31:08.522728] Epoch: [7]  [620/969]  eta: 0:03:19  lr: 0.000239  loss: 0.6895 (0.6899)  time: 0.5687  data: 0.0002  max mem: 6820\n[22:31:19.919994] Epoch: [7]  [640/969]  eta: 0:03:08  lr: 0.000239  loss: 0.6994 (0.6901)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:31:31.335424] Epoch: [7]  [660/969]  eta: 0:02:56  lr: 0.000240  loss: 0.6802 (0.6898)  time: 0.5707  data: 0.0002  max mem: 6820\n[22:31:42.774679] Epoch: [7]  [680/969]  eta: 0:02:45  lr: 0.000241  loss: 0.6958 (0.6900)  time: 0.5719  data: 0.0002  max mem: 6820\n[22:31:54.227885] Epoch: [7]  [700/969]  eta: 0:02:33  lr: 0.000241  loss: 0.6774 (0.6898)  time: 0.5726  data: 0.0002  max mem: 6820\n[22:32:05.700735] Epoch: [7]  [720/969]  eta: 0:02:22  lr: 0.000242  loss: 0.6782 (0.6898)  time: 0.5736  data: 0.0002  max mem: 6820\n[22:32:17.163510] Epoch: [7]  [740/969]  eta: 0:02:10  lr: 0.000243  loss: 0.6786 (0.6897)  time: 0.5731  data: 0.0002  max mem: 6820\n[22:32:28.583218] Epoch: [7]  [760/969]  eta: 0:01:59  lr: 0.000243  loss: 0.6908 (0.6898)  time: 0.5709  data: 0.0002  max mem: 6820\n[22:32:39.997235] Epoch: [7]  [780/969]  eta: 0:01:48  lr: 0.000244  loss: 0.6732 (0.6896)  time: 0.5706  data: 0.0002  max mem: 6820\n[22:32:51.418417] Epoch: [7]  [800/969]  eta: 0:01:36  lr: 0.000245  loss: 0.6979 (0.6899)  time: 0.5710  data: 0.0002  max mem: 6820\n[22:33:02.818313] Epoch: [7]  [820/969]  eta: 0:01:25  lr: 0.000245  loss: 0.6802 (0.6900)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:33:14.236469] Epoch: [7]  [840/969]  eta: 0:01:13  lr: 0.000246  loss: 0.6703 (0.6900)  time: 0.5708  data: 0.0003  max mem: 6820\n[22:33:25.620822] Epoch: [7]  [860/969]  eta: 0:01:02  lr: 0.000246  loss: 0.6736 (0.6896)  time: 0.5692  data: 0.0002  max mem: 6820\n[22:33:37.022220] Epoch: [7]  [880/969]  eta: 0:00:50  lr: 0.000247  loss: 0.6705 (0.6892)  time: 0.5700  data: 0.0002  max mem: 6820\n[22:33:48.413076] Epoch: [7]  [900/969]  eta: 0:00:39  lr: 0.000248  loss: 0.6864 (0.6894)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:33:59.805085] Epoch: [7]  [920/969]  eta: 0:00:27  lr: 0.000248  loss: 0.6922 (0.6895)  time: 0.5695  data: 0.0002  max mem: 6820\n[22:34:11.170492] Epoch: [7]  [940/969]  eta: 0:00:16  lr: 0.000249  loss: 0.6837 (0.6894)  time: 0.5682  data: 0.0002  max mem: 6820\n[22:34:22.536562] Epoch: [7]  [960/969]  eta: 0:00:05  lr: 0.000250  loss: 0.7004 (0.6897)  time: 0.5683  data: 0.0002  max mem: 6820\n[22:34:27.074059] Epoch: [7]  [968/969]  eta: 0:00:00  lr: 0.000250  loss: 0.7053 (0.6898)  time: 0.5673  data: 0.0002  max mem: 6820\n[22:34:27.200010] Epoch: [7] Total time: 0:09:13 (0.5714 s / it)\n[22:34:27.200131] Averaged stats: lr: 0.000250  loss: 0.7053 (0.6898)\n[22:34:28.055016] val:  [  0/139]  eta: 0:01:57  loss: 0.6432 (0.6432)  time: 0.8467  data: 0.6705  max mem: 6820\n[22:34:29.537088] val:  [ 10/139]  eta: 0:00:27  loss: 0.6403 (0.6206)  time: 0.2116  data: 0.0611  max mem: 6820\n[22:34:31.039019] val:  [ 20/139]  eta: 0:00:21  loss: 0.6071 (0.6165)  time: 0.1491  data: 0.0002  max mem: 6820\n[22:34:32.538874] val:  [ 30/139]  eta: 0:00:18  loss: 0.6118 (0.6170)  time: 0.1500  data: 0.0002  max mem: 6820\n[22:34:34.044158] val:  [ 40/139]  eta: 0:00:16  loss: 0.6328 (0.6229)  time: 0.1502  data: 0.0002  max mem: 6820\n[22:34:35.553133] val:  [ 50/139]  eta: 0:00:14  loss: 0.6339 (0.6263)  time: 0.1506  data: 0.0002  max mem: 6820\n[22:34:37.063769] val:  [ 60/139]  eta: 0:00:12  loss: 0.6377 (0.6285)  time: 0.1509  data: 0.0002  max mem: 6820\n[22:34:38.575011] val:  [ 70/139]  eta: 0:00:11  loss: 0.6492 (0.6335)  time: 0.1510  data: 0.0002  max mem: 6820\n[22:34:40.079246] val:  [ 80/139]  eta: 0:00:09  loss: 0.7154 (0.6460)  time: 0.1507  data: 0.0002  max mem: 6820\n[22:34:41.591796] val:  [ 90/139]  eta: 0:00:07  loss: 0.7303 (0.6564)  time: 0.1508  data: 0.0002  max mem: 6820\n[22:34:43.105209] val:  [100/139]  eta: 0:00:06  loss: 0.7418 (0.6650)  time: 0.1512  data: 0.0002  max mem: 6820\n[22:34:44.614661] val:  [110/139]  eta: 0:00:04  loss: 0.7341 (0.6708)  time: 0.1511  data: 0.0002  max mem: 6820\n[22:34:46.120430] val:  [120/139]  eta: 0:00:02  loss: 0.7244 (0.6751)  time: 0.1507  data: 0.0002  max mem: 6820\n[22:34:47.634039] val:  [130/139]  eta: 0:00:01  loss: 0.7144 (0.6782)  time: 0.1509  data: 0.0002  max mem: 6820\n[22:34:48.748112] val:  [138/139]  eta: 0:00:00  loss: 0.7134 (0.6807)  time: 0.1463  data: 0.0002  max mem: 6820\n[22:34:48.856874] val: Total time: 0:00:21 (0.1558 s / it)\n[22:34:48.912039] val loss: 0.6807258660844762\n[22:34:48.912092] Accuracy: 0.5556, F1 Score: 0.5165, ROC AUC: 0.6358, Hamming Loss: 0.4444,\n Jaccard Score: 0.3599, Precision: 0.5822, Recall: 0.5556,\n Average Precision: 0.6227, Kappa: 0.1112, Score: 0.4212\n[22:34:48.913854] Best epoch = 5, Best score = 0.4670\n[22:34:48.916400] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[22:34:50.241645] Epoch: [8]  [  0/969]  eta: 0:21:22  lr: 0.000250  loss: 0.7137 (0.7137)  time: 1.3238  data: 0.7424  max mem: 6820\n[22:35:01.578994] Epoch: [8]  [ 20/969]  eta: 0:09:32  lr: 0.000251  loss: 0.6785 (0.6831)  time: 0.5668  data: 0.0002  max mem: 6820\n[22:35:12.954858] Epoch: [8]  [ 40/969]  eta: 0:09:04  lr: 0.000251  loss: 0.6779 (0.6866)  time: 0.5687  data: 0.0002  max mem: 6820\n[22:35:24.394751] Epoch: [8]  [ 60/969]  eta: 0:08:48  lr: 0.000252  loss: 0.6860 (0.6852)  time: 0.5719  data: 0.0002  max mem: 6820\n[22:35:35.831318] Epoch: [8]  [ 80/969]  eta: 0:08:34  lr: 0.000253  loss: 0.6848 (0.6849)  time: 0.5718  data: 0.0002  max mem: 6820\n[22:35:47.188194] Epoch: [8]  [100/969]  eta: 0:08:21  lr: 0.000253  loss: 0.6791 (0.6839)  time: 0.5678  data: 0.0002  max mem: 6820\n[22:35:58.645306] Epoch: [8]  [120/969]  eta: 0:08:09  lr: 0.000254  loss: 0.6769 (0.6839)  time: 0.5728  data: 0.0002  max mem: 6820\n[22:36:10.116487] Epoch: [8]  [140/969]  eta: 0:07:57  lr: 0.000255  loss: 0.6936 (0.6869)  time: 0.5735  data: 0.0002  max mem: 6820\n[22:36:21.580233] Epoch: [8]  [160/969]  eta: 0:07:45  lr: 0.000255  loss: 0.6895 (0.6873)  time: 0.5731  data: 0.0001  max mem: 6820\n[22:36:33.056567] Epoch: [8]  [180/969]  eta: 0:07:33  lr: 0.000256  loss: 0.6872 (0.6874)  time: 0.5738  data: 0.0002  max mem: 6820\n[22:36:44.516738] Epoch: [8]  [200/969]  eta: 0:07:22  lr: 0.000256  loss: 0.6998 (0.6879)  time: 0.5730  data: 0.0002  max mem: 6820\n[22:36:55.962585] Epoch: [8]  [220/969]  eta: 0:07:10  lr: 0.000257  loss: 0.6818 (0.6873)  time: 0.5722  data: 0.0002  max mem: 6820\n[22:37:07.426642] Epoch: [8]  [240/969]  eta: 0:06:58  lr: 0.000258  loss: 0.7050 (0.6892)  time: 0.5731  data: 0.0002  max mem: 6820\n[22:37:18.867964] Epoch: [8]  [260/969]  eta: 0:06:47  lr: 0.000258  loss: 0.6772 (0.6889)  time: 0.5720  data: 0.0002  max mem: 6820\n[22:37:30.305774] Epoch: [8]  [280/969]  eta: 0:06:35  lr: 0.000259  loss: 0.6813 (0.6888)  time: 0.5718  data: 0.0002  max mem: 6820\n[22:37:41.749802] Epoch: [8]  [300/969]  eta: 0:06:24  lr: 0.000260  loss: 0.6897 (0.6886)  time: 0.5721  data: 0.0002  max mem: 6820\n[22:37:53.189774] Epoch: [8]  [320/969]  eta: 0:06:12  lr: 0.000260  loss: 0.6796 (0.6885)  time: 0.5719  data: 0.0002  max mem: 6820\n[22:38:04.634195] Epoch: [8]  [340/969]  eta: 0:06:00  lr: 0.000261  loss: 0.6852 (0.6889)  time: 0.5722  data: 0.0002  max mem: 6820\n[22:38:16.040875] Epoch: [8]  [360/969]  eta: 0:05:49  lr: 0.000262  loss: 0.6895 (0.6893)  time: 0.5703  data: 0.0002  max mem: 6820\n[22:38:27.445496] Epoch: [8]  [380/969]  eta: 0:05:37  lr: 0.000262  loss: 0.6938 (0.6894)  time: 0.5702  data: 0.0002  max mem: 6820\n[22:38:38.844437] Epoch: [8]  [400/969]  eta: 0:05:26  lr: 0.000263  loss: 0.6846 (0.6893)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:38:50.241124] Epoch: [8]  [420/969]  eta: 0:05:14  lr: 0.000264  loss: 0.6756 (0.6893)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:39:01.643022] Epoch: [8]  [440/969]  eta: 0:05:03  lr: 0.000264  loss: 0.6772 (0.6896)  time: 0.5700  data: 0.0002  max mem: 6820\n[22:39:13.015208] Epoch: [8]  [460/969]  eta: 0:04:51  lr: 0.000265  loss: 0.6821 (0.6895)  time: 0.5686  data: 0.0002  max mem: 6820\n[22:39:24.408703] Epoch: [8]  [480/969]  eta: 0:04:40  lr: 0.000265  loss: 0.6954 (0.6896)  time: 0.5696  data: 0.0002  max mem: 6820\n[22:39:35.807612] Epoch: [8]  [500/969]  eta: 0:04:28  lr: 0.000266  loss: 0.6921 (0.6898)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:39:47.196142] Epoch: [8]  [520/969]  eta: 0:04:17  lr: 0.000267  loss: 0.6735 (0.6895)  time: 0.5694  data: 0.0002  max mem: 6820\n[22:39:58.604350] Epoch: [8]  [540/969]  eta: 0:04:05  lr: 0.000267  loss: 0.6632 (0.6886)  time: 0.5704  data: 0.0002  max mem: 6820\n[22:40:10.005579] Epoch: [8]  [560/969]  eta: 0:03:54  lr: 0.000268  loss: 0.6700 (0.6876)  time: 0.5700  data: 0.0002  max mem: 6820\n[22:40:21.378053] Epoch: [8]  [580/969]  eta: 0:03:42  lr: 0.000269  loss: 0.6937 (0.6883)  time: 0.5686  data: 0.0002  max mem: 6820\n[22:40:32.706264] Epoch: [8]  [600/969]  eta: 0:03:31  lr: 0.000269  loss: 0.6965 (0.6885)  time: 0.5663  data: 0.0002  max mem: 6820\n[22:40:44.055313] Epoch: [8]  [620/969]  eta: 0:03:19  lr: 0.000270  loss: 0.6943 (0.6889)  time: 0.5674  data: 0.0002  max mem: 6820\n[22:40:55.383948] Epoch: [8]  [640/969]  eta: 0:03:08  lr: 0.000271  loss: 0.6830 (0.6892)  time: 0.5664  data: 0.0002  max mem: 6820\n[22:41:06.728322] Epoch: [8]  [660/969]  eta: 0:02:56  lr: 0.000271  loss: 0.6862 (0.6890)  time: 0.5672  data: 0.0002  max mem: 6820\n[22:41:18.088121] Epoch: [8]  [680/969]  eta: 0:02:45  lr: 0.000272  loss: 0.6966 (0.6894)  time: 0.5679  data: 0.0002  max mem: 6820\n[22:41:29.444821] Epoch: [8]  [700/969]  eta: 0:02:33  lr: 0.000273  loss: 0.6911 (0.6893)  time: 0.5678  data: 0.0002  max mem: 6820\n[22:41:40.797615] Epoch: [8]  [720/969]  eta: 0:02:22  lr: 0.000273  loss: 0.6941 (0.6895)  time: 0.5676  data: 0.0002  max mem: 6820\n[22:41:52.166428] Epoch: [8]  [740/969]  eta: 0:02:10  lr: 0.000274  loss: 0.6818 (0.6892)  time: 0.5684  data: 0.0002  max mem: 6820\n[22:42:03.540641] Epoch: [8]  [760/969]  eta: 0:01:59  lr: 0.000275  loss: 0.6738 (0.6888)  time: 0.5687  data: 0.0002  max mem: 6820\n[22:42:14.917581] Epoch: [8]  [780/969]  eta: 0:01:47  lr: 0.000275  loss: 0.6785 (0.6890)  time: 0.5688  data: 0.0002  max mem: 6820\n[22:42:26.261858] Epoch: [8]  [800/969]  eta: 0:01:36  lr: 0.000276  loss: 0.7003 (0.6893)  time: 0.5672  data: 0.0002  max mem: 6820\n[22:42:37.605010] Epoch: [8]  [820/969]  eta: 0:01:25  lr: 0.000276  loss: 0.6858 (0.6894)  time: 0.5671  data: 0.0001  max mem: 6820\n[22:42:48.948612] Epoch: [8]  [840/969]  eta: 0:01:13  lr: 0.000277  loss: 0.6891 (0.6894)  time: 0.5671  data: 0.0002  max mem: 6820\n[22:43:00.330042] Epoch: [8]  [860/969]  eta: 0:01:02  lr: 0.000278  loss: 0.6991 (0.6896)  time: 0.5690  data: 0.0002  max mem: 6820\n[22:43:11.658946] Epoch: [8]  [880/969]  eta: 0:00:50  lr: 0.000278  loss: 0.6852 (0.6895)  time: 0.5664  data: 0.0002  max mem: 6820\n[22:43:23.023895] Epoch: [8]  [900/969]  eta: 0:00:39  lr: 0.000279  loss: 0.6951 (0.6895)  time: 0.5682  data: 0.0002  max mem: 6820\n[22:43:34.388385] Epoch: [8]  [920/969]  eta: 0:00:27  lr: 0.000280  loss: 0.6896 (0.6896)  time: 0.5682  data: 0.0002  max mem: 6820\n[22:43:45.767254] Epoch: [8]  [940/969]  eta: 0:00:16  lr: 0.000280  loss: 0.6786 (0.6894)  time: 0.5689  data: 0.0002  max mem: 6820\n[22:43:57.148251] Epoch: [8]  [960/969]  eta: 0:00:05  lr: 0.000281  loss: 0.6929 (0.6896)  time: 0.5690  data: 0.0002  max mem: 6820\n[22:44:01.678511] Epoch: [8]  [968/969]  eta: 0:00:00  lr: 0.000281  loss: 0.6990 (0.6896)  time: 0.5683  data: 0.0001  max mem: 6820\n[22:44:01.794506] Epoch: [8] Total time: 0:09:12 (0.5706 s / it)\n[22:44:01.794624] Averaged stats: lr: 0.000281  loss: 0.6990 (0.6896)\n[22:44:02.727590] val:  [  0/139]  eta: 0:02:09  loss: 0.6336 (0.6336)  time: 0.9284  data: 0.7898  max mem: 6820\n[22:44:04.220340] val:  [ 10/139]  eta: 0:00:28  loss: 0.6167 (0.6016)  time: 0.2200  data: 0.0720  max mem: 6820\n[22:44:05.718883] val:  [ 20/139]  eta: 0:00:22  loss: 0.5860 (0.5993)  time: 0.1495  data: 0.0002  max mem: 6820\n[22:44:07.225863] val:  [ 30/139]  eta: 0:00:19  loss: 0.6003 (0.6005)  time: 0.1502  data: 0.0002  max mem: 6820\n[22:44:08.727839] val:  [ 40/139]  eta: 0:00:16  loss: 0.6118 (0.6065)  time: 0.1504  data: 0.0002  max mem: 6820\n[22:44:10.233915] val:  [ 50/139]  eta: 0:00:14  loss: 0.6170 (0.6099)  time: 0.1503  data: 0.0002  max mem: 6820\n[22:44:11.747507] val:  [ 60/139]  eta: 0:00:12  loss: 0.6269 (0.6130)  time: 0.1509  data: 0.0002  max mem: 6820\n[22:44:13.257920] val:  [ 70/139]  eta: 0:00:11  loss: 0.6351 (0.6189)  time: 0.1511  data: 0.0002  max mem: 6820\n[22:44:14.764745] val:  [ 80/139]  eta: 0:00:09  loss: 0.7275 (0.6352)  time: 0.1508  data: 0.0002  max mem: 6820\n[22:44:16.279466] val:  [ 90/139]  eta: 0:00:07  loss: 0.7534 (0.6485)  time: 0.1510  data: 0.0002  max mem: 6820\n[22:44:17.786200] val:  [100/139]  eta: 0:00:06  loss: 0.7567 (0.6593)  time: 0.1510  data: 0.0002  max mem: 6820\n[22:44:19.298821] val:  [110/139]  eta: 0:00:04  loss: 0.7511 (0.6671)  time: 0.1509  data: 0.0002  max mem: 6820\n[22:44:20.811659] val:  [120/139]  eta: 0:00:02  loss: 0.7382 (0.6729)  time: 0.1512  data: 0.0002  max mem: 6820\n[22:44:22.323739] val:  [130/139]  eta: 0:00:01  loss: 0.7338 (0.6775)  time: 0.1512  data: 0.0001  max mem: 6820\n[22:44:23.437785] val:  [138/139]  eta: 0:00:00  loss: 0.7312 (0.6810)  time: 0.1463  data: 0.0001  max mem: 6820\n[22:44:23.541572] val: Total time: 0:00:21 (0.1564 s / it)\n[22:44:23.600529] val loss: 0.6809558456750224\n[22:44:23.600613] Accuracy: 0.5533, F1 Score: 0.5070, ROC AUC: 0.6299, Hamming Loss: 0.4467,\n Jaccard Score: 0.3535, Precision: 0.5855, Recall: 0.5533,\n Average Precision: 0.6137, Kappa: 0.1067, Score: 0.4145\n[22:44:23.602567] Best epoch = 5, Best score = 0.4670\n[22:44:23.605463] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[22:44:24.921651] Epoch: [9]  [  0/969]  eta: 0:21:13  lr: 0.000281  loss: 0.6811 (0.6811)  time: 1.3143  data: 0.7275  max mem: 6820\n[22:44:36.249938] Epoch: [9]  [ 20/969]  eta: 0:09:31  lr: 0.000282  loss: 0.6899 (0.6901)  time: 0.5664  data: 0.0002  max mem: 6820\n[22:44:47.620631] Epoch: [9]  [ 40/969]  eta: 0:09:04  lr: 0.000283  loss: 0.6804 (0.6871)  time: 0.5685  data: 0.0002  max mem: 6820\n[22:44:59.033478] Epoch: [9]  [ 60/969]  eta: 0:08:47  lr: 0.000283  loss: 0.6814 (0.6855)  time: 0.5706  data: 0.0002  max mem: 6820\n[22:45:10.477298] Epoch: [9]  [ 80/969]  eta: 0:08:34  lr: 0.000284  loss: 0.6825 (0.6855)  time: 0.5721  data: 0.0002  max mem: 6820\n[22:45:21.927816] Epoch: [9]  [100/969]  eta: 0:08:21  lr: 0.000284  loss: 0.6893 (0.6873)  time: 0.5725  data: 0.0002  max mem: 6820\n[22:45:33.357196] Epoch: [9]  [120/969]  eta: 0:08:09  lr: 0.000285  loss: 0.6885 (0.6888)  time: 0.5714  data: 0.0002  max mem: 6820\n[22:45:44.773544] Epoch: [9]  [140/969]  eta: 0:07:57  lr: 0.000286  loss: 0.6953 (0.6897)  time: 0.5708  data: 0.0002  max mem: 6820\n[22:45:56.205535] Epoch: [9]  [160/969]  eta: 0:07:45  lr: 0.000286  loss: 0.6717 (0.6881)  time: 0.5715  data: 0.0002  max mem: 6820\n[22:46:07.631056] Epoch: [9]  [180/969]  eta: 0:07:33  lr: 0.000287  loss: 0.6969 (0.6885)  time: 0.5712  data: 0.0002  max mem: 6820\n[22:46:19.059936] Epoch: [9]  [200/969]  eta: 0:07:21  lr: 0.000288  loss: 0.6918 (0.6887)  time: 0.5714  data: 0.0002  max mem: 6820\n[22:46:30.465645] Epoch: [9]  [220/969]  eta: 0:07:09  lr: 0.000288  loss: 0.6866 (0.6885)  time: 0.5702  data: 0.0002  max mem: 6820\n[22:46:41.875918] Epoch: [9]  [240/969]  eta: 0:06:58  lr: 0.000289  loss: 0.6846 (0.6884)  time: 0.5705  data: 0.0002  max mem: 6820\n[22:46:53.292800] Epoch: [9]  [260/969]  eta: 0:06:46  lr: 0.000290  loss: 0.6976 (0.6880)  time: 0.5708  data: 0.0002  max mem: 6820\n[22:47:04.715256] Epoch: [9]  [280/969]  eta: 0:06:34  lr: 0.000290  loss: 0.6758 (0.6882)  time: 0.5711  data: 0.0002  max mem: 6820\n[22:47:16.114773] Epoch: [9]  [300/969]  eta: 0:06:23  lr: 0.000291  loss: 0.6955 (0.6882)  time: 0.5699  data: 0.0002  max mem: 6820\n[22:47:27.495862] Epoch: [9]  [320/969]  eta: 0:06:11  lr: 0.000292  loss: 0.6797 (0.6879)  time: 0.5690  data: 0.0002  max mem: 6820\n[22:47:38.915398] Epoch: [9]  [340/969]  eta: 0:06:00  lr: 0.000292  loss: 0.6847 (0.6884)  time: 0.5709  data: 0.0002  max mem: 6820\n[22:47:50.272451] Epoch: [9]  [360/969]  eta: 0:05:48  lr: 0.000293  loss: 0.6886 (0.6890)  time: 0.5678  data: 0.0002  max mem: 6820\n[22:48:01.652087] Epoch: [9]  [380/969]  eta: 0:05:37  lr: 0.000294  loss: 0.6774 (0.6888)  time: 0.5689  data: 0.0002  max mem: 6820\n[22:48:13.073572] Epoch: [9]  [400/969]  eta: 0:05:25  lr: 0.000294  loss: 0.6696 (0.6886)  time: 0.5710  data: 0.0002  max mem: 6820\n[22:48:24.470832] Epoch: [9]  [420/969]  eta: 0:05:14  lr: 0.000295  loss: 0.6658 (0.6888)  time: 0.5698  data: 0.0002  max mem: 6820\n[22:48:35.875611] Epoch: [9]  [440/969]  eta: 0:05:02  lr: 0.000295  loss: 0.6839 (0.6887)  time: 0.5702  data: 0.0002  max mem: 6820\n[22:48:47.274893] Epoch: [9]  [460/969]  eta: 0:04:51  lr: 0.000296  loss: 0.6762 (0.6886)  time: 0.5699  data: 0.0001  max mem: 6820\n[22:48:58.669156] Epoch: [9]  [480/969]  eta: 0:04:39  lr: 0.000297  loss: 0.6954 (0.6888)  time: 0.5697  data: 0.0002  max mem: 6820\n[22:49:10.022910] Epoch: [9]  [500/969]  eta: 0:04:28  lr: 0.000297  loss: 0.6868 (0.6889)  time: 0.5676  data: 0.0002  max mem: 6820\n[22:49:21.408745] Epoch: [9]  [520/969]  eta: 0:04:16  lr: 0.000298  loss: 0.6879 (0.6884)  time: 0.5692  data: 0.0002  max mem: 6820\n[22:49:32.809507] Epoch: [9]  [540/969]  eta: 0:04:05  lr: 0.000299  loss: 0.6815 (0.6882)  time: 0.5700  data: 0.0002  max mem: 6820\n[22:49:44.222826] Epoch: [9]  [560/969]  eta: 0:03:53  lr: 0.000299  loss: 0.6511 (0.6878)  time: 0.5706  data: 0.0002  max mem: 6820\n[22:49:55.636703] Epoch: [9]  [580/969]  eta: 0:03:42  lr: 0.000300  loss: 0.6817 (0.6876)  time: 0.5706  data: 0.0002  max mem: 6820\n[22:50:07.012603] Epoch: [9]  [600/969]  eta: 0:03:30  lr: 0.000301  loss: 0.6985 (0.6881)  time: 0.5687  data: 0.0003  max mem: 6820\n[22:50:18.338705] Epoch: [9]  [620/969]  eta: 0:03:19  lr: 0.000301  loss: 0.6945 (0.6885)  time: 0.5663  data: 0.0002  max mem: 6820\n[22:50:29.675837] Epoch: [9]  [640/969]  eta: 0:03:07  lr: 0.000302  loss: 0.6945 (0.6888)  time: 0.5668  data: 0.0002  max mem: 6820\n[22:50:41.028458] Epoch: [9]  [660/969]  eta: 0:02:56  lr: 0.000303  loss: 0.6847 (0.6886)  time: 0.5676  data: 0.0002  max mem: 6820\n[22:50:52.382372] Epoch: [9]  [680/969]  eta: 0:02:44  lr: 0.000303  loss: 0.6817 (0.6887)  time: 0.5676  data: 0.0002  max mem: 6820\n[22:51:03.763387] Epoch: [9]  [700/969]  eta: 0:02:33  lr: 0.000304  loss: 0.6862 (0.6886)  time: 0.5690  data: 0.0002  max mem: 6820\n[22:51:15.148913] Epoch: [9]  [720/969]  eta: 0:02:22  lr: 0.000304  loss: 0.6917 (0.6888)  time: 0.5692  data: 0.0002  max mem: 6820\n[22:51:26.513985] Epoch: [9]  [740/969]  eta: 0:02:10  lr: 0.000305  loss: 0.6762 (0.6887)  time: 0.5682  data: 0.0002  max mem: 6820\n[22:51:37.884393] Epoch: [9]  [760/969]  eta: 0:01:59  lr: 0.000306  loss: 0.6891 (0.6885)  time: 0.5685  data: 0.0002  max mem: 6820\n[22:51:49.287586] Epoch: [9]  [780/969]  eta: 0:01:47  lr: 0.000306  loss: 0.6725 (0.6888)  time: 0.5701  data: 0.0002  max mem: 6820\n[22:52:00.639132] Epoch: [9]  [800/969]  eta: 0:01:36  lr: 0.000307  loss: 0.6987 (0.6891)  time: 0.5675  data: 0.0002  max mem: 6820\n[22:52:12.015530] Epoch: [9]  [820/969]  eta: 0:01:25  lr: 0.000308  loss: 0.6930 (0.6892)  time: 0.5688  data: 0.0002  max mem: 6820\n[22:52:23.399084] Epoch: [9]  [840/969]  eta: 0:01:13  lr: 0.000308  loss: 0.6966 (0.6895)  time: 0.5691  data: 0.0002  max mem: 6820\n[22:52:34.732191] Epoch: [9]  [860/969]  eta: 0:01:02  lr: 0.000309  loss: 0.6857 (0.6894)  time: 0.5666  data: 0.0002  max mem: 6820\n[22:52:46.074845] Epoch: [9]  [880/969]  eta: 0:00:50  lr: 0.000310  loss: 0.6655 (0.6892)  time: 0.5671  data: 0.0002  max mem: 6820\n[22:52:57.381131] Epoch: [9]  [900/969]  eta: 0:00:39  lr: 0.000310  loss: 0.6941 (0.6894)  time: 0.5652  data: 0.0002  max mem: 6820\n[22:53:08.712342] Epoch: [9]  [920/969]  eta: 0:00:27  lr: 0.000311  loss: 0.6969 (0.6896)  time: 0.5665  data: 0.0002  max mem: 6820\n[22:53:20.037833] Epoch: [9]  [940/969]  eta: 0:00:16  lr: 0.000312  loss: 0.6908 (0.6897)  time: 0.5662  data: 0.0002  max mem: 6820\n[22:53:31.320713] Epoch: [9]  [960/969]  eta: 0:00:05  lr: 0.000312  loss: 0.6952 (0.6899)  time: 0.5641  data: 0.0002  max mem: 6820\n[22:53:35.828821] Epoch: [9]  [968/969]  eta: 0:00:00  lr: 0.000312  loss: 0.6947 (0.6900)  time: 0.5639  data: 0.0002  max mem: 6820\n[22:53:35.948648] Epoch: [9] Total time: 0:09:12 (0.5700 s / it)\n[22:53:35.948745] Averaged stats: lr: 0.000312  loss: 0.6947 (0.6900)\n[22:53:36.766698] val:  [  0/139]  eta: 0:01:53  loss: 0.6575 (0.6575)  time: 0.8135  data: 0.6799  max mem: 6820\n[22:53:38.262568] val:  [ 10/139]  eta: 0:00:27  loss: 0.6614 (0.6514)  time: 0.2099  data: 0.0620  max mem: 6820\n[22:53:39.754756] val:  [ 20/139]  eta: 0:00:21  loss: 0.6476 (0.6500)  time: 0.1493  data: 0.0002  max mem: 6820\n[22:53:41.264831] val:  [ 30/139]  eta: 0:00:18  loss: 0.6419 (0.6522)  time: 0.1500  data: 0.0002  max mem: 6820\n[22:53:42.775916] val:  [ 40/139]  eta: 0:00:16  loss: 0.6708 (0.6610)  time: 0.1510  data: 0.0002  max mem: 6820\n[22:53:44.288722] val:  [ 50/139]  eta: 0:00:14  loss: 0.6804 (0.6644)  time: 0.1511  data: 0.0002  max mem: 6820\n[22:53:45.804526] val:  [ 60/139]  eta: 0:00:12  loss: 0.6853 (0.6702)  time: 0.1514  data: 0.0002  max mem: 6820\n[22:53:47.318127] val:  [ 70/139]  eta: 0:00:11  loss: 0.7013 (0.6738)  time: 0.1514  data: 0.0002  max mem: 6820\n[22:53:48.828627] val:  [ 80/139]  eta: 0:00:09  loss: 0.7151 (0.6794)  time: 0.1511  data: 0.0002  max mem: 6820\n[22:53:50.347411] val:  [ 90/139]  eta: 0:00:07  loss: 0.6960 (0.6808)  time: 0.1514  data: 0.0002  max mem: 6820\n[22:53:51.863444] val:  [100/139]  eta: 0:00:06  loss: 0.6935 (0.6816)  time: 0.1517  data: 0.0002  max mem: 6820\n[22:53:53.377875] val:  [110/139]  eta: 0:00:04  loss: 0.6848 (0.6824)  time: 0.1514  data: 0.0002  max mem: 6820\n[22:53:54.888018] val:  [120/139]  eta: 0:00:02  loss: 0.7015 (0.6844)  time: 0.1512  data: 0.0002  max mem: 6820\n[22:53:56.400124] val:  [130/139]  eta: 0:00:01  loss: 0.7082 (0.6861)  time: 0.1510  data: 0.0001  max mem: 6820\n[22:53:57.517429] val:  [138/139]  eta: 0:00:00  loss: 0.6977 (0.6863)  time: 0.1463  data: 0.0001  max mem: 6820\n[22:53:57.619510] val: Total time: 0:00:21 (0.1559 s / it)\n[22:53:57.672343] val loss: 0.6862511943570144\n[22:53:57.672783] Accuracy: 0.5610, F1 Score: 0.5537, ROC AUC: 0.5963, Hamming Loss: 0.4390,\n Jaccard Score: 0.3850, Precision: 0.5653, Recall: 0.5610,\n Average Precision: 0.5858, Kappa: 0.1221, Score: 0.4240\n[22:53:57.674175] Best epoch = 5, Best score = 0.4670\n[22:53:57.676723] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[22:53:59.007273] Epoch: [10]  [  0/969]  eta: 0:21:28  lr: 0.000313  loss: 0.7082 (0.7082)  time: 1.3292  data: 0.7247  max mem: 6820\n[22:54:10.311494] Epoch: [10]  [ 20/969]  eta: 0:09:30  lr: 0.000312  loss: 0.6898 (0.6922)  time: 0.5651  data: 0.0002  max mem: 6820\n[22:54:21.619162] Epoch: [10]  [ 40/969]  eta: 0:09:02  lr: 0.000312  loss: 0.6948 (0.6931)  time: 0.5653  data: 0.0002  max mem: 6820\n[22:54:32.945154] Epoch: [10]  [ 60/969]  eta: 0:08:45  lr: 0.000312  loss: 0.6888 (0.6918)  time: 0.5663  data: 0.0002  max mem: 6820\n[22:54:44.260942] Epoch: [10]  [ 80/969]  eta: 0:08:31  lr: 0.000312  loss: 0.7009 (0.6935)  time: 0.5657  data: 0.0002  max mem: 6820\n[22:54:55.564088] Epoch: [10]  [100/969]  eta: 0:08:18  lr: 0.000312  loss: 0.6994 (0.6948)  time: 0.5651  data: 0.0002  max mem: 6820\n[22:55:06.883535] Epoch: [10]  [120/969]  eta: 0:08:05  lr: 0.000312  loss: 0.6918 (0.6950)  time: 0.5659  data: 0.0002  max mem: 6820\n[22:55:18.199220] Epoch: [10]  [140/969]  eta: 0:07:53  lr: 0.000312  loss: 0.6998 (0.6959)  time: 0.5657  data: 0.0002  max mem: 6820\n[22:55:29.526425] Epoch: [10]  [160/969]  eta: 0:07:41  lr: 0.000312  loss: 0.6969 (0.6964)  time: 0.5663  data: 0.0002  max mem: 6820\n[22:55:40.729093] Epoch: [10]  [180/969]  eta: 0:07:29  lr: 0.000312  loss: 0.6919 (0.6961)  time: 0.5601  data: 0.0002  max mem: 6820\n[22:55:52.003003] Epoch: [10]  [200/969]  eta: 0:07:17  lr: 0.000312  loss: 0.6866 (0.6963)  time: 0.5636  data: 0.0002  max mem: 6820\n[22:56:03.268699] Epoch: [10]  [220/969]  eta: 0:07:05  lr: 0.000312  loss: 0.6932 (0.6964)  time: 0.5632  data: 0.0002  max mem: 6820\n[22:56:14.539411] Epoch: [10]  [240/969]  eta: 0:06:53  lr: 0.000312  loss: 0.6936 (0.6965)  time: 0.5635  data: 0.0002  max mem: 6820\n[22:56:25.813658] Epoch: [10]  [260/969]  eta: 0:06:42  lr: 0.000312  loss: 0.6904 (0.6963)  time: 0.5637  data: 0.0002  max mem: 6820\n[22:56:37.078725] Epoch: [10]  [280/969]  eta: 0:06:30  lr: 0.000312  loss: 0.7026 (0.6966)  time: 0.5632  data: 0.0002  max mem: 6820\n[22:56:48.322405] Epoch: [10]  [300/969]  eta: 0:06:19  lr: 0.000312  loss: 0.6922 (0.6966)  time: 0.5621  data: 0.0002  max mem: 6820\n[22:56:59.588723] Epoch: [10]  [320/969]  eta: 0:06:07  lr: 0.000312  loss: 0.6901 (0.6964)  time: 0.5633  data: 0.0002  max mem: 6820\n[22:57:10.880492] Epoch: [10]  [340/969]  eta: 0:05:56  lr: 0.000312  loss: 0.6889 (0.6967)  time: 0.5645  data: 0.0002  max mem: 6820\n[22:57:22.139352] Epoch: [10]  [360/969]  eta: 0:05:44  lr: 0.000312  loss: 0.6899 (0.6968)  time: 0.5629  data: 0.0002  max mem: 6820\n[22:57:33.406618] Epoch: [10]  [380/969]  eta: 0:05:33  lr: 0.000312  loss: 0.6979 (0.6969)  time: 0.5633  data: 0.0002  max mem: 6820\n[22:57:44.654930] Epoch: [10]  [400/969]  eta: 0:05:22  lr: 0.000312  loss: 0.6976 (0.6968)  time: 0.5624  data: 0.0002  max mem: 6820\n[22:57:55.923061] Epoch: [10]  [420/969]  eta: 0:05:10  lr: 0.000312  loss: 0.6985 (0.6973)  time: 0.5634  data: 0.0002  max mem: 6820\n[22:58:07.194092] Epoch: [10]  [440/969]  eta: 0:04:59  lr: 0.000312  loss: 0.6972 (0.6973)  time: 0.5635  data: 0.0002  max mem: 6820\n[22:58:18.463120] Epoch: [10]  [460/969]  eta: 0:04:47  lr: 0.000312  loss: 0.6912 (0.6972)  time: 0.5634  data: 0.0002  max mem: 6820\n[22:58:29.733745] Epoch: [10]  [480/969]  eta: 0:04:36  lr: 0.000312  loss: 0.6902 (0.6971)  time: 0.5635  data: 0.0002  max mem: 6820\n[22:58:41.019250] Epoch: [10]  [500/969]  eta: 0:04:25  lr: 0.000312  loss: 0.6982 (0.6974)  time: 0.5642  data: 0.0002  max mem: 6820\n[22:58:52.245551] Epoch: [10]  [520/969]  eta: 0:04:13  lr: 0.000312  loss: 0.6918 (0.6973)  time: 0.5613  data: 0.0002  max mem: 6820\n[22:59:03.544043] Epoch: [10]  [540/969]  eta: 0:04:02  lr: 0.000312  loss: 0.6887 (0.6971)  time: 0.5649  data: 0.0002  max mem: 6820\n[22:59:14.826255] Epoch: [10]  [560/969]  eta: 0:03:51  lr: 0.000312  loss: 0.6855 (0.6971)  time: 0.5641  data: 0.0002  max mem: 6820\n[22:59:26.089175] Epoch: [10]  [580/969]  eta: 0:03:39  lr: 0.000312  loss: 0.6898 (0.6971)  time: 0.5631  data: 0.0002  max mem: 6820\n[22:59:37.377699] Epoch: [10]  [600/969]  eta: 0:03:28  lr: 0.000312  loss: 0.6927 (0.6970)  time: 0.5644  data: 0.0002  max mem: 6820\n[22:59:48.684026] Epoch: [10]  [620/969]  eta: 0:03:17  lr: 0.000312  loss: 0.6968 (0.6972)  time: 0.5653  data: 0.0002  max mem: 6820\n[22:59:59.966340] Epoch: [10]  [640/969]  eta: 0:03:05  lr: 0.000312  loss: 0.6850 (0.6971)  time: 0.5641  data: 0.0002  max mem: 6820\n[23:00:11.249600] Epoch: [10]  [660/969]  eta: 0:02:54  lr: 0.000312  loss: 0.6929 (0.6969)  time: 0.5641  data: 0.0002  max mem: 6820\n[23:00:22.527860] Epoch: [10]  [680/969]  eta: 0:02:43  lr: 0.000312  loss: 0.6890 (0.6969)  time: 0.5639  data: 0.0002  max mem: 6820\n[23:00:33.822270] Epoch: [10]  [700/969]  eta: 0:02:32  lr: 0.000312  loss: 0.6877 (0.6968)  time: 0.5646  data: 0.0002  max mem: 6820\n[23:00:45.139556] Epoch: [10]  [720/969]  eta: 0:02:20  lr: 0.000312  loss: 0.6942 (0.6969)  time: 0.5658  data: 0.0002  max mem: 6820\n[23:00:56.442557] Epoch: [10]  [740/969]  eta: 0:02:09  lr: 0.000312  loss: 0.6882 (0.6966)  time: 0.5651  data: 0.0002  max mem: 6820\n[23:01:07.756021] Epoch: [10]  [760/969]  eta: 0:01:58  lr: 0.000312  loss: 0.6944 (0.6966)  time: 0.5656  data: 0.0002  max mem: 6820\n[23:01:19.074253] Epoch: [10]  [780/969]  eta: 0:01:46  lr: 0.000312  loss: 0.6850 (0.6964)  time: 0.5659  data: 0.0002  max mem: 6820\n[23:01:30.396592] Epoch: [10]  [800/969]  eta: 0:01:35  lr: 0.000312  loss: 0.7013 (0.6966)  time: 0.5660  data: 0.0002  max mem: 6820\n[23:01:41.558212] Epoch: [10]  [820/969]  eta: 0:01:24  lr: 0.000312  loss: 0.6917 (0.6968)  time: 0.5580  data: 0.0002  max mem: 6820\n[23:01:52.788509] Epoch: [10]  [840/969]  eta: 0:01:12  lr: 0.000312  loss: 0.6935 (0.6968)  time: 0.5615  data: 0.0002  max mem: 6820\n[23:02:04.061050] Epoch: [10]  [860/969]  eta: 0:01:01  lr: 0.000312  loss: 0.6974 (0.6968)  time: 0.5636  data: 0.0002  max mem: 6820\n[23:02:15.355344] Epoch: [10]  [880/969]  eta: 0:00:50  lr: 0.000312  loss: 0.6908 (0.6967)  time: 0.5647  data: 0.0002  max mem: 6820\n[23:02:26.636394] Epoch: [10]  [900/969]  eta: 0:00:38  lr: 0.000312  loss: 0.6876 (0.6965)  time: 0.5640  data: 0.0002  max mem: 6820\n[23:02:37.925842] Epoch: [10]  [920/969]  eta: 0:00:27  lr: 0.000312  loss: 0.6956 (0.6965)  time: 0.5644  data: 0.0002  max mem: 6820\n[23:02:49.202431] Epoch: [10]  [940/969]  eta: 0:00:16  lr: 0.000312  loss: 0.6852 (0.6964)  time: 0.5638  data: 0.0002  max mem: 6820\n[23:03:00.455864] Epoch: [10]  [960/969]  eta: 0:00:05  lr: 0.000312  loss: 0.6963 (0.6963)  time: 0.5626  data: 0.0002  max mem: 6820\n[23:03:04.976933] Epoch: [10]  [968/969]  eta: 0:00:00  lr: 0.000312  loss: 0.6963 (0.6963)  time: 0.5640  data: 0.0001  max mem: 6820\n[23:03:05.122093] Epoch: [10] Total time: 0:09:07 (0.5650 s / it)\n[23:03:05.122218] Averaged stats: lr: 0.000312  loss: 0.6963 (0.6963)\n[23:03:06.029004] val:  [  0/139]  eta: 0:02:04  loss: 0.6530 (0.6530)  time: 0.8991  data: 0.7680  max mem: 6820\n[23:03:07.520071] val:  [ 10/139]  eta: 0:00:28  loss: 0.6515 (0.6504)  time: 0.2172  data: 0.0700  max mem: 6820\n[23:03:09.024682] val:  [ 20/139]  eta: 0:00:22  loss: 0.6489 (0.6480)  time: 0.1497  data: 0.0002  max mem: 6820\n[23:03:10.529038] val:  [ 30/139]  eta: 0:00:18  loss: 0.6489 (0.6474)  time: 0.1504  data: 0.0002  max mem: 6820\n[23:03:12.033164] val:  [ 40/139]  eta: 0:00:16  loss: 0.6528 (0.6495)  time: 0.1504  data: 0.0002  max mem: 6820\n[23:03:13.541436] val:  [ 50/139]  eta: 0:00:14  loss: 0.6551 (0.6505)  time: 0.1505  data: 0.0002  max mem: 6820\n[23:03:15.047835] val:  [ 60/139]  eta: 0:00:12  loss: 0.6525 (0.6506)  time: 0.1507  data: 0.0002  max mem: 6820\n[23:03:16.554333] val:  [ 70/139]  eta: 0:00:11  loss: 0.6524 (0.6534)  time: 0.1506  data: 0.0002  max mem: 6820\n[23:03:18.058336] val:  [ 80/139]  eta: 0:00:09  loss: 0.7236 (0.6633)  time: 0.1504  data: 0.0002  max mem: 6820\n[23:03:19.566123] val:  [ 90/139]  eta: 0:00:07  loss: 0.7316 (0.6711)  time: 0.1505  data: 0.0002  max mem: 6820\n[23:03:21.076105] val:  [100/139]  eta: 0:00:06  loss: 0.7314 (0.6770)  time: 0.1508  data: 0.0002  max mem: 6820\n[23:03:22.586700] val:  [110/139]  eta: 0:00:04  loss: 0.7295 (0.6815)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:03:24.089045] val:  [120/139]  eta: 0:00:02  loss: 0.7267 (0.6851)  time: 0.1506  data: 0.0002  max mem: 6820\n[23:03:25.599734] val:  [130/139]  eta: 0:00:01  loss: 0.7244 (0.6881)  time: 0.1506  data: 0.0001  max mem: 6820\n[23:03:26.714202] val:  [138/139]  eta: 0:00:00  loss: 0.7215 (0.6902)  time: 0.1461  data: 0.0001  max mem: 6820\n[23:03:26.817647] val: Total time: 0:00:21 (0.1561 s / it)\n[23:03:26.872152] val loss: 0.690190157444357\n[23:03:26.872211] Accuracy: 0.5000, F1 Score: 0.3333, ROC AUC: 0.6065, Hamming Loss: 0.5000,\n Jaccard Score: 0.2500, Precision: 0.2500, Recall: 0.5000,\n Average Precision: 0.5886, Kappa: 0.0000, Score: 0.3133\n[23:03:26.874012] Best epoch = 5, Best score = 0.4670\n[23:03:26.876432] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[23:03:28.312295] Epoch: [11]  [  0/969]  eta: 0:23:09  lr: 0.000312  loss: 0.6781 (0.6781)  time: 1.4345  data: 0.8177  max mem: 6820\n[23:03:39.553608] Epoch: [11]  [ 20/969]  eta: 0:09:32  lr: 0.000312  loss: 0.6932 (0.6952)  time: 0.5620  data: 0.0002  max mem: 6820\n[23:03:50.815675] Epoch: [11]  [ 40/969]  eta: 0:09:02  lr: 0.000312  loss: 0.6910 (0.6942)  time: 0.5631  data: 0.0002  max mem: 6820\n[23:04:02.133900] Epoch: [11]  [ 60/969]  eta: 0:08:45  lr: 0.000312  loss: 0.6899 (0.6940)  time: 0.5659  data: 0.0002  max mem: 6820\n[23:04:13.416227] Epoch: [11]  [ 80/969]  eta: 0:08:30  lr: 0.000312  loss: 0.6961 (0.6948)  time: 0.5641  data: 0.0002  max mem: 6820\n[23:04:24.699490] Epoch: [11]  [100/969]  eta: 0:08:17  lr: 0.000312  loss: 0.6961 (0.6949)  time: 0.5641  data: 0.0002  max mem: 6820\n[23:04:36.026161] Epoch: [11]  [120/969]  eta: 0:08:05  lr: 0.000312  loss: 0.6976 (0.6956)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:04:47.305521] Epoch: [11]  [140/969]  eta: 0:07:52  lr: 0.000312  loss: 0.6928 (0.6961)  time: 0.5639  data: 0.0002  max mem: 6820\n[23:04:58.594805] Epoch: [11]  [160/969]  eta: 0:07:40  lr: 0.000312  loss: 0.6877 (0.6957)  time: 0.5644  data: 0.0002  max mem: 6820\n[23:05:09.869920] Epoch: [11]  [180/969]  eta: 0:07:28  lr: 0.000312  loss: 0.6889 (0.6955)  time: 0.5637  data: 0.0002  max mem: 6820\n[23:05:21.187832] Epoch: [11]  [200/969]  eta: 0:07:17  lr: 0.000312  loss: 0.6830 (0.6951)  time: 0.5658  data: 0.0002  max mem: 6820\n[23:05:32.518767] Epoch: [11]  [220/969]  eta: 0:07:05  lr: 0.000312  loss: 0.6894 (0.6952)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:05:43.781462] Epoch: [11]  [240/969]  eta: 0:06:54  lr: 0.000312  loss: 0.6999 (0.6956)  time: 0.5631  data: 0.0002  max mem: 6820\n[23:05:55.004172] Epoch: [11]  [260/969]  eta: 0:06:42  lr: 0.000312  loss: 0.6947 (0.6957)  time: 0.5611  data: 0.0002  max mem: 6820\n[23:06:06.193001] Epoch: [11]  [280/969]  eta: 0:06:30  lr: 0.000312  loss: 0.6963 (0.6959)  time: 0.5594  data: 0.0002  max mem: 6820\n[23:06:17.384266] Epoch: [11]  [300/969]  eta: 0:06:18  lr: 0.000312  loss: 0.6961 (0.6959)  time: 0.5595  data: 0.0002  max mem: 6820\n[23:06:28.574815] Epoch: [11]  [320/969]  eta: 0:06:07  lr: 0.000312  loss: 0.6914 (0.6957)  time: 0.5595  data: 0.0002  max mem: 6820\n[23:06:39.753074] Epoch: [11]  [340/969]  eta: 0:05:55  lr: 0.000312  loss: 0.6935 (0.6959)  time: 0.5589  data: 0.0002  max mem: 6820\n[23:06:50.952093] Epoch: [11]  [360/969]  eta: 0:05:44  lr: 0.000312  loss: 0.6976 (0.6962)  time: 0.5599  data: 0.0002  max mem: 6820\n[23:07:02.120214] Epoch: [11]  [380/969]  eta: 0:05:32  lr: 0.000312  loss: 0.6930 (0.6963)  time: 0.5584  data: 0.0002  max mem: 6820\n[23:07:13.351044] Epoch: [11]  [400/969]  eta: 0:05:21  lr: 0.000312  loss: 0.6948 (0.6961)  time: 0.5615  data: 0.0002  max mem: 6820\n[23:07:24.598686] Epoch: [11]  [420/969]  eta: 0:05:09  lr: 0.000312  loss: 0.6942 (0.6964)  time: 0.5623  data: 0.0002  max mem: 6820\n[23:07:35.872154] Epoch: [11]  [440/969]  eta: 0:04:58  lr: 0.000312  loss: 0.6864 (0.6964)  time: 0.5636  data: 0.0002  max mem: 6820\n[23:07:47.149600] Epoch: [11]  [460/969]  eta: 0:04:47  lr: 0.000312  loss: 0.6845 (0.6962)  time: 0.5638  data: 0.0002  max mem: 6820\n[23:07:58.456008] Epoch: [11]  [480/969]  eta: 0:04:36  lr: 0.000312  loss: 0.6956 (0.6961)  time: 0.5653  data: 0.0002  max mem: 6820\n[23:08:09.764945] Epoch: [11]  [500/969]  eta: 0:04:24  lr: 0.000312  loss: 0.6907 (0.6962)  time: 0.5654  data: 0.0002  max mem: 6820\n[23:08:20.949644] Epoch: [11]  [520/969]  eta: 0:04:13  lr: 0.000312  loss: 0.6935 (0.6962)  time: 0.5592  data: 0.0002  max mem: 6820\n[23:08:32.108451] Epoch: [11]  [540/969]  eta: 0:04:02  lr: 0.000312  loss: 0.6930 (0.6960)  time: 0.5579  data: 0.0002  max mem: 6820\n[23:08:43.293149] Epoch: [11]  [560/969]  eta: 0:03:50  lr: 0.000312  loss: 0.6826 (0.6960)  time: 0.5592  data: 0.0002  max mem: 6820\n[23:08:54.533176] Epoch: [11]  [580/969]  eta: 0:03:39  lr: 0.000312  loss: 0.6874 (0.6959)  time: 0.5620  data: 0.0002  max mem: 6820\n[23:09:05.768291] Epoch: [11]  [600/969]  eta: 0:03:28  lr: 0.000312  loss: 0.6951 (0.6961)  time: 0.5617  data: 0.0002  max mem: 6820\n[23:09:16.976064] Epoch: [11]  [620/969]  eta: 0:03:16  lr: 0.000312  loss: 0.6968 (0.6962)  time: 0.5603  data: 0.0002  max mem: 6820\n[23:09:28.151656] Epoch: [11]  [640/969]  eta: 0:03:05  lr: 0.000312  loss: 0.6867 (0.6962)  time: 0.5587  data: 0.0002  max mem: 6820\n[23:09:39.389506] Epoch: [11]  [660/969]  eta: 0:02:54  lr: 0.000312  loss: 0.6886 (0.6959)  time: 0.5618  data: 0.0002  max mem: 6820\n[23:09:50.634143] Epoch: [11]  [680/969]  eta: 0:02:42  lr: 0.000312  loss: 0.6977 (0.6961)  time: 0.5622  data: 0.0002  max mem: 6820\n[23:10:01.887794] Epoch: [11]  [700/969]  eta: 0:02:31  lr: 0.000312  loss: 0.6941 (0.6961)  time: 0.5626  data: 0.0002  max mem: 6820\n[23:10:13.148214] Epoch: [11]  [720/969]  eta: 0:02:20  lr: 0.000312  loss: 0.6906 (0.6961)  time: 0.5630  data: 0.0002  max mem: 6820\n[23:10:24.418184] Epoch: [11]  [740/969]  eta: 0:02:09  lr: 0.000312  loss: 0.6786 (0.6958)  time: 0.5634  data: 0.0002  max mem: 6820\n[23:10:35.705522] Epoch: [11]  [760/969]  eta: 0:01:57  lr: 0.000312  loss: 0.6944 (0.6958)  time: 0.5643  data: 0.0002  max mem: 6820\n[23:10:47.004222] Epoch: [11]  [780/969]  eta: 0:01:46  lr: 0.000312  loss: 0.6918 (0.6956)  time: 0.5649  data: 0.0002  max mem: 6820\n[23:10:58.319378] Epoch: [11]  [800/969]  eta: 0:01:35  lr: 0.000312  loss: 0.7047 (0.6957)  time: 0.5657  data: 0.0002  max mem: 6820\n[23:11:09.611533] Epoch: [11]  [820/969]  eta: 0:01:23  lr: 0.000312  loss: 0.7000 (0.6957)  time: 0.5646  data: 0.0002  max mem: 6820\n[23:11:20.932450] Epoch: [11]  [840/969]  eta: 0:01:12  lr: 0.000312  loss: 0.6893 (0.6957)  time: 0.5660  data: 0.0002  max mem: 6820\n[23:11:32.243878] Epoch: [11]  [860/969]  eta: 0:01:01  lr: 0.000312  loss: 0.6941 (0.6958)  time: 0.5655  data: 0.0002  max mem: 6820\n[23:11:43.541924] Epoch: [11]  [880/969]  eta: 0:00:50  lr: 0.000312  loss: 0.6897 (0.6957)  time: 0.5649  data: 0.0002  max mem: 6820\n[23:11:54.834379] Epoch: [11]  [900/969]  eta: 0:00:38  lr: 0.000312  loss: 0.6976 (0.6957)  time: 0.5646  data: 0.0002  max mem: 6820\n[23:12:06.124941] Epoch: [11]  [920/969]  eta: 0:00:27  lr: 0.000312  loss: 0.6917 (0.6957)  time: 0.5645  data: 0.0002  max mem: 6820\n[23:12:17.441549] Epoch: [11]  [940/969]  eta: 0:00:16  lr: 0.000312  loss: 0.6846 (0.6955)  time: 0.5658  data: 0.0001  max mem: 6820\n[23:12:28.754843] Epoch: [11]  [960/969]  eta: 0:00:05  lr: 0.000312  loss: 0.6978 (0.6956)  time: 0.5656  data: 0.0002  max mem: 6820\n[23:12:33.262014] Epoch: [11]  [968/969]  eta: 0:00:00  lr: 0.000312  loss: 0.6970 (0.6956)  time: 0.5643  data: 0.0001  max mem: 6820\n[23:12:33.383213] Epoch: [11] Total time: 0:09:06 (0.5640 s / it)\n[23:12:33.383326] Averaged stats: lr: 0.000312  loss: 0.6970 (0.6956)\n[23:12:34.120667] val:  [  0/139]  eta: 0:01:41  loss: 0.6703 (0.6703)  time: 0.7324  data: 0.5957  max mem: 6820\n[23:12:35.620348] val:  [ 10/139]  eta: 0:00:26  loss: 0.6631 (0.6607)  time: 0.2027  data: 0.0544  max mem: 6820\n[23:12:37.116465] val:  [ 20/139]  eta: 0:00:21  loss: 0.6559 (0.6565)  time: 0.1496  data: 0.0002  max mem: 6820\n[23:12:38.617952] val:  [ 30/139]  eta: 0:00:18  loss: 0.6549 (0.6555)  time: 0.1498  data: 0.0002  max mem: 6820\n[23:12:40.129490] val:  [ 40/139]  eta: 0:00:16  loss: 0.6674 (0.6596)  time: 0.1506  data: 0.0002  max mem: 6820\n[23:12:41.638433] val:  [ 50/139]  eta: 0:00:14  loss: 0.6702 (0.6618)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:12:43.147880] val:  [ 60/139]  eta: 0:00:12  loss: 0.6702 (0.6629)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:12:44.656588] val:  [ 70/139]  eta: 0:00:10  loss: 0.6701 (0.6656)  time: 0.1508  data: 0.0002  max mem: 6820\n[23:12:46.160944] val:  [ 80/139]  eta: 0:00:09  loss: 0.6969 (0.6715)  time: 0.1506  data: 0.0002  max mem: 6820\n[23:12:47.668853] val:  [ 90/139]  eta: 0:00:07  loss: 0.7104 (0.6763)  time: 0.1505  data: 0.0002  max mem: 6820\n[23:12:49.175500] val:  [100/139]  eta: 0:00:06  loss: 0.7115 (0.6796)  time: 0.1507  data: 0.0002  max mem: 6820\n[23:12:50.687020] val:  [110/139]  eta: 0:00:04  loss: 0.7082 (0.6817)  time: 0.1508  data: 0.0002  max mem: 6820\n[23:12:52.195648] val:  [120/139]  eta: 0:00:02  loss: 0.7029 (0.6834)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:12:53.703037] val:  [130/139]  eta: 0:00:01  loss: 0.7017 (0.6846)  time: 0.1507  data: 0.0001  max mem: 6820\n[23:12:54.813568] val:  [138/139]  eta: 0:00:00  loss: 0.6977 (0.6856)  time: 0.1459  data: 0.0001  max mem: 6820\n[23:12:54.918601] val: Total time: 0:00:21 (0.1549 s / it)\n[23:12:54.976611] val loss: 0.6855690496430981\n[23:12:54.976674] Accuracy: 0.5723, F1 Score: 0.5612, ROC AUC: 0.6209, Hamming Loss: 0.4277,\n Jaccard Score: 0.3933, Precision: 0.5805, Recall: 0.5723,\n Average Precision: 0.6035, Kappa: 0.1447, Score: 0.4423\n[23:12:54.978497] Best epoch = 5, Best score = 0.4670\n[23:12:54.980945] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[23:12:56.380796] Epoch: [12]  [  0/969]  eta: 0:22:35  lr: 0.000312  loss: 0.6829 (0.6829)  time: 1.3986  data: 0.8227  max mem: 6820\n[23:13:07.690340] Epoch: [12]  [ 20/969]  eta: 0:09:34  lr: 0.000312  loss: 0.6914 (0.6891)  time: 0.5654  data: 0.0002  max mem: 6820\n[23:13:19.009051] Epoch: [12]  [ 40/969]  eta: 0:09:04  lr: 0.000312  loss: 0.6940 (0.6917)  time: 0.5659  data: 0.0002  max mem: 6820\n[23:13:30.334791] Epoch: [12]  [ 60/969]  eta: 0:08:46  lr: 0.000312  loss: 0.6850 (0.6901)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:13:41.657488] Epoch: [12]  [ 80/969]  eta: 0:08:32  lr: 0.000312  loss: 0.7005 (0.6930)  time: 0.5661  data: 0.0002  max mem: 6820\n[23:13:52.982810] Epoch: [12]  [100/969]  eta: 0:08:18  lr: 0.000312  loss: 0.6950 (0.6935)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:14:04.319278] Epoch: [12]  [120/969]  eta: 0:08:06  lr: 0.000312  loss: 0.6953 (0.6944)  time: 0.5668  data: 0.0002  max mem: 6820\n[23:14:15.664535] Epoch: [12]  [140/969]  eta: 0:07:54  lr: 0.000312  loss: 0.6932 (0.6954)  time: 0.5672  data: 0.0002  max mem: 6820\n[23:14:26.983547] Epoch: [12]  [160/969]  eta: 0:07:42  lr: 0.000312  loss: 0.6842 (0.6943)  time: 0.5659  data: 0.0002  max mem: 6820\n[23:14:38.306953] Epoch: [12]  [180/969]  eta: 0:07:30  lr: 0.000312  loss: 0.6885 (0.6938)  time: 0.5661  data: 0.0002  max mem: 6820\n[23:14:49.653191] Epoch: [12]  [200/969]  eta: 0:07:18  lr: 0.000312  loss: 0.6987 (0.6939)  time: 0.5673  data: 0.0002  max mem: 6820\n[23:15:00.961396] Epoch: [12]  [220/969]  eta: 0:07:06  lr: 0.000312  loss: 0.6866 (0.6936)  time: 0.5654  data: 0.0002  max mem: 6820\n[23:15:12.272752] Epoch: [12]  [240/969]  eta: 0:06:55  lr: 0.000312  loss: 0.6906 (0.6937)  time: 0.5655  data: 0.0002  max mem: 6820\n[23:15:23.594552] Epoch: [12]  [260/969]  eta: 0:06:43  lr: 0.000312  loss: 0.6957 (0.6938)  time: 0.5660  data: 0.0002  max mem: 6820\n[23:15:34.906773] Epoch: [12]  [280/969]  eta: 0:06:32  lr: 0.000312  loss: 0.6914 (0.6941)  time: 0.5656  data: 0.0001  max mem: 6820\n[23:15:46.200118] Epoch: [12]  [300/969]  eta: 0:06:20  lr: 0.000312  loss: 0.6925 (0.6942)  time: 0.5646  data: 0.0002  max mem: 6820\n[23:15:57.470101] Epoch: [12]  [320/969]  eta: 0:06:08  lr: 0.000312  loss: 0.6895 (0.6940)  time: 0.5634  data: 0.0002  max mem: 6820\n[23:16:08.735040] Epoch: [12]  [340/969]  eta: 0:05:57  lr: 0.000312  loss: 0.6924 (0.6941)  time: 0.5632  data: 0.0002  max mem: 6820\n[23:16:20.036122] Epoch: [12]  [360/969]  eta: 0:05:45  lr: 0.000312  loss: 0.6882 (0.6943)  time: 0.5650  data: 0.0002  max mem: 6820\n[23:16:31.307885] Epoch: [12]  [380/969]  eta: 0:05:34  lr: 0.000312  loss: 0.6893 (0.6944)  time: 0.5635  data: 0.0002  max mem: 6820\n[23:16:42.601655] Epoch: [12]  [400/969]  eta: 0:05:22  lr: 0.000312  loss: 0.6902 (0.6943)  time: 0.5646  data: 0.0002  max mem: 6820\n[23:16:53.888036] Epoch: [12]  [420/969]  eta: 0:05:11  lr: 0.000312  loss: 0.7009 (0.6948)  time: 0.5643  data: 0.0002  max mem: 6820\n[23:17:05.196449] Epoch: [12]  [440/969]  eta: 0:05:00  lr: 0.000312  loss: 0.6857 (0.6946)  time: 0.5654  data: 0.0002  max mem: 6820\n[23:17:16.544125] Epoch: [12]  [460/969]  eta: 0:04:48  lr: 0.000312  loss: 0.6884 (0.6948)  time: 0.5673  data: 0.0002  max mem: 6820\n[23:17:27.848039] Epoch: [12]  [480/969]  eta: 0:04:37  lr: 0.000312  loss: 0.6904 (0.6948)  time: 0.5651  data: 0.0002  max mem: 6820\n[23:17:39.146439] Epoch: [12]  [500/969]  eta: 0:04:25  lr: 0.000312  loss: 0.6947 (0.6950)  time: 0.5649  data: 0.0002  max mem: 6820\n[23:17:50.460945] Epoch: [12]  [520/969]  eta: 0:04:14  lr: 0.000311  loss: 0.6894 (0.6948)  time: 0.5657  data: 0.0002  max mem: 6820\n[23:18:01.792894] Epoch: [12]  [540/969]  eta: 0:04:03  lr: 0.000311  loss: 0.6870 (0.6945)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:18:13.140574] Epoch: [12]  [560/969]  eta: 0:03:51  lr: 0.000311  loss: 0.6745 (0.6944)  time: 0.5673  data: 0.0002  max mem: 6820\n[23:18:24.468166] Epoch: [12]  [580/969]  eta: 0:03:40  lr: 0.000311  loss: 0.6965 (0.6943)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:18:35.789211] Epoch: [12]  [600/969]  eta: 0:03:29  lr: 0.000311  loss: 0.6950 (0.6944)  time: 0.5660  data: 0.0002  max mem: 6820\n[23:18:47.120196] Epoch: [12]  [620/969]  eta: 0:03:17  lr: 0.000311  loss: 0.6955 (0.6946)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:18:58.408119] Epoch: [12]  [640/969]  eta: 0:03:06  lr: 0.000311  loss: 0.6860 (0.6945)  time: 0.5643  data: 0.0002  max mem: 6820\n[23:19:09.706518] Epoch: [12]  [660/969]  eta: 0:02:55  lr: 0.000311  loss: 0.6903 (0.6944)  time: 0.5649  data: 0.0002  max mem: 6820\n[23:19:21.012197] Epoch: [12]  [680/969]  eta: 0:02:43  lr: 0.000311  loss: 0.6895 (0.6944)  time: 0.5652  data: 0.0002  max mem: 6820\n[23:19:32.307107] Epoch: [12]  [700/969]  eta: 0:02:32  lr: 0.000311  loss: 0.6899 (0.6944)  time: 0.5647  data: 0.0002  max mem: 6820\n[23:19:43.628238] Epoch: [12]  [720/969]  eta: 0:02:21  lr: 0.000311  loss: 0.6974 (0.6944)  time: 0.5660  data: 0.0002  max mem: 6820\n[23:19:54.960088] Epoch: [12]  [740/969]  eta: 0:02:09  lr: 0.000311  loss: 0.6685 (0.6937)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:20:06.291110] Epoch: [12]  [760/969]  eta: 0:01:58  lr: 0.000311  loss: 0.6851 (0.6936)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:20:17.633142] Epoch: [12]  [780/969]  eta: 0:01:47  lr: 0.000311  loss: 0.6787 (0.6933)  time: 0.5670  data: 0.0002  max mem: 6820\n[23:20:28.955519] Epoch: [12]  [800/969]  eta: 0:01:35  lr: 0.000311  loss: 0.6888 (0.6933)  time: 0.5661  data: 0.0002  max mem: 6820\n[23:20:40.294624] Epoch: [12]  [820/969]  eta: 0:01:24  lr: 0.000311  loss: 0.6939 (0.6934)  time: 0.5669  data: 0.0002  max mem: 6820\n[23:20:51.637109] Epoch: [12]  [840/969]  eta: 0:01:13  lr: 0.000311  loss: 0.6885 (0.6933)  time: 0.5671  data: 0.0002  max mem: 6820\n[23:21:02.985935] Epoch: [12]  [860/969]  eta: 0:01:01  lr: 0.000311  loss: 0.6984 (0.6932)  time: 0.5674  data: 0.0003  max mem: 6820\n[23:21:14.347611] Epoch: [12]  [880/969]  eta: 0:00:50  lr: 0.000311  loss: 0.6769 (0.6932)  time: 0.5680  data: 0.0002  max mem: 6820\n[23:21:25.722677] Epoch: [12]  [900/969]  eta: 0:00:39  lr: 0.000311  loss: 0.6919 (0.6932)  time: 0.5687  data: 0.0002  max mem: 6820\n[23:21:37.078142] Epoch: [12]  [920/969]  eta: 0:00:27  lr: 0.000311  loss: 0.6963 (0.6932)  time: 0.5677  data: 0.0002  max mem: 6820\n[23:21:48.411651] Epoch: [12]  [940/969]  eta: 0:00:16  lr: 0.000311  loss: 0.6812 (0.6931)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:21:59.639313] Epoch: [12]  [960/969]  eta: 0:00:05  lr: 0.000311  loss: 0.6960 (0.6931)  time: 0.5613  data: 0.0002  max mem: 6820\n[23:22:04.153832] Epoch: [12]  [968/969]  eta: 0:00:00  lr: 0.000311  loss: 0.6815 (0.6930)  time: 0.5648  data: 0.0002  max mem: 6820\n[23:22:04.269712] Epoch: [12] Total time: 0:09:09 (0.5669 s / it)\n[23:22:04.269829] Averaged stats: lr: 0.000311  loss: 0.6815 (0.6930)\n[23:22:05.207711] val:  [  0/139]  eta: 0:02:09  loss: 0.6856 (0.6856)  time: 0.9333  data: 0.7933  max mem: 6820\n[23:22:06.701202] val:  [ 10/139]  eta: 0:00:28  loss: 0.6549 (0.6306)  time: 0.2205  data: 0.0726  max mem: 6820\n[23:22:08.207456] val:  [ 20/139]  eta: 0:00:22  loss: 0.6048 (0.6269)  time: 0.1499  data: 0.0003  max mem: 6820\n[23:22:09.705386] val:  [ 30/139]  eta: 0:00:19  loss: 0.6100 (0.6253)  time: 0.1501  data: 0.0002  max mem: 6820\n[23:22:11.214257] val:  [ 40/139]  eta: 0:00:16  loss: 0.6381 (0.6324)  time: 0.1503  data: 0.0002  max mem: 6820\n[23:22:12.713878] val:  [ 50/139]  eta: 0:00:14  loss: 0.6446 (0.6358)  time: 0.1503  data: 0.0002  max mem: 6820\n[23:22:14.224432] val:  [ 60/139]  eta: 0:00:12  loss: 0.6447 (0.6388)  time: 0.1504  data: 0.0002  max mem: 6820\n[23:22:15.735542] val:  [ 70/139]  eta: 0:00:11  loss: 0.6586 (0.6441)  time: 0.1510  data: 0.0002  max mem: 6820\n[23:22:17.242384] val:  [ 80/139]  eta: 0:00:09  loss: 0.6707 (0.6519)  time: 0.1508  data: 0.0002  max mem: 6820\n[23:22:18.753070] val:  [ 90/139]  eta: 0:00:07  loss: 0.7058 (0.6598)  time: 0.1508  data: 0.0002  max mem: 6820\n[23:22:20.264318] val:  [100/139]  eta: 0:00:06  loss: 0.7192 (0.6660)  time: 0.1510  data: 0.0002  max mem: 6820\n[23:22:21.776734] val:  [110/139]  eta: 0:00:04  loss: 0.7160 (0.6696)  time: 0.1511  data: 0.0002  max mem: 6820\n[23:22:23.280917] val:  [120/139]  eta: 0:00:02  loss: 0.7098 (0.6718)  time: 0.1507  data: 0.0002  max mem: 6820\n[23:22:24.792587] val:  [130/139]  eta: 0:00:01  loss: 0.6809 (0.6726)  time: 0.1507  data: 0.0002  max mem: 6820\n[23:22:25.906863] val:  [138/139]  eta: 0:00:00  loss: 0.6809 (0.6743)  time: 0.1463  data: 0.0002  max mem: 6820\n[23:22:26.016178] val: Total time: 0:00:21 (0.1564 s / it)\n[23:22:26.073796] val loss: 0.6743487900109599\n[23:22:26.073868] Accuracy: 0.6062, F1 Score: 0.6060, ROC AUC: 0.6333, Hamming Loss: 0.3938,\n Jaccard Score: 0.4348, Precision: 0.6065, Recall: 0.6062,\n Average Precision: 0.6107, Kappa: 0.2125, Score: 0.4839\n[23:22:29.075085] Best epoch = 12, Best score = 0.4839\n[23:22:29.077842] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[23:22:30.536084] Epoch: [13]  [  0/969]  eta: 0:23:31  lr: 0.000311  loss: 0.6953 (0.6953)  time: 1.4570  data: 0.8529  max mem: 6820\n[23:22:41.777252] Epoch: [13]  [ 20/969]  eta: 0:09:33  lr: 0.000311  loss: 0.6883 (0.6880)  time: 0.5620  data: 0.0002  max mem: 6820\n[23:22:53.233932] Epoch: [13]  [ 40/969]  eta: 0:09:07  lr: 0.000311  loss: 0.6794 (0.6848)  time: 0.5728  data: 0.0003  max mem: 6820\n[23:23:04.786092] Epoch: [13]  [ 60/969]  eta: 0:08:52  lr: 0.000311  loss: 0.6760 (0.6847)  time: 0.5776  data: 0.0002  max mem: 6820\n[23:23:16.274789] Epoch: [13]  [ 80/969]  eta: 0:08:37  lr: 0.000311  loss: 0.6889 (0.6855)  time: 0.5744  data: 0.0002  max mem: 6820\n[23:23:27.653769] Epoch: [13]  [100/969]  eta: 0:08:23  lr: 0.000311  loss: 0.6828 (0.6846)  time: 0.5689  data: 0.0002  max mem: 6820\n[23:23:38.951235] Epoch: [13]  [120/969]  eta: 0:08:10  lr: 0.000311  loss: 0.6938 (0.6851)  time: 0.5648  data: 0.0002  max mem: 6820\n[23:23:50.225617] Epoch: [13]  [140/969]  eta: 0:07:57  lr: 0.000311  loss: 0.7035 (0.6885)  time: 0.5637  data: 0.0002  max mem: 6820\n[23:24:01.506200] Epoch: [13]  [160/969]  eta: 0:07:44  lr: 0.000311  loss: 0.6892 (0.6887)  time: 0.5640  data: 0.0002  max mem: 6820\n[23:24:12.808886] Epoch: [13]  [180/969]  eta: 0:07:32  lr: 0.000311  loss: 0.6874 (0.6886)  time: 0.5651  data: 0.0002  max mem: 6820\n[23:24:24.165173] Epoch: [13]  [200/969]  eta: 0:07:20  lr: 0.000311  loss: 0.6969 (0.6883)  time: 0.5678  data: 0.0002  max mem: 6820\n[23:24:35.542601] Epoch: [13]  [220/969]  eta: 0:07:08  lr: 0.000311  loss: 0.6828 (0.6882)  time: 0.5688  data: 0.0002  max mem: 6820\n[23:24:46.974498] Epoch: [13]  [240/969]  eta: 0:06:57  lr: 0.000311  loss: 0.6935 (0.6883)  time: 0.5715  data: 0.0002  max mem: 6820\n[23:24:58.398912] Epoch: [13]  [260/969]  eta: 0:06:45  lr: 0.000311  loss: 0.6879 (0.6884)  time: 0.5712  data: 0.0002  max mem: 6820\n[23:25:09.746905] Epoch: [13]  [280/969]  eta: 0:06:33  lr: 0.000311  loss: 0.6947 (0.6888)  time: 0.5674  data: 0.0002  max mem: 6820\n[23:25:21.088703] Epoch: [13]  [300/969]  eta: 0:06:22  lr: 0.000311  loss: 0.6835 (0.6886)  time: 0.5670  data: 0.0002  max mem: 6820\n[23:25:32.408509] Epoch: [13]  [320/969]  eta: 0:06:10  lr: 0.000311  loss: 0.6846 (0.6882)  time: 0.5659  data: 0.0002  max mem: 6820\n[23:25:43.732929] Epoch: [13]  [340/969]  eta: 0:05:59  lr: 0.000311  loss: 0.6839 (0.6886)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:25:55.036567] Epoch: [13]  [360/969]  eta: 0:05:47  lr: 0.000311  loss: 0.6756 (0.6887)  time: 0.5651  data: 0.0002  max mem: 6820\n[23:26:06.343502] Epoch: [13]  [380/969]  eta: 0:05:35  lr: 0.000311  loss: 0.6784 (0.6885)  time: 0.5653  data: 0.0002  max mem: 6820\n[23:26:17.671409] Epoch: [13]  [400/969]  eta: 0:05:24  lr: 0.000311  loss: 0.6818 (0.6883)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:26:29.025139] Epoch: [13]  [420/969]  eta: 0:05:12  lr: 0.000311  loss: 0.6827 (0.6882)  time: 0.5676  data: 0.0002  max mem: 6820\n[23:26:40.385047] Epoch: [13]  [440/969]  eta: 0:05:01  lr: 0.000311  loss: 0.6849 (0.6882)  time: 0.5679  data: 0.0002  max mem: 6820\n[23:26:51.758175] Epoch: [13]  [460/969]  eta: 0:04:50  lr: 0.000311  loss: 0.6725 (0.6881)  time: 0.5686  data: 0.0002  max mem: 6820\n[23:27:03.119498] Epoch: [13]  [480/969]  eta: 0:04:38  lr: 0.000311  loss: 0.6795 (0.6880)  time: 0.5680  data: 0.0002  max mem: 6820\n[23:27:14.516519] Epoch: [13]  [500/969]  eta: 0:04:27  lr: 0.000311  loss: 0.6865 (0.6879)  time: 0.5698  data: 0.0002  max mem: 6820\n[23:27:25.917288] Epoch: [13]  [520/969]  eta: 0:04:15  lr: 0.000311  loss: 0.6826 (0.6874)  time: 0.5700  data: 0.0002  max mem: 6820\n[23:27:37.279602] Epoch: [13]  [540/969]  eta: 0:04:04  lr: 0.000311  loss: 0.6779 (0.6871)  time: 0.5681  data: 0.0002  max mem: 6820\n[23:27:48.667769] Epoch: [13]  [560/969]  eta: 0:03:52  lr: 0.000310  loss: 0.6808 (0.6870)  time: 0.5694  data: 0.0002  max mem: 6820\n[23:28:00.002165] Epoch: [13]  [580/969]  eta: 0:03:41  lr: 0.000310  loss: 0.6937 (0.6874)  time: 0.5667  data: 0.0002  max mem: 6820\n[23:28:11.303396] Epoch: [13]  [600/969]  eta: 0:03:30  lr: 0.000310  loss: 0.6784 (0.6874)  time: 0.5650  data: 0.0002  max mem: 6820\n[23:28:22.607178] Epoch: [13]  [620/969]  eta: 0:03:18  lr: 0.000310  loss: 0.6878 (0.6876)  time: 0.5651  data: 0.0001  max mem: 6820\n[23:28:33.921350] Epoch: [13]  [640/969]  eta: 0:03:07  lr: 0.000310  loss: 0.6896 (0.6878)  time: 0.5657  data: 0.0002  max mem: 6820\n[23:28:45.236730] Epoch: [13]  [660/969]  eta: 0:02:55  lr: 0.000310  loss: 0.6748 (0.6875)  time: 0.5657  data: 0.0002  max mem: 6820\n[23:28:56.561852] Epoch: [13]  [680/969]  eta: 0:02:44  lr: 0.000310  loss: 0.6867 (0.6877)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:29:07.889124] Epoch: [13]  [700/969]  eta: 0:02:33  lr: 0.000310  loss: 0.6809 (0.6875)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:29:19.247134] Epoch: [13]  [720/969]  eta: 0:02:21  lr: 0.000310  loss: 0.6857 (0.6874)  time: 0.5679  data: 0.0002  max mem: 6820\n[23:29:30.615509] Epoch: [13]  [740/969]  eta: 0:02:10  lr: 0.000310  loss: 0.6869 (0.6873)  time: 0.5684  data: 0.0002  max mem: 6820\n[23:29:41.984099] Epoch: [13]  [760/969]  eta: 0:01:58  lr: 0.000310  loss: 0.6897 (0.6872)  time: 0.5684  data: 0.0002  max mem: 6820\n[23:29:53.376400] Epoch: [13]  [780/969]  eta: 0:01:47  lr: 0.000310  loss: 0.6816 (0.6869)  time: 0.5696  data: 0.0002  max mem: 6820\n[23:30:04.770593] Epoch: [13]  [800/969]  eta: 0:01:36  lr: 0.000310  loss: 0.7072 (0.6874)  time: 0.5697  data: 0.0002  max mem: 6820\n[23:30:16.098948] Epoch: [13]  [820/969]  eta: 0:01:24  lr: 0.000310  loss: 0.6873 (0.6874)  time: 0.5664  data: 0.0002  max mem: 6820\n[23:30:27.484186] Epoch: [13]  [840/969]  eta: 0:01:13  lr: 0.000310  loss: 0.6824 (0.6873)  time: 0.5692  data: 0.0002  max mem: 6820\n[23:30:38.841130] Epoch: [13]  [860/969]  eta: 0:01:01  lr: 0.000310  loss: 0.6794 (0.6872)  time: 0.5678  data: 0.0002  max mem: 6820\n[23:30:50.203003] Epoch: [13]  [880/969]  eta: 0:00:50  lr: 0.000310  loss: 0.6740 (0.6871)  time: 0.5680  data: 0.0002  max mem: 6820\n[23:31:01.549506] Epoch: [13]  [900/969]  eta: 0:00:39  lr: 0.000310  loss: 0.6953 (0.6873)  time: 0.5673  data: 0.0002  max mem: 6820\n[23:31:12.875801] Epoch: [13]  [920/969]  eta: 0:00:27  lr: 0.000310  loss: 0.6839 (0.6873)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:31:24.239374] Epoch: [13]  [940/969]  eta: 0:00:16  lr: 0.000310  loss: 0.6815 (0.6872)  time: 0.5681  data: 0.0002  max mem: 6820\n[23:31:35.592681] Epoch: [13]  [960/969]  eta: 0:00:05  lr: 0.000310  loss: 0.6823 (0.6872)  time: 0.5676  data: 0.0002  max mem: 6820\n[23:31:40.121202] Epoch: [13]  [968/969]  eta: 0:00:00  lr: 0.000310  loss: 0.6676 (0.6871)  time: 0.5672  data: 0.0002  max mem: 6820\n[23:31:40.248799] Epoch: [13] Total time: 0:09:11 (0.5688 s / it)\n[23:31:40.248906] Averaged stats: lr: 0.000310  loss: 0.6676 (0.6871)\n[23:31:41.155951] val:  [  0/139]  eta: 0:02:05  loss: 0.7184 (0.7184)  time: 0.9022  data: 0.7627  max mem: 6820\n[23:31:42.656306] val:  [ 10/139]  eta: 0:00:28  loss: 0.6760 (0.6540)  time: 0.2182  data: 0.0696  max mem: 6820\n[23:31:44.161037] val:  [ 20/139]  eta: 0:00:22  loss: 0.6257 (0.6452)  time: 0.1501  data: 0.0002  max mem: 6820\n[23:31:45.675658] val:  [ 30/139]  eta: 0:00:19  loss: 0.6667 (0.6449)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:31:47.183746] val:  [ 40/139]  eta: 0:00:16  loss: 0.6795 (0.6603)  time: 0.1511  data: 0.0002  max mem: 6820\n[23:31:48.700443] val:  [ 50/139]  eta: 0:00:14  loss: 0.7037 (0.6672)  time: 0.1512  data: 0.0002  max mem: 6820\n[23:31:50.214662] val:  [ 60/139]  eta: 0:00:12  loss: 0.6926 (0.6694)  time: 0.1515  data: 0.0002  max mem: 6820\n[23:31:51.724316] val:  [ 70/139]  eta: 0:00:11  loss: 0.6926 (0.6741)  time: 0.1511  data: 0.0002  max mem: 6820\n[23:31:53.240166] val:  [ 80/139]  eta: 0:00:09  loss: 0.6791 (0.6752)  time: 0.1512  data: 0.0002  max mem: 6820\n[23:31:54.757739] val:  [ 90/139]  eta: 0:00:07  loss: 0.6766 (0.6776)  time: 0.1516  data: 0.0002  max mem: 6820\n[23:31:56.278119] val:  [100/139]  eta: 0:00:06  loss: 0.6887 (0.6798)  time: 0.1518  data: 0.0002  max mem: 6820\n[23:31:57.787695] val:  [110/139]  eta: 0:00:04  loss: 0.6776 (0.6777)  time: 0.1514  data: 0.0002  max mem: 6820\n[23:31:59.297844] val:  [120/139]  eta: 0:00:02  loss: 0.6332 (0.6738)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:32:00.807595] val:  [130/139]  eta: 0:00:01  loss: 0.6228 (0.6692)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:32:01.920872] val:  [138/139]  eta: 0:00:00  loss: 0.6174 (0.6674)  time: 0.1461  data: 0.0001  max mem: 6820\n[23:32:02.030536] val: Total time: 0:00:21 (0.1567 s / it)\n[23:32:02.085111] val loss: 0.667375084307554\n[23:32:02.085175] Accuracy: 0.6076, F1 Score: 0.5978, ROC AUC: 0.6280, Hamming Loss: 0.3924,\n Jaccard Score: 0.4292, Precision: 0.6193, Recall: 0.6076,\n Average Precision: 0.6094, Kappa: 0.2152, Score: 0.4803\n[23:32:02.087114] Best epoch = 12, Best score = 0.4839\n[23:32:02.089825] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[23:32:03.362196] Epoch: [14]  [  0/969]  eta: 0:20:31  lr: 0.000310  loss: 0.6555 (0.6555)  time: 1.2708  data: 0.6541  max mem: 6820\n[23:32:14.688852] Epoch: [14]  [ 20/969]  eta: 0:09:29  lr: 0.000310  loss: 0.6705 (0.6783)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:32:26.016595] Epoch: [14]  [ 40/969]  eta: 0:09:02  lr: 0.000310  loss: 0.6858 (0.6843)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:32:37.353188] Epoch: [14]  [ 60/969]  eta: 0:08:45  lr: 0.000310  loss: 0.6866 (0.6854)  time: 0.5668  data: 0.0002  max mem: 6820\n[23:32:48.693281] Epoch: [14]  [ 80/969]  eta: 0:08:31  lr: 0.000310  loss: 0.6815 (0.6856)  time: 0.5670  data: 0.0002  max mem: 6820\n[23:33:00.037142] Epoch: [14]  [100/969]  eta: 0:08:18  lr: 0.000310  loss: 0.6835 (0.6877)  time: 0.5671  data: 0.0002  max mem: 6820\n[23:33:11.381495] Epoch: [14]  [120/969]  eta: 0:08:06  lr: 0.000310  loss: 0.6887 (0.6877)  time: 0.5672  data: 0.0002  max mem: 6820\n[23:33:22.748592] Epoch: [14]  [140/969]  eta: 0:07:54  lr: 0.000310  loss: 0.6893 (0.6882)  time: 0.5683  data: 0.0002  max mem: 6820\n[23:33:34.120116] Epoch: [14]  [160/969]  eta: 0:07:42  lr: 0.000310  loss: 0.6722 (0.6865)  time: 0.5685  data: 0.0002  max mem: 6820\n[23:33:45.454956] Epoch: [14]  [180/969]  eta: 0:07:30  lr: 0.000310  loss: 0.6879 (0.6862)  time: 0.5667  data: 0.0002  max mem: 6820\n[23:33:56.828435] Epoch: [14]  [200/969]  eta: 0:07:18  lr: 0.000310  loss: 0.6884 (0.6876)  time: 0.5686  data: 0.0002  max mem: 6820\n[23:34:08.154956] Epoch: [14]  [220/969]  eta: 0:07:07  lr: 0.000310  loss: 0.6777 (0.6870)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:34:19.510168] Epoch: [14]  [240/969]  eta: 0:06:55  lr: 0.000310  loss: 0.6853 (0.6869)  time: 0.5677  data: 0.0002  max mem: 6820\n[23:34:30.904309] Epoch: [14]  [260/969]  eta: 0:06:44  lr: 0.000310  loss: 0.6797 (0.6869)  time: 0.5697  data: 0.0002  max mem: 6820\n[23:34:42.263663] Epoch: [14]  [280/969]  eta: 0:06:32  lr: 0.000310  loss: 0.6859 (0.6874)  time: 0.5679  data: 0.0002  max mem: 6820\n[23:34:53.602466] Epoch: [14]  [300/969]  eta: 0:06:21  lr: 0.000310  loss: 0.6845 (0.6868)  time: 0.5669  data: 0.0002  max mem: 6820\n[23:35:04.954047] Epoch: [14]  [320/969]  eta: 0:06:09  lr: 0.000310  loss: 0.6685 (0.6864)  time: 0.5675  data: 0.0002  max mem: 6820\n[23:35:16.297061] Epoch: [14]  [340/969]  eta: 0:05:58  lr: 0.000310  loss: 0.6815 (0.6868)  time: 0.5671  data: 0.0002  max mem: 6820\n[23:35:27.597612] Epoch: [14]  [360/969]  eta: 0:05:46  lr: 0.000310  loss: 0.6867 (0.6870)  time: 0.5650  data: 0.0002  max mem: 6820\n[23:35:38.919527] Epoch: [14]  [380/969]  eta: 0:05:35  lr: 0.000309  loss: 0.6836 (0.6868)  time: 0.5660  data: 0.0002  max mem: 6820\n[23:35:50.249944] Epoch: [14]  [400/969]  eta: 0:05:23  lr: 0.000309  loss: 0.6837 (0.6864)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:36:01.575150] Epoch: [14]  [420/969]  eta: 0:05:12  lr: 0.000309  loss: 0.6911 (0.6865)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:36:12.877707] Epoch: [14]  [440/969]  eta: 0:05:00  lr: 0.000309  loss: 0.6914 (0.6870)  time: 0.5651  data: 0.0002  max mem: 6820\n[23:36:24.202797] Epoch: [14]  [460/969]  eta: 0:04:49  lr: 0.000309  loss: 0.6744 (0.6868)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:36:35.517615] Epoch: [14]  [480/969]  eta: 0:04:37  lr: 0.000309  loss: 0.6771 (0.6868)  time: 0.5657  data: 0.0002  max mem: 6820\n[23:36:46.834184] Epoch: [14]  [500/969]  eta: 0:04:26  lr: 0.000309  loss: 0.7007 (0.6875)  time: 0.5658  data: 0.0002  max mem: 6820\n[23:36:58.126948] Epoch: [14]  [520/969]  eta: 0:04:15  lr: 0.000309  loss: 0.6821 (0.6873)  time: 0.5646  data: 0.0002  max mem: 6820\n[23:37:09.445836] Epoch: [14]  [540/969]  eta: 0:04:03  lr: 0.000309  loss: 0.6575 (0.6865)  time: 0.5659  data: 0.0002  max mem: 6820\n[23:37:20.779803] Epoch: [14]  [560/969]  eta: 0:03:52  lr: 0.000309  loss: 0.6496 (0.6858)  time: 0.5667  data: 0.0002  max mem: 6820\n[23:37:32.106093] Epoch: [14]  [580/969]  eta: 0:03:40  lr: 0.000309  loss: 0.6832 (0.6862)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:37:43.423594] Epoch: [14]  [600/969]  eta: 0:03:29  lr: 0.000309  loss: 0.6919 (0.6865)  time: 0.5658  data: 0.0002  max mem: 6820\n[23:37:54.740797] Epoch: [14]  [620/969]  eta: 0:03:18  lr: 0.000309  loss: 0.6909 (0.6870)  time: 0.5658  data: 0.0002  max mem: 6820\n[23:38:06.066683] Epoch: [14]  [640/969]  eta: 0:03:06  lr: 0.000309  loss: 0.6979 (0.6870)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:38:17.393734] Epoch: [14]  [660/969]  eta: 0:02:55  lr: 0.000309  loss: 0.6729 (0.6868)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:38:28.719818] Epoch: [14]  [680/969]  eta: 0:02:44  lr: 0.000309  loss: 0.6931 (0.6871)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:38:40.052168] Epoch: [14]  [700/969]  eta: 0:02:32  lr: 0.000309  loss: 0.6851 (0.6870)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:38:51.384357] Epoch: [14]  [720/969]  eta: 0:02:21  lr: 0.000309  loss: 0.6898 (0.6871)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:39:02.715584] Epoch: [14]  [740/969]  eta: 0:02:09  lr: 0.000309  loss: 0.6787 (0.6867)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:39:14.083847] Epoch: [14]  [760/969]  eta: 0:01:58  lr: 0.000309  loss: 0.6518 (0.6862)  time: 0.5684  data: 0.0002  max mem: 6820\n[23:39:25.479523] Epoch: [14]  [780/969]  eta: 0:01:47  lr: 0.000309  loss: 0.6812 (0.6860)  time: 0.5697  data: 0.0002  max mem: 6820\n[23:39:36.813408] Epoch: [14]  [800/969]  eta: 0:01:35  lr: 0.000309  loss: 0.6961 (0.6864)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:39:48.158204] Epoch: [14]  [820/969]  eta: 0:01:24  lr: 0.000309  loss: 0.6718 (0.6863)  time: 0.5672  data: 0.0002  max mem: 6820\n[23:39:59.511267] Epoch: [14]  [840/969]  eta: 0:01:13  lr: 0.000309  loss: 0.6827 (0.6862)  time: 0.5676  data: 0.0002  max mem: 6820\n[23:40:10.856128] Epoch: [14]  [860/969]  eta: 0:01:01  lr: 0.000309  loss: 0.6815 (0.6862)  time: 0.5672  data: 0.0002  max mem: 6820\n[23:40:22.214353] Epoch: [14]  [880/969]  eta: 0:00:50  lr: 0.000309  loss: 0.6847 (0.6862)  time: 0.5679  data: 0.0002  max mem: 6820\n[23:40:33.561588] Epoch: [14]  [900/969]  eta: 0:00:39  lr: 0.000309  loss: 0.6778 (0.6861)  time: 0.5673  data: 0.0002  max mem: 6820\n[23:40:44.891301] Epoch: [14]  [920/969]  eta: 0:00:27  lr: 0.000309  loss: 0.6787 (0.6861)  time: 0.5664  data: 0.0002  max mem: 6820\n[23:40:56.243568] Epoch: [14]  [940/969]  eta: 0:00:16  lr: 0.000309  loss: 0.6704 (0.6859)  time: 0.5676  data: 0.0002  max mem: 6820\n[23:41:07.572618] Epoch: [14]  [960/969]  eta: 0:00:05  lr: 0.000309  loss: 0.6950 (0.6863)  time: 0.5664  data: 0.0002  max mem: 6820\n[23:41:12.102049] Epoch: [14]  [968/969]  eta: 0:00:00  lr: 0.000309  loss: 0.6913 (0.6863)  time: 0.5661  data: 0.0002  max mem: 6820\n[23:41:12.226086] Epoch: [14] Total time: 0:09:10 (0.5677 s / it)\n[23:41:12.226221] Averaged stats: lr: 0.000309  loss: 0.6913 (0.6863)\n[23:41:12.973411] val:  [  0/139]  eta: 0:01:43  loss: 0.6854 (0.6854)  time: 0.7415  data: 0.6126  max mem: 6820\n[23:41:14.462135] val:  [ 10/139]  eta: 0:00:26  loss: 0.6564 (0.6467)  time: 0.2027  data: 0.0559  max mem: 6820\n[23:41:15.966854] val:  [ 20/139]  eta: 0:00:21  loss: 0.6244 (0.6422)  time: 0.1496  data: 0.0002  max mem: 6820\n[23:41:17.474523] val:  [ 30/139]  eta: 0:00:18  loss: 0.6499 (0.6424)  time: 0.1505  data: 0.0002  max mem: 6820\n[23:41:18.986700] val:  [ 40/139]  eta: 0:00:16  loss: 0.6540 (0.6484)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:41:20.494662] val:  [ 50/139]  eta: 0:00:14  loss: 0.6598 (0.6515)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:41:22.005853] val:  [ 60/139]  eta: 0:00:12  loss: 0.6682 (0.6534)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:41:23.513788] val:  [ 70/139]  eta: 0:00:10  loss: 0.6682 (0.6569)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:41:25.015647] val:  [ 80/139]  eta: 0:00:09  loss: 0.6793 (0.6631)  time: 0.1504  data: 0.0002  max mem: 6820\n[23:41:26.524519] val:  [ 90/139]  eta: 0:00:07  loss: 0.7059 (0.6688)  time: 0.1505  data: 0.0002  max mem: 6820\n[23:41:28.037344] val:  [100/139]  eta: 0:00:06  loss: 0.7128 (0.6733)  time: 0.1510  data: 0.0002  max mem: 6820\n[23:41:29.550208] val:  [110/139]  eta: 0:00:04  loss: 0.7096 (0.6756)  time: 0.1512  data: 0.0002  max mem: 6820\n[23:41:31.057369] val:  [120/139]  eta: 0:00:02  loss: 0.6906 (0.6767)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:41:32.569417] val:  [130/139]  eta: 0:00:01  loss: 0.6884 (0.6771)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:41:33.679491] val:  [138/139]  eta: 0:00:00  loss: 0.6791 (0.6780)  time: 0.1461  data: 0.0001  max mem: 6820\n[23:41:33.794956] val: Total time: 0:00:21 (0.1551 s / it)\n[23:41:33.850795] val loss: 0.6779563409819019\n[23:41:33.850844] Accuracy: 0.5841, F1 Score: 0.5827, ROC AUC: 0.6346, Hamming Loss: 0.4159,\n Jaccard Score: 0.4115, Precision: 0.5852, Recall: 0.5841,\n Average Precision: 0.6148, Kappa: 0.1682, Score: 0.4618\n[23:41:33.852807] Best epoch = 12, Best score = 0.4839\n[23:41:33.855622] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[23:41:35.210880] Epoch: [15]  [  0/969]  eta: 0:21:51  lr: 0.000309  loss: 0.6754 (0.6754)  time: 1.3539  data: 0.7515  max mem: 6820\n[23:41:46.538568] Epoch: [15]  [ 20/969]  eta: 0:09:33  lr: 0.000309  loss: 0.6816 (0.6847)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:41:57.869739] Epoch: [15]  [ 40/969]  eta: 0:09:04  lr: 0.000309  loss: 0.6890 (0.6847)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:42:09.256141] Epoch: [15]  [ 60/969]  eta: 0:08:47  lr: 0.000308  loss: 0.6682 (0.6840)  time: 0.5693  data: 0.0002  max mem: 6820\n[23:42:20.665416] Epoch: [15]  [ 80/969]  eta: 0:08:33  lr: 0.000308  loss: 0.6902 (0.6862)  time: 0.5704  data: 0.0002  max mem: 6820\n[23:42:31.937158] Epoch: [15]  [100/969]  eta: 0:08:19  lr: 0.000308  loss: 0.6775 (0.6866)  time: 0.5635  data: 0.0002  max mem: 6820\n[23:42:43.272552] Epoch: [15]  [120/969]  eta: 0:08:07  lr: 0.000308  loss: 0.6910 (0.6878)  time: 0.5667  data: 0.0002  max mem: 6820\n[23:42:54.644155] Epoch: [15]  [140/969]  eta: 0:07:54  lr: 0.000308  loss: 0.6912 (0.6877)  time: 0.5685  data: 0.0002  max mem: 6820\n[23:43:06.016762] Epoch: [15]  [160/969]  eta: 0:07:43  lr: 0.000308  loss: 0.6748 (0.6872)  time: 0.5686  data: 0.0002  max mem: 6820\n[23:43:17.391129] Epoch: [15]  [180/969]  eta: 0:07:31  lr: 0.000308  loss: 0.6839 (0.6866)  time: 0.5686  data: 0.0002  max mem: 6820\n[23:43:28.747806] Epoch: [15]  [200/969]  eta: 0:07:19  lr: 0.000308  loss: 0.6747 (0.6865)  time: 0.5678  data: 0.0002  max mem: 6820\n[23:43:40.108941] Epoch: [15]  [220/969]  eta: 0:07:07  lr: 0.000308  loss: 0.6716 (0.6856)  time: 0.5680  data: 0.0002  max mem: 6820\n[23:43:51.456511] Epoch: [15]  [240/969]  eta: 0:06:56  lr: 0.000308  loss: 0.6848 (0.6859)  time: 0.5673  data: 0.0002  max mem: 6820\n[23:44:02.820919] Epoch: [15]  [260/969]  eta: 0:06:44  lr: 0.000308  loss: 0.6894 (0.6861)  time: 0.5682  data: 0.0002  max mem: 6820\n[23:44:14.150633] Epoch: [15]  [280/969]  eta: 0:06:32  lr: 0.000308  loss: 0.6883 (0.6863)  time: 0.5664  data: 0.0002  max mem: 6820\n[23:44:25.475076] Epoch: [15]  [300/969]  eta: 0:06:21  lr: 0.000308  loss: 0.6940 (0.6872)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:44:36.815928] Epoch: [15]  [320/969]  eta: 0:06:09  lr: 0.000308  loss: 0.6782 (0.6874)  time: 0.5670  data: 0.0002  max mem: 6820\n[23:44:48.141065] Epoch: [15]  [340/969]  eta: 0:05:58  lr: 0.000308  loss: 0.6914 (0.6875)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:44:59.473175] Epoch: [15]  [360/969]  eta: 0:05:46  lr: 0.000308  loss: 0.6820 (0.6881)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:45:10.794940] Epoch: [15]  [380/969]  eta: 0:05:35  lr: 0.000308  loss: 0.6856 (0.6881)  time: 0.5660  data: 0.0002  max mem: 6820\n[23:45:22.128881] Epoch: [15]  [400/969]  eta: 0:05:23  lr: 0.000308  loss: 0.6734 (0.6875)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:45:33.478371] Epoch: [15]  [420/969]  eta: 0:05:12  lr: 0.000308  loss: 0.6725 (0.6876)  time: 0.5674  data: 0.0002  max mem: 6820\n[23:45:44.845174] Epoch: [15]  [440/969]  eta: 0:05:01  lr: 0.000308  loss: 0.6821 (0.6875)  time: 0.5683  data: 0.0002  max mem: 6820\n[23:45:56.188687] Epoch: [15]  [460/969]  eta: 0:04:49  lr: 0.000308  loss: 0.6688 (0.6873)  time: 0.5671  data: 0.0002  max mem: 6820\n[23:46:07.527292] Epoch: [15]  [480/969]  eta: 0:04:38  lr: 0.000308  loss: 0.6812 (0.6873)  time: 0.5669  data: 0.0002  max mem: 6820\n[23:46:18.908509] Epoch: [15]  [500/969]  eta: 0:04:26  lr: 0.000308  loss: 0.6902 (0.6878)  time: 0.5690  data: 0.0002  max mem: 6820\n[23:46:30.241486] Epoch: [15]  [520/969]  eta: 0:04:15  lr: 0.000308  loss: 0.6834 (0.6876)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:46:41.597619] Epoch: [15]  [540/969]  eta: 0:04:04  lr: 0.000308  loss: 0.6671 (0.6871)  time: 0.5678  data: 0.0002  max mem: 6820\n[23:46:52.953180] Epoch: [15]  [560/969]  eta: 0:03:52  lr: 0.000308  loss: 0.6681 (0.6871)  time: 0.5677  data: 0.0002  max mem: 6820\n[23:47:04.298892] Epoch: [15]  [580/969]  eta: 0:03:41  lr: 0.000308  loss: 0.6835 (0.6866)  time: 0.5672  data: 0.0002  max mem: 6820\n[23:47:15.639743] Epoch: [15]  [600/969]  eta: 0:03:29  lr: 0.000308  loss: 0.6812 (0.6867)  time: 0.5670  data: 0.0002  max mem: 6820\n[23:47:26.965013] Epoch: [15]  [620/969]  eta: 0:03:18  lr: 0.000308  loss: 0.6888 (0.6872)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:47:38.292310] Epoch: [15]  [640/969]  eta: 0:03:07  lr: 0.000308  loss: 0.6915 (0.6875)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:47:49.621433] Epoch: [15]  [660/969]  eta: 0:02:55  lr: 0.000307  loss: 0.6755 (0.6872)  time: 0.5664  data: 0.0002  max mem: 6820\n[23:48:00.966787] Epoch: [15]  [680/969]  eta: 0:02:44  lr: 0.000307  loss: 0.6723 (0.6872)  time: 0.5672  data: 0.0002  max mem: 6820\n[23:48:12.313269] Epoch: [15]  [700/969]  eta: 0:02:32  lr: 0.000307  loss: 0.6808 (0.6871)  time: 0.5673  data: 0.0002  max mem: 6820\n[23:48:23.641649] Epoch: [15]  [720/969]  eta: 0:02:21  lr: 0.000307  loss: 0.6895 (0.6872)  time: 0.5664  data: 0.0002  max mem: 6820\n[23:48:34.965616] Epoch: [15]  [740/969]  eta: 0:02:10  lr: 0.000307  loss: 0.6838 (0.6871)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:48:46.301853] Epoch: [15]  [760/969]  eta: 0:01:58  lr: 0.000307  loss: 0.6854 (0.6873)  time: 0.5668  data: 0.0002  max mem: 6820\n[23:48:57.611850] Epoch: [15]  [780/969]  eta: 0:01:47  lr: 0.000307  loss: 0.6864 (0.6873)  time: 0.5654  data: 0.0002  max mem: 6820\n[23:49:08.946435] Epoch: [15]  [800/969]  eta: 0:01:36  lr: 0.000307  loss: 0.6908 (0.6875)  time: 0.5667  data: 0.0002  max mem: 6820\n[23:49:20.275157] Epoch: [15]  [820/969]  eta: 0:01:24  lr: 0.000307  loss: 0.6964 (0.6877)  time: 0.5664  data: 0.0002  max mem: 6820\n[23:49:31.606425] Epoch: [15]  [840/969]  eta: 0:01:13  lr: 0.000307  loss: 0.6856 (0.6876)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:49:42.928922] Epoch: [15]  [860/969]  eta: 0:01:01  lr: 0.000307  loss: 0.6763 (0.6874)  time: 0.5661  data: 0.0002  max mem: 6820\n[23:49:54.282709] Epoch: [15]  [880/969]  eta: 0:00:50  lr: 0.000307  loss: 0.6857 (0.6872)  time: 0.5676  data: 0.0002  max mem: 6820\n[23:50:05.608259] Epoch: [15]  [900/969]  eta: 0:00:39  lr: 0.000307  loss: 0.6978 (0.6873)  time: 0.5662  data: 0.0002  max mem: 6820\n[23:50:16.931317] Epoch: [15]  [920/969]  eta: 0:00:27  lr: 0.000307  loss: 0.6848 (0.6871)  time: 0.5661  data: 0.0002  max mem: 6820\n[23:50:28.247446] Epoch: [15]  [940/969]  eta: 0:00:16  lr: 0.000307  loss: 0.6768 (0.6869)  time: 0.5658  data: 0.0002  max mem: 6820\n[23:50:39.579510] Epoch: [15]  [960/969]  eta: 0:00:05  lr: 0.000307  loss: 0.6845 (0.6870)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:50:44.102090] Epoch: [15]  [968/969]  eta: 0:00:00  lr: 0.000307  loss: 0.6832 (0.6869)  time: 0.5660  data: 0.0001  max mem: 6820\n[23:50:44.246672] Epoch: [15] Total time: 0:09:10 (0.5680 s / it)\n[23:50:44.246794] Averaged stats: lr: 0.000307  loss: 0.6832 (0.6869)\n[23:50:45.430589] val:  [  0/139]  eta: 0:02:42  loss: 0.6751 (0.6751)  time: 1.1712  data: 0.9908  max mem: 6820\n[23:50:46.917424] val:  [ 10/139]  eta: 0:00:31  loss: 0.6321 (0.6030)  time: 0.2415  data: 0.0904  max mem: 6820\n[23:50:48.422871] val:  [ 20/139]  eta: 0:00:23  loss: 0.5713 (0.5999)  time: 0.1495  data: 0.0003  max mem: 6820\n[23:50:49.927420] val:  [ 30/139]  eta: 0:00:19  loss: 0.6048 (0.6014)  time: 0.1504  data: 0.0002  max mem: 6820\n[23:50:51.440239] val:  [ 40/139]  eta: 0:00:17  loss: 0.6294 (0.6139)  time: 0.1508  data: 0.0002  max mem: 6820\n[23:50:52.951919] val:  [ 50/139]  eta: 0:00:15  loss: 0.6427 (0.6200)  time: 0.1512  data: 0.0002  max mem: 6820\n[23:50:54.463876] val:  [ 60/139]  eta: 0:00:13  loss: 0.6536 (0.6241)  time: 0.1511  data: 0.0002  max mem: 6820\n[23:50:55.975716] val:  [ 70/139]  eta: 0:00:11  loss: 0.6534 (0.6310)  time: 0.1511  data: 0.0002  max mem: 6820\n[23:50:57.484177] val:  [ 80/139]  eta: 0:00:09  loss: 0.6720 (0.6419)  time: 0.1509  data: 0.0002  max mem: 6820\n[23:50:58.999958] val:  [ 90/139]  eta: 0:00:07  loss: 0.7188 (0.6519)  time: 0.1511  data: 0.0002  max mem: 6820\n[23:51:00.516229] val:  [100/139]  eta: 0:00:06  loss: 0.7274 (0.6603)  time: 0.1515  data: 0.0002  max mem: 6820\n[23:51:02.028666] val:  [110/139]  eta: 0:00:04  loss: 0.7262 (0.6645)  time: 0.1514  data: 0.0002  max mem: 6820\n[23:51:03.540526] val:  [120/139]  eta: 0:00:03  loss: 0.6884 (0.6663)  time: 0.1511  data: 0.0002  max mem: 6820\n[23:51:05.056290] val:  [130/139]  eta: 0:00:01  loss: 0.6738 (0.6662)  time: 0.1513  data: 0.0002  max mem: 6820\n[23:51:06.167130] val:  [138/139]  eta: 0:00:00  loss: 0.6674 (0.6674)  time: 0.1464  data: 0.0002  max mem: 6820\n[23:51:06.277820] val: Total time: 0:00:22 (0.1585 s / it)\n[23:51:06.335445] val loss: 0.6674025281727742\n[23:51:06.335545] Accuracy: 0.6049, F1 Score: 0.6047, ROC AUC: 0.6370, Hamming Loss: 0.3951,\n Jaccard Score: 0.4334, Precision: 0.6051, Recall: 0.6049,\n Average Precision: 0.6182, Kappa: 0.2098, Score: 0.4838\n[23:51:06.337414] Best epoch = 12, Best score = 0.4839\n[23:51:06.340189] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[23:51:07.834902] Epoch: [16]  [  0/969]  eta: 0:24:07  lr: 0.000307  loss: 0.6662 (0.6662)  time: 1.4934  data: 0.9340  max mem: 6820\n[23:51:19.199314] Epoch: [16]  [ 20/969]  eta: 0:09:41  lr: 0.000307  loss: 0.6868 (0.6830)  time: 0.5682  data: 0.0002  max mem: 6820\n[23:51:30.588313] Epoch: [16]  [ 40/969]  eta: 0:09:09  lr: 0.000307  loss: 0.6599 (0.6710)  time: 0.5694  data: 0.0002  max mem: 6820\n[23:51:41.992888] Epoch: [16]  [ 60/969]  eta: 0:08:51  lr: 0.000307  loss: 0.6758 (0.6781)  time: 0.5702  data: 0.0002  max mem: 6820\n[23:51:53.347706] Epoch: [16]  [ 80/969]  eta: 0:08:35  lr: 0.000307  loss: 0.6770 (0.6780)  time: 0.5677  data: 0.0002  max mem: 6820\n[23:52:04.714351] Epoch: [16]  [100/969]  eta: 0:08:22  lr: 0.000307  loss: 0.6929 (0.6803)  time: 0.5683  data: 0.0002  max mem: 6820\n[23:52:16.071917] Epoch: [16]  [120/969]  eta: 0:08:09  lr: 0.000307  loss: 0.6814 (0.6816)  time: 0.5678  data: 0.0002  max mem: 6820\n[23:52:27.413091] Epoch: [16]  [140/969]  eta: 0:07:56  lr: 0.000307  loss: 0.6926 (0.6828)  time: 0.5670  data: 0.0002  max mem: 6820\n[23:52:38.724601] Epoch: [16]  [160/969]  eta: 0:07:44  lr: 0.000307  loss: 0.6640 (0.6819)  time: 0.5655  data: 0.0002  max mem: 6820\n[23:52:50.051919] Epoch: [16]  [180/969]  eta: 0:07:32  lr: 0.000307  loss: 0.6762 (0.6825)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:53:01.344137] Epoch: [16]  [200/969]  eta: 0:07:19  lr: 0.000306  loss: 0.6834 (0.6832)  time: 0.5646  data: 0.0002  max mem: 6820\n[23:53:12.631623] Epoch: [16]  [220/969]  eta: 0:07:07  lr: 0.000306  loss: 0.6880 (0.6839)  time: 0.5643  data: 0.0002  max mem: 6820\n[23:53:23.920528] Epoch: [16]  [240/969]  eta: 0:06:56  lr: 0.000306  loss: 0.6792 (0.6838)  time: 0.5644  data: 0.0002  max mem: 6820\n[23:53:35.217754] Epoch: [16]  [260/969]  eta: 0:06:44  lr: 0.000306  loss: 0.6915 (0.6844)  time: 0.5648  data: 0.0002  max mem: 6820\n[23:53:46.534348] Epoch: [16]  [280/969]  eta: 0:06:32  lr: 0.000306  loss: 0.6959 (0.6847)  time: 0.5658  data: 0.0002  max mem: 6820\n[23:53:57.813292] Epoch: [16]  [300/969]  eta: 0:06:21  lr: 0.000306  loss: 0.6759 (0.6849)  time: 0.5639  data: 0.0002  max mem: 6820\n[23:54:09.135756] Epoch: [16]  [320/969]  eta: 0:06:09  lr: 0.000306  loss: 0.6696 (0.6844)  time: 0.5661  data: 0.0001  max mem: 6820\n[23:54:20.454486] Epoch: [16]  [340/969]  eta: 0:05:58  lr: 0.000306  loss: 0.6802 (0.6848)  time: 0.5659  data: 0.0002  max mem: 6820\n[23:54:31.774848] Epoch: [16]  [360/969]  eta: 0:05:46  lr: 0.000306  loss: 0.6867 (0.6850)  time: 0.5660  data: 0.0002  max mem: 6820\n[23:54:43.084751] Epoch: [16]  [380/969]  eta: 0:05:35  lr: 0.000306  loss: 0.6780 (0.6848)  time: 0.5654  data: 0.0002  max mem: 6820\n[23:54:54.408520] Epoch: [16]  [400/969]  eta: 0:05:23  lr: 0.000306  loss: 0.6823 (0.6848)  time: 0.5661  data: 0.0002  max mem: 6820\n[23:55:05.735433] Epoch: [16]  [420/969]  eta: 0:05:12  lr: 0.000306  loss: 0.6915 (0.6853)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:55:17.043211] Epoch: [16]  [440/969]  eta: 0:05:00  lr: 0.000306  loss: 0.6797 (0.6849)  time: 0.5653  data: 0.0002  max mem: 6820\n[23:55:28.356461] Epoch: [16]  [460/969]  eta: 0:04:49  lr: 0.000306  loss: 0.6663 (0.6845)  time: 0.5656  data: 0.0002  max mem: 6820\n[23:55:39.687597] Epoch: [16]  [480/969]  eta: 0:04:37  lr: 0.000306  loss: 0.6832 (0.6850)  time: 0.5665  data: 0.0002  max mem: 6820\n[23:55:51.045125] Epoch: [16]  [500/969]  eta: 0:04:26  lr: 0.000306  loss: 0.6846 (0.6852)  time: 0.5678  data: 0.0002  max mem: 6820\n[23:56:02.371869] Epoch: [16]  [520/969]  eta: 0:04:15  lr: 0.000306  loss: 0.6912 (0.6851)  time: 0.5663  data: 0.0002  max mem: 6820\n[23:56:13.760587] Epoch: [16]  [540/969]  eta: 0:04:03  lr: 0.000306  loss: 0.6654 (0.6847)  time: 0.5694  data: 0.0002  max mem: 6820\n[23:56:25.144211] Epoch: [16]  [560/969]  eta: 0:03:52  lr: 0.000306  loss: 0.6365 (0.6837)  time: 0.5691  data: 0.0002  max mem: 6820\n[23:56:36.512657] Epoch: [16]  [580/969]  eta: 0:03:41  lr: 0.000306  loss: 0.6616 (0.6835)  time: 0.5684  data: 0.0002  max mem: 6820\n[23:56:47.850285] Epoch: [16]  [600/969]  eta: 0:03:29  lr: 0.000306  loss: 0.6915 (0.6840)  time: 0.5668  data: 0.0002  max mem: 6820\n[23:56:59.115055] Epoch: [16]  [620/969]  eta: 0:03:18  lr: 0.000306  loss: 0.6944 (0.6844)  time: 0.5632  data: 0.0001  max mem: 6820\n[23:57:10.374095] Epoch: [16]  [640/969]  eta: 0:03:06  lr: 0.000306  loss: 0.6887 (0.6845)  time: 0.5629  data: 0.0002  max mem: 6820\n[23:57:21.656098] Epoch: [16]  [660/969]  eta: 0:02:55  lr: 0.000306  loss: 0.6883 (0.6845)  time: 0.5641  data: 0.0002  max mem: 6820\n[23:57:32.924528] Epoch: [16]  [680/969]  eta: 0:02:44  lr: 0.000306  loss: 0.6855 (0.6848)  time: 0.5633  data: 0.0002  max mem: 6820\n[23:57:44.231905] Epoch: [16]  [700/969]  eta: 0:02:32  lr: 0.000305  loss: 0.6749 (0.6846)  time: 0.5653  data: 0.0002  max mem: 6820\n[23:57:55.565823] Epoch: [16]  [720/969]  eta: 0:02:21  lr: 0.000305  loss: 0.6930 (0.6850)  time: 0.5666  data: 0.0002  max mem: 6820\n[23:58:06.918475] Epoch: [16]  [740/969]  eta: 0:02:09  lr: 0.000305  loss: 0.6770 (0.6848)  time: 0.5676  data: 0.0002  max mem: 6820\n[23:58:18.246684] Epoch: [16]  [760/969]  eta: 0:01:58  lr: 0.000305  loss: 0.6919 (0.6850)  time: 0.5664  data: 0.0002  max mem: 6820\n[23:58:29.583691] Epoch: [16]  [780/969]  eta: 0:01:47  lr: 0.000305  loss: 0.6916 (0.6849)  time: 0.5668  data: 0.0002  max mem: 6820\n[23:58:40.954814] Epoch: [16]  [800/969]  eta: 0:01:35  lr: 0.000305  loss: 0.6920 (0.6852)  time: 0.5685  data: 0.0002  max mem: 6820\n[23:58:52.346827] Epoch: [16]  [820/969]  eta: 0:01:24  lr: 0.000305  loss: 0.6895 (0.6853)  time: 0.5696  data: 0.0002  max mem: 6820\n[23:59:03.710699] Epoch: [16]  [840/969]  eta: 0:01:13  lr: 0.000305  loss: 0.6877 (0.6853)  time: 0.5681  data: 0.0002  max mem: 6820\n[23:59:15.051747] Epoch: [16]  [860/969]  eta: 0:01:01  lr: 0.000305  loss: 0.6723 (0.6850)  time: 0.5670  data: 0.0002  max mem: 6820\n[23:59:26.412352] Epoch: [16]  [880/969]  eta: 0:00:50  lr: 0.000305  loss: 0.6706 (0.6848)  time: 0.5680  data: 0.0002  max mem: 6820\n[23:59:37.751162] Epoch: [16]  [900/969]  eta: 0:00:39  lr: 0.000305  loss: 0.6796 (0.6849)  time: 0.5669  data: 0.0002  max mem: 6820\n[23:59:49.091930] Epoch: [16]  [920/969]  eta: 0:00:27  lr: 0.000305  loss: 0.6810 (0.6846)  time: 0.5670  data: 0.0002  max mem: 6820\n[00:00:00.428358] Epoch: [16]  [940/969]  eta: 0:00:16  lr: 0.000305  loss: 0.6743 (0.6846)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:00:11.770737] Epoch: [16]  [960/969]  eta: 0:00:05  lr: 0.000305  loss: 0.6917 (0.6848)  time: 0.5671  data: 0.0002  max mem: 6820\n[00:00:16.302328] Epoch: [16]  [968/969]  eta: 0:00:00  lr: 0.000305  loss: 0.6829 (0.6848)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:00:16.422994] Epoch: [16] Total time: 0:09:10 (0.5677 s / it)\n[00:00:16.423115] Averaged stats: lr: 0.000305  loss: 0.6829 (0.6848)\n[00:00:17.372021] val:  [  0/139]  eta: 0:02:11  loss: 0.7035 (0.7035)  time: 0.9440  data: 0.7915  max mem: 6820\n[00:00:18.867784] val:  [ 10/139]  eta: 0:00:28  loss: 0.6588 (0.6326)  time: 0.2217  data: 0.0722  max mem: 6820\n[00:00:20.363416] val:  [ 20/139]  eta: 0:00:22  loss: 0.6094 (0.6279)  time: 0.1495  data: 0.0002  max mem: 6820\n[00:00:21.869026] val:  [ 30/139]  eta: 0:00:19  loss: 0.5945 (0.6239)  time: 0.1500  data: 0.0002  max mem: 6820\n[00:00:23.375556] val:  [ 40/139]  eta: 0:00:16  loss: 0.6343 (0.6334)  time: 0.1505  data: 0.0002  max mem: 6820\n[00:00:24.890360] val:  [ 50/139]  eta: 0:00:14  loss: 0.6448 (0.6370)  time: 0.1510  data: 0.0002  max mem: 6820\n[00:00:26.401799] val:  [ 60/139]  eta: 0:00:12  loss: 0.6485 (0.6380)  time: 0.1512  data: 0.0002  max mem: 6820\n[00:00:27.910690] val:  [ 70/139]  eta: 0:00:11  loss: 0.6555 (0.6417)  time: 0.1509  data: 0.0002  max mem: 6820\n[00:00:29.421924] val:  [ 80/139]  eta: 0:00:09  loss: 0.6578 (0.6471)  time: 0.1509  data: 0.0002  max mem: 6820\n[00:00:30.932178] val:  [ 90/139]  eta: 0:00:07  loss: 0.7321 (0.6581)  time: 0.1510  data: 0.0002  max mem: 6820\n[00:00:32.445664] val:  [100/139]  eta: 0:00:06  loss: 0.7438 (0.6668)  time: 0.1511  data: 0.0002  max mem: 6820\n[00:00:33.956263] val:  [110/139]  eta: 0:00:04  loss: 0.7302 (0.6701)  time: 0.1511  data: 0.0002  max mem: 6820\n[00:00:35.470432] val:  [120/139]  eta: 0:00:02  loss: 0.6829 (0.6700)  time: 0.1512  data: 0.0002  max mem: 6820\n[00:00:36.980476] val:  [130/139]  eta: 0:00:01  loss: 0.6288 (0.6664)  time: 0.1511  data: 0.0002  max mem: 6820\n[00:00:38.091121] val:  [138/139]  eta: 0:00:00  loss: 0.6284 (0.6664)  time: 0.1460  data: 0.0001  max mem: 6820\n[00:00:38.211579] val: Total time: 0:00:21 (0.1567 s / it)\n[00:00:38.270569] val loss: 0.6663721756969424\n[00:00:38.270651] Accuracy: 0.6053, F1 Score: 0.6048, ROC AUC: 0.6377, Hamming Loss: 0.3947,\n Jaccard Score: 0.4336, Precision: 0.6060, Recall: 0.6053,\n Average Precision: 0.6199, Kappa: 0.2107, Score: 0.4844\n[00:00:41.383759] Best epoch = 16, Best score = 0.4844\n[00:00:41.386528] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[00:00:43.091910] Epoch: [17]  [  0/969]  eta: 0:27:30  lr: 0.000305  loss: 0.6632 (0.6632)  time: 1.7035  data: 1.0372  max mem: 6820\n[00:00:54.306439] Epoch: [17]  [ 20/969]  eta: 0:09:43  lr: 0.000305  loss: 0.6834 (0.6855)  time: 0.5607  data: 0.0002  max mem: 6820\n[00:01:05.772833] Epoch: [17]  [ 40/969]  eta: 0:09:12  lr: 0.000305  loss: 0.6718 (0.6848)  time: 0.5733  data: 0.0002  max mem: 6820\n[00:01:17.299210] Epoch: [17]  [ 60/969]  eta: 0:08:55  lr: 0.000305  loss: 0.6695 (0.6798)  time: 0.5763  data: 0.0002  max mem: 6820\n[00:01:28.722539] Epoch: [17]  [ 80/969]  eta: 0:08:39  lr: 0.000305  loss: 0.6765 (0.6819)  time: 0.5711  data: 0.0002  max mem: 6820\n[00:01:40.060897] Epoch: [17]  [100/969]  eta: 0:08:24  lr: 0.000305  loss: 0.6810 (0.6835)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:01:51.341845] Epoch: [17]  [120/969]  eta: 0:08:10  lr: 0.000305  loss: 0.6977 (0.6851)  time: 0.5640  data: 0.0002  max mem: 6820\n[00:02:02.614101] Epoch: [17]  [140/969]  eta: 0:07:57  lr: 0.000305  loss: 0.6944 (0.6865)  time: 0.5636  data: 0.0002  max mem: 6820\n[00:02:13.923884] Epoch: [17]  [160/969]  eta: 0:07:44  lr: 0.000305  loss: 0.6873 (0.6870)  time: 0.5654  data: 0.0002  max mem: 6820\n[00:02:25.171825] Epoch: [17]  [180/969]  eta: 0:07:32  lr: 0.000304  loss: 0.6790 (0.6868)  time: 0.5624  data: 0.0002  max mem: 6820\n[00:02:36.534124] Epoch: [17]  [200/969]  eta: 0:07:20  lr: 0.000304  loss: 0.7071 (0.6880)  time: 0.5681  data: 0.0002  max mem: 6820\n[00:02:47.915073] Epoch: [17]  [220/969]  eta: 0:07:08  lr: 0.000304  loss: 0.6811 (0.6877)  time: 0.5690  data: 0.0002  max mem: 6820\n[00:02:59.269565] Epoch: [17]  [240/969]  eta: 0:06:57  lr: 0.000304  loss: 0.6858 (0.6879)  time: 0.5677  data: 0.0002  max mem: 6820\n[00:03:10.633238] Epoch: [17]  [260/969]  eta: 0:06:45  lr: 0.000304  loss: 0.6777 (0.6870)  time: 0.5681  data: 0.0002  max mem: 6820\n[00:03:21.962143] Epoch: [17]  [280/969]  eta: 0:06:33  lr: 0.000304  loss: 0.6694 (0.6860)  time: 0.5664  data: 0.0002  max mem: 6820\n[00:03:33.300303] Epoch: [17]  [300/969]  eta: 0:06:22  lr: 0.000304  loss: 0.6905 (0.6866)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:03:44.637857] Epoch: [17]  [320/969]  eta: 0:06:10  lr: 0.000304  loss: 0.6738 (0.6863)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:03:55.951300] Epoch: [17]  [340/969]  eta: 0:05:58  lr: 0.000304  loss: 0.6784 (0.6861)  time: 0.5656  data: 0.0002  max mem: 6820\n[00:04:07.263258] Epoch: [17]  [360/969]  eta: 0:05:47  lr: 0.000304  loss: 0.6792 (0.6863)  time: 0.5655  data: 0.0002  max mem: 6820\n[00:04:18.585688] Epoch: [17]  [380/969]  eta: 0:05:35  lr: 0.000304  loss: 0.6799 (0.6860)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:04:29.912914] Epoch: [17]  [400/969]  eta: 0:05:24  lr: 0.000304  loss: 0.6841 (0.6860)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:04:41.209807] Epoch: [17]  [420/969]  eta: 0:05:12  lr: 0.000304  loss: 0.6829 (0.6862)  time: 0.5648  data: 0.0002  max mem: 6820\n[00:04:52.510276] Epoch: [17]  [440/969]  eta: 0:05:01  lr: 0.000304  loss: 0.6669 (0.6859)  time: 0.5650  data: 0.0002  max mem: 6820\n[00:05:03.843543] Epoch: [17]  [460/969]  eta: 0:04:49  lr: 0.000304  loss: 0.6873 (0.6859)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:05:15.171087] Epoch: [17]  [480/969]  eta: 0:04:38  lr: 0.000304  loss: 0.6841 (0.6859)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:05:26.490316] Epoch: [17]  [500/969]  eta: 0:04:26  lr: 0.000304  loss: 0.6754 (0.6860)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:05:37.815492] Epoch: [17]  [520/969]  eta: 0:04:15  lr: 0.000304  loss: 0.6858 (0.6857)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:05:49.161128] Epoch: [17]  [540/969]  eta: 0:04:04  lr: 0.000304  loss: 0.6660 (0.6852)  time: 0.5672  data: 0.0002  max mem: 6820\n[00:06:00.539411] Epoch: [17]  [560/969]  eta: 0:03:52  lr: 0.000304  loss: 0.6618 (0.6850)  time: 0.5689  data: 0.0002  max mem: 6820\n[00:06:11.913418] Epoch: [17]  [580/969]  eta: 0:03:41  lr: 0.000304  loss: 0.6834 (0.6854)  time: 0.5687  data: 0.0002  max mem: 6820\n[00:06:23.269336] Epoch: [17]  [600/969]  eta: 0:03:29  lr: 0.000303  loss: 0.6889 (0.6857)  time: 0.5677  data: 0.0002  max mem: 6820\n[00:06:34.597873] Epoch: [17]  [620/969]  eta: 0:03:18  lr: 0.000303  loss: 0.6951 (0.6861)  time: 0.5664  data: 0.0002  max mem: 6820\n[00:06:45.953569] Epoch: [17]  [640/969]  eta: 0:03:07  lr: 0.000303  loss: 0.6787 (0.6859)  time: 0.5677  data: 0.0002  max mem: 6820\n[00:06:57.284780] Epoch: [17]  [660/969]  eta: 0:02:55  lr: 0.000303  loss: 0.6703 (0.6856)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:07:08.619553] Epoch: [17]  [680/969]  eta: 0:02:44  lr: 0.000303  loss: 0.6798 (0.6859)  time: 0.5667  data: 0.0002  max mem: 6820\n[00:07:19.954934] Epoch: [17]  [700/969]  eta: 0:02:32  lr: 0.000303  loss: 0.6738 (0.6855)  time: 0.5667  data: 0.0002  max mem: 6820\n[00:07:31.273720] Epoch: [17]  [720/969]  eta: 0:02:21  lr: 0.000303  loss: 0.6917 (0.6857)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:07:42.584318] Epoch: [17]  [740/969]  eta: 0:02:10  lr: 0.000303  loss: 0.6989 (0.6858)  time: 0.5655  data: 0.0002  max mem: 6820\n[00:07:53.900365] Epoch: [17]  [760/969]  eta: 0:01:58  lr: 0.000303  loss: 0.6771 (0.6857)  time: 0.5657  data: 0.0002  max mem: 6820\n[00:08:05.196427] Epoch: [17]  [780/969]  eta: 0:01:47  lr: 0.000303  loss: 0.6883 (0.6857)  time: 0.5648  data: 0.0002  max mem: 6820\n[00:08:16.495065] Epoch: [17]  [800/969]  eta: 0:01:36  lr: 0.000303  loss: 0.6920 (0.6860)  time: 0.5649  data: 0.0002  max mem: 6820\n[00:08:27.799227] Epoch: [17]  [820/969]  eta: 0:01:24  lr: 0.000303  loss: 0.6648 (0.6857)  time: 0.5652  data: 0.0002  max mem: 6820\n[00:08:39.124157] Epoch: [17]  [840/969]  eta: 0:01:13  lr: 0.000303  loss: 0.6748 (0.6857)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:08:50.428707] Epoch: [17]  [860/969]  eta: 0:01:01  lr: 0.000303  loss: 0.6960 (0.6859)  time: 0.5652  data: 0.0002  max mem: 6820\n[00:09:01.690140] Epoch: [17]  [880/969]  eta: 0:00:50  lr: 0.000303  loss: 0.6661 (0.6857)  time: 0.5630  data: 0.0002  max mem: 6820\n[00:09:12.976100] Epoch: [17]  [900/969]  eta: 0:00:39  lr: 0.000303  loss: 0.6961 (0.6859)  time: 0.5643  data: 0.0002  max mem: 6820\n[00:09:24.272509] Epoch: [17]  [920/969]  eta: 0:00:27  lr: 0.000303  loss: 0.6595 (0.6857)  time: 0.5648  data: 0.0002  max mem: 6820\n[00:09:35.600470] Epoch: [17]  [940/969]  eta: 0:00:16  lr: 0.000303  loss: 0.6806 (0.6854)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:09:46.922859] Epoch: [17]  [960/969]  eta: 0:00:05  lr: 0.000303  loss: 0.6768 (0.6855)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:09:51.434424] Epoch: [17]  [968/969]  eta: 0:00:00  lr: 0.000303  loss: 0.6734 (0.6855)  time: 0.5654  data: 0.0002  max mem: 6820\n[00:09:51.555515] Epoch: [17] Total time: 0:09:10 (0.5678 s / it)\n[00:09:51.555623] Averaged stats: lr: 0.000303  loss: 0.6734 (0.6855)\n[00:09:52.301571] val:  [  0/139]  eta: 0:01:43  loss: 0.7132 (0.7132)  time: 0.7415  data: 0.5953  max mem: 6820\n[00:09:53.798261] val:  [ 10/139]  eta: 0:00:26  loss: 0.6479 (0.6265)  time: 0.2031  data: 0.0543  max mem: 6820\n[00:09:55.300580] val:  [ 20/139]  eta: 0:00:21  loss: 0.5769 (0.6223)  time: 0.1497  data: 0.0002  max mem: 6820\n[00:09:56.802898] val:  [ 30/139]  eta: 0:00:18  loss: 0.5769 (0.6194)  time: 0.1502  data: 0.0002  max mem: 6820\n[00:09:58.309383] val:  [ 40/139]  eta: 0:00:16  loss: 0.6383 (0.6318)  time: 0.1504  data: 0.0002  max mem: 6820\n[00:09:59.824718] val:  [ 50/139]  eta: 0:00:14  loss: 0.6455 (0.6374)  time: 0.1510  data: 0.0002  max mem: 6820\n[00:10:01.327091] val:  [ 60/139]  eta: 0:00:12  loss: 0.6514 (0.6397)  time: 0.1508  data: 0.0002  max mem: 6820\n[00:10:02.837510] val:  [ 70/139]  eta: 0:00:10  loss: 0.6665 (0.6438)  time: 0.1506  data: 0.0002  max mem: 6820\n[00:10:04.349702] val:  [ 80/139]  eta: 0:00:09  loss: 0.6626 (0.6485)  time: 0.1510  data: 0.0002  max mem: 6820\n[00:10:05.864241] val:  [ 90/139]  eta: 0:00:07  loss: 0.7197 (0.6589)  time: 0.1513  data: 0.0002  max mem: 6820\n[00:10:07.378769] val:  [100/139]  eta: 0:00:06  loss: 0.7357 (0.6673)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:10:08.889144] val:  [110/139]  eta: 0:00:04  loss: 0.7243 (0.6697)  time: 0.1512  data: 0.0002  max mem: 6820\n[00:10:10.394787] val:  [120/139]  eta: 0:00:02  loss: 0.6935 (0.6698)  time: 0.1507  data: 0.0002  max mem: 6820\n[00:10:11.908402] val:  [130/139]  eta: 0:00:01  loss: 0.6203 (0.6649)  time: 0.1509  data: 0.0001  max mem: 6820\n[00:10:13.018115] val:  [138/139]  eta: 0:00:00  loss: 0.6143 (0.6633)  time: 0.1461  data: 0.0001  max mem: 6820\n[00:10:13.120467] val: Total time: 0:00:21 (0.1551 s / it)\n[00:10:13.176098] val loss: 0.6632810084939861\n[00:10:13.176488] Accuracy: 0.6153, F1 Score: 0.6142, ROC AUC: 0.6433, Hamming Loss: 0.3847,\n Jaccard Score: 0.4436, Precision: 0.6165, Recall: 0.6153,\n Average Precision: 0.6310, Kappa: 0.2306, Score: 0.4960\n[00:10:15.947579] Best epoch = 17, Best score = 0.4960\n[00:10:15.950270] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[00:10:17.328846] Epoch: [18]  [  0/969]  eta: 0:22:14  lr: 0.000303  loss: 0.6822 (0.6822)  time: 1.3772  data: 0.7911  max mem: 6820\n[00:10:28.541292] Epoch: [18]  [ 20/969]  eta: 0:09:28  lr: 0.000303  loss: 0.6817 (0.6801)  time: 0.5606  data: 0.0002  max mem: 6820\n[00:10:39.917100] Epoch: [18]  [ 40/969]  eta: 0:09:02  lr: 0.000302  loss: 0.6854 (0.6835)  time: 0.5687  data: 0.0002  max mem: 6820\n[00:10:51.367077] Epoch: [18]  [ 60/969]  eta: 0:08:47  lr: 0.000302  loss: 0.6765 (0.6827)  time: 0.5724  data: 0.0002  max mem: 6820\n[00:11:02.772910] Epoch: [18]  [ 80/969]  eta: 0:08:33  lr: 0.000302  loss: 0.6842 (0.6828)  time: 0.5702  data: 0.0002  max mem: 6820\n[00:11:14.126103] Epoch: [18]  [100/969]  eta: 0:08:20  lr: 0.000302  loss: 0.7028 (0.6861)  time: 0.5676  data: 0.0002  max mem: 6820\n[00:11:25.406843] Epoch: [18]  [120/969]  eta: 0:08:07  lr: 0.000302  loss: 0.6897 (0.6867)  time: 0.5640  data: 0.0002  max mem: 6820\n[00:11:36.652338] Epoch: [18]  [140/969]  eta: 0:07:54  lr: 0.000302  loss: 0.6789 (0.6862)  time: 0.5622  data: 0.0002  max mem: 6820\n[00:11:47.954879] Epoch: [18]  [160/969]  eta: 0:07:42  lr: 0.000302  loss: 0.6694 (0.6849)  time: 0.5651  data: 0.0002  max mem: 6820\n[00:11:59.279180] Epoch: [18]  [180/969]  eta: 0:07:30  lr: 0.000302  loss: 0.6752 (0.6852)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:12:10.647041] Epoch: [18]  [200/969]  eta: 0:07:18  lr: 0.000302  loss: 0.6982 (0.6866)  time: 0.5683  data: 0.0002  max mem: 6820\n[00:12:21.986960] Epoch: [18]  [220/969]  eta: 0:07:07  lr: 0.000302  loss: 0.6947 (0.6868)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:12:33.335838] Epoch: [18]  [240/969]  eta: 0:06:55  lr: 0.000302  loss: 0.6906 (0.6874)  time: 0.5674  data: 0.0002  max mem: 6820\n[00:12:44.679198] Epoch: [18]  [260/969]  eta: 0:06:43  lr: 0.000302  loss: 0.6853 (0.6871)  time: 0.5671  data: 0.0002  max mem: 6820\n[00:12:56.026431] Epoch: [18]  [280/969]  eta: 0:06:32  lr: 0.000302  loss: 0.6815 (0.6877)  time: 0.5673  data: 0.0002  max mem: 6820\n[00:13:07.346965] Epoch: [18]  [300/969]  eta: 0:06:20  lr: 0.000302  loss: 0.6853 (0.6878)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:13:18.677230] Epoch: [18]  [320/969]  eta: 0:06:09  lr: 0.000302  loss: 0.6694 (0.6871)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:13:30.003598] Epoch: [18]  [340/969]  eta: 0:05:57  lr: 0.000302  loss: 0.6831 (0.6873)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:13:41.324436] Epoch: [18]  [360/969]  eta: 0:05:46  lr: 0.000302  loss: 0.6853 (0.6874)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:13:52.632047] Epoch: [18]  [380/969]  eta: 0:05:34  lr: 0.000302  loss: 0.6834 (0.6873)  time: 0.5653  data: 0.0002  max mem: 6820\n[00:14:03.959272] Epoch: [18]  [400/969]  eta: 0:05:23  lr: 0.000302  loss: 0.6835 (0.6869)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:14:15.258077] Epoch: [18]  [420/969]  eta: 0:05:12  lr: 0.000301  loss: 0.6804 (0.6866)  time: 0.5649  data: 0.0002  max mem: 6820\n[00:14:26.563421] Epoch: [18]  [440/969]  eta: 0:05:00  lr: 0.000301  loss: 0.6736 (0.6866)  time: 0.5652  data: 0.0002  max mem: 6820\n[00:14:37.876142] Epoch: [18]  [460/969]  eta: 0:04:49  lr: 0.000301  loss: 0.6728 (0.6862)  time: 0.5656  data: 0.0002  max mem: 6820\n[00:14:49.169687] Epoch: [18]  [480/969]  eta: 0:04:37  lr: 0.000301  loss: 0.7000 (0.6864)  time: 0.5646  data: 0.0002  max mem: 6820\n[00:15:00.459052] Epoch: [18]  [500/969]  eta: 0:04:26  lr: 0.000301  loss: 0.6920 (0.6868)  time: 0.5644  data: 0.0002  max mem: 6820\n[00:15:11.760230] Epoch: [18]  [520/969]  eta: 0:04:14  lr: 0.000301  loss: 0.6732 (0.6865)  time: 0.5650  data: 0.0002  max mem: 6820\n[00:15:23.085938] Epoch: [18]  [540/969]  eta: 0:04:03  lr: 0.000301  loss: 0.6705 (0.6861)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:15:34.411694] Epoch: [18]  [560/969]  eta: 0:03:52  lr: 0.000301  loss: 0.6634 (0.6858)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:15:45.744051] Epoch: [18]  [580/969]  eta: 0:03:40  lr: 0.000301  loss: 0.6666 (0.6853)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:15:57.076962] Epoch: [18]  [600/969]  eta: 0:03:29  lr: 0.000301  loss: 0.6554 (0.6848)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:16:08.378877] Epoch: [18]  [620/969]  eta: 0:03:18  lr: 0.000301  loss: 0.6994 (0.6851)  time: 0.5650  data: 0.0002  max mem: 6820\n[00:16:19.670790] Epoch: [18]  [640/969]  eta: 0:03:06  lr: 0.000301  loss: 0.6775 (0.6850)  time: 0.5645  data: 0.0002  max mem: 6820\n[00:16:30.996414] Epoch: [18]  [660/969]  eta: 0:02:55  lr: 0.000301  loss: 0.6763 (0.6850)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:16:42.318350] Epoch: [18]  [680/969]  eta: 0:02:43  lr: 0.000301  loss: 0.6880 (0.6854)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:16:53.654592] Epoch: [18]  [700/969]  eta: 0:02:32  lr: 0.000301  loss: 0.6899 (0.6856)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:17:04.965208] Epoch: [18]  [720/969]  eta: 0:02:21  lr: 0.000301  loss: 0.6977 (0.6861)  time: 0.5655  data: 0.0002  max mem: 6820\n[00:17:16.286100] Epoch: [18]  [740/969]  eta: 0:02:09  lr: 0.000301  loss: 0.6700 (0.6859)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:17:27.619728] Epoch: [18]  [760/969]  eta: 0:01:58  lr: 0.000301  loss: 0.6770 (0.6859)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:17:38.974312] Epoch: [18]  [780/969]  eta: 0:01:47  lr: 0.000300  loss: 0.6622 (0.6856)  time: 0.5677  data: 0.0002  max mem: 6820\n[00:17:50.354536] Epoch: [18]  [800/969]  eta: 0:01:35  lr: 0.000300  loss: 0.6714 (0.6858)  time: 0.5690  data: 0.0002  max mem: 6820\n[00:18:01.731791] Epoch: [18]  [820/969]  eta: 0:01:24  lr: 0.000300  loss: 0.6838 (0.6858)  time: 0.5688  data: 0.0002  max mem: 6820\n[00:18:13.111772] Epoch: [18]  [840/969]  eta: 0:01:13  lr: 0.000300  loss: 0.6812 (0.6856)  time: 0.5690  data: 0.0002  max mem: 6820\n[00:18:24.478634] Epoch: [18]  [860/969]  eta: 0:01:01  lr: 0.000300  loss: 0.6741 (0.6857)  time: 0.5683  data: 0.0002  max mem: 6820\n[00:18:35.828674] Epoch: [18]  [880/969]  eta: 0:00:50  lr: 0.000300  loss: 0.6715 (0.6856)  time: 0.5675  data: 0.0002  max mem: 6820\n[00:18:47.163807] Epoch: [18]  [900/969]  eta: 0:00:39  lr: 0.000300  loss: 0.6735 (0.6854)  time: 0.5667  data: 0.0002  max mem: 6820\n[00:18:58.539194] Epoch: [18]  [920/969]  eta: 0:00:27  lr: 0.000300  loss: 0.6833 (0.6852)  time: 0.5687  data: 0.0002  max mem: 6820\n[00:19:09.876760] Epoch: [18]  [940/969]  eta: 0:00:16  lr: 0.000300  loss: 0.6819 (0.6853)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:19:21.210036] Epoch: [18]  [960/969]  eta: 0:00:05  lr: 0.000300  loss: 0.6835 (0.6854)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:19:25.747578] Epoch: [18]  [968/969]  eta: 0:00:00  lr: 0.000300  loss: 0.6834 (0.6854)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:19:25.866191] Epoch: [18] Total time: 0:09:09 (0.5675 s / it)\n[00:19:25.866305] Averaged stats: lr: 0.000300  loss: 0.6834 (0.6854)\n[00:19:26.766783] val:  [  0/139]  eta: 0:02:04  loss: 0.6593 (0.6593)  time: 0.8958  data: 0.7635  max mem: 6820\n[00:19:28.266466] val:  [ 10/139]  eta: 0:00:28  loss: 0.6241 (0.5931)  time: 0.2177  data: 0.0696  max mem: 6820\n[00:19:29.775713] val:  [ 20/139]  eta: 0:00:22  loss: 0.5662 (0.5907)  time: 0.1504  data: 0.0002  max mem: 6820\n[00:19:31.275516] val:  [ 30/139]  eta: 0:00:18  loss: 0.5662 (0.5915)  time: 0.1504  data: 0.0002  max mem: 6820\n[00:19:32.786861] val:  [ 40/139]  eta: 0:00:16  loss: 0.6015 (0.6015)  time: 0.1505  data: 0.0002  max mem: 6820\n[00:19:34.297894] val:  [ 50/139]  eta: 0:00:14  loss: 0.6224 (0.6079)  time: 0.1510  data: 0.0002  max mem: 6820\n[00:19:35.802585] val:  [ 60/139]  eta: 0:00:12  loss: 0.6269 (0.6137)  time: 0.1507  data: 0.0002  max mem: 6820\n[00:19:37.315449] val:  [ 70/139]  eta: 0:00:11  loss: 0.6495 (0.6211)  time: 0.1508  data: 0.0002  max mem: 6820\n[00:19:38.823275] val:  [ 80/139]  eta: 0:00:09  loss: 0.6726 (0.6319)  time: 0.1510  data: 0.0002  max mem: 6820\n[00:19:40.334200] val:  [ 90/139]  eta: 0:00:07  loss: 0.7294 (0.6439)  time: 0.1509  data: 0.0002  max mem: 6820\n[00:19:41.850454] val:  [100/139]  eta: 0:00:06  loss: 0.7440 (0.6543)  time: 0.1513  data: 0.0002  max mem: 6820\n[00:19:43.359571] val:  [110/139]  eta: 0:00:04  loss: 0.7322 (0.6607)  time: 0.1512  data: 0.0002  max mem: 6820\n[00:19:44.868407] val:  [120/139]  eta: 0:00:02  loss: 0.7168 (0.6648)  time: 0.1508  data: 0.0002  max mem: 6820\n[00:19:46.374463] val:  [130/139]  eta: 0:00:01  loss: 0.6688 (0.6643)  time: 0.1507  data: 0.0001  max mem: 6820\n[00:19:47.485838] val:  [138/139]  eta: 0:00:00  loss: 0.6653 (0.6653)  time: 0.1459  data: 0.0001  max mem: 6820\n[00:19:47.593151] val: Total time: 0:00:21 (0.1563 s / it)\n[00:19:47.652431] val loss: 0.6652500509358138\n[00:19:47.652479] Accuracy: 0.6171, F1 Score: 0.6169, ROC AUC: 0.6482, Hamming Loss: 0.3829,\n Jaccard Score: 0.4461, Precision: 0.6173, Recall: 0.6171,\n Average Precision: 0.6320, Kappa: 0.2342, Score: 0.4998\n[00:19:50.468763] Best epoch = 18, Best score = 0.4998\n[00:19:50.471568] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[00:19:51.909418] Epoch: [19]  [  0/969]  eta: 0:23:12  lr: 0.000300  loss: 0.6944 (0.6944)  time: 1.4366  data: 0.8510  max mem: 6820\n[00:20:03.124122] Epoch: [19]  [ 20/969]  eta: 0:09:31  lr: 0.000300  loss: 0.6714 (0.6831)  time: 0.5607  data: 0.0002  max mem: 6820\n[00:20:14.525569] Epoch: [19]  [ 40/969]  eta: 0:09:04  lr: 0.000300  loss: 0.6768 (0.6785)  time: 0.5700  data: 0.0002  max mem: 6820\n[00:20:25.983200] Epoch: [19]  [ 60/969]  eta: 0:08:49  lr: 0.000300  loss: 0.6707 (0.6772)  time: 0.5728  data: 0.0002  max mem: 6820\n[00:20:37.454262] Epoch: [19]  [ 80/969]  eta: 0:08:35  lr: 0.000300  loss: 0.6720 (0.6774)  time: 0.5735  data: 0.0002  max mem: 6820\n[00:20:48.825646] Epoch: [19]  [100/969]  eta: 0:08:22  lr: 0.000300  loss: 0.6973 (0.6825)  time: 0.5685  data: 0.0002  max mem: 6820\n[00:21:00.138857] Epoch: [19]  [120/969]  eta: 0:08:08  lr: 0.000300  loss: 0.6756 (0.6811)  time: 0.5656  data: 0.0002  max mem: 6820\n[00:21:11.425995] Epoch: [19]  [140/969]  eta: 0:07:55  lr: 0.000300  loss: 0.6907 (0.6822)  time: 0.5643  data: 0.0002  max mem: 6820\n[00:21:22.720333] Epoch: [19]  [160/969]  eta: 0:07:43  lr: 0.000300  loss: 0.6617 (0.6806)  time: 0.5647  data: 0.0002  max mem: 6820\n[00:21:34.072816] Epoch: [19]  [180/969]  eta: 0:07:31  lr: 0.000299  loss: 0.6554 (0.6803)  time: 0.5676  data: 0.0002  max mem: 6820\n[00:21:45.398695] Epoch: [19]  [200/969]  eta: 0:07:19  lr: 0.000299  loss: 0.6847 (0.6815)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:21:56.724792] Epoch: [19]  [220/969]  eta: 0:07:07  lr: 0.000299  loss: 0.6805 (0.6821)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:22:08.056139] Epoch: [19]  [240/969]  eta: 0:06:56  lr: 0.000299  loss: 0.6863 (0.6822)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:22:19.317170] Epoch: [19]  [260/969]  eta: 0:06:44  lr: 0.000299  loss: 0.6917 (0.6833)  time: 0.5630  data: 0.0002  max mem: 6820\n[00:22:30.678042] Epoch: [19]  [280/969]  eta: 0:06:32  lr: 0.000299  loss: 0.6840 (0.6833)  time: 0.5680  data: 0.0002  max mem: 6820\n[00:22:42.040568] Epoch: [19]  [300/969]  eta: 0:06:21  lr: 0.000299  loss: 0.6909 (0.6842)  time: 0.5681  data: 0.0002  max mem: 6820\n[00:22:53.414478] Epoch: [19]  [320/969]  eta: 0:06:09  lr: 0.000299  loss: 0.6785 (0.6840)  time: 0.5686  data: 0.0002  max mem: 6820\n[00:23:04.770957] Epoch: [19]  [340/969]  eta: 0:05:58  lr: 0.000299  loss: 0.6967 (0.6847)  time: 0.5678  data: 0.0002  max mem: 6820\n[00:23:16.102403] Epoch: [19]  [360/969]  eta: 0:05:46  lr: 0.000299  loss: 0.6805 (0.6849)  time: 0.5665  data: 0.0003  max mem: 6820\n[00:23:27.425288] Epoch: [19]  [380/969]  eta: 0:05:35  lr: 0.000299  loss: 0.6669 (0.6849)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:23:38.763651] Epoch: [19]  [400/969]  eta: 0:05:23  lr: 0.000299  loss: 0.6739 (0.6845)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:23:50.104499] Epoch: [19]  [420/969]  eta: 0:05:12  lr: 0.000299  loss: 0.6768 (0.6839)  time: 0.5670  data: 0.0002  max mem: 6820\n[00:24:01.430690] Epoch: [19]  [440/969]  eta: 0:05:01  lr: 0.000299  loss: 0.6807 (0.6839)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:24:12.749284] Epoch: [19]  [460/969]  eta: 0:04:49  lr: 0.000299  loss: 0.6586 (0.6835)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:24:24.082375] Epoch: [19]  [480/969]  eta: 0:04:38  lr: 0.000299  loss: 0.6710 (0.6836)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:24:35.406216] Epoch: [19]  [500/969]  eta: 0:04:26  lr: 0.000299  loss: 0.6867 (0.6839)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:24:46.744023] Epoch: [19]  [520/969]  eta: 0:04:15  lr: 0.000298  loss: 0.6734 (0.6838)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:24:58.061421] Epoch: [19]  [540/969]  eta: 0:04:03  lr: 0.000298  loss: 0.6751 (0.6833)  time: 0.5658  data: 0.0002  max mem: 6820\n[00:25:09.412712] Epoch: [19]  [560/969]  eta: 0:03:52  lr: 0.000298  loss: 0.6678 (0.6835)  time: 0.5675  data: 0.0002  max mem: 6820\n[00:25:20.743586] Epoch: [19]  [580/969]  eta: 0:03:41  lr: 0.000298  loss: 0.6820 (0.6835)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:25:32.074194] Epoch: [19]  [600/969]  eta: 0:03:29  lr: 0.000298  loss: 0.6910 (0.6839)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:25:43.392421] Epoch: [19]  [620/969]  eta: 0:03:18  lr: 0.000298  loss: 0.6964 (0.6842)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:25:54.717374] Epoch: [19]  [640/969]  eta: 0:03:06  lr: 0.000298  loss: 0.6897 (0.6844)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:26:06.052404] Epoch: [19]  [660/969]  eta: 0:02:55  lr: 0.000298  loss: 0.6875 (0.6845)  time: 0.5667  data: 0.0002  max mem: 6820\n[00:26:17.378148] Epoch: [19]  [680/969]  eta: 0:02:44  lr: 0.000298  loss: 0.6805 (0.6848)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:26:28.700958] Epoch: [19]  [700/969]  eta: 0:02:32  lr: 0.000298  loss: 0.6672 (0.6847)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:26:40.018567] Epoch: [19]  [720/969]  eta: 0:02:21  lr: 0.000298  loss: 0.6765 (0.6848)  time: 0.5658  data: 0.0002  max mem: 6820\n[00:26:51.339339] Epoch: [19]  [740/969]  eta: 0:02:10  lr: 0.000298  loss: 0.6717 (0.6843)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:27:02.656322] Epoch: [19]  [760/969]  eta: 0:01:58  lr: 0.000298  loss: 0.6969 (0.6843)  time: 0.5658  data: 0.0002  max mem: 6820\n[00:27:13.985787] Epoch: [19]  [780/969]  eta: 0:01:47  lr: 0.000298  loss: 0.6698 (0.6842)  time: 0.5664  data: 0.0002  max mem: 6820\n[00:27:25.307006] Epoch: [19]  [800/969]  eta: 0:01:35  lr: 0.000298  loss: 0.6986 (0.6847)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:27:36.630120] Epoch: [19]  [820/969]  eta: 0:01:24  lr: 0.000298  loss: 0.6594 (0.6843)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:27:47.956289] Epoch: [19]  [840/969]  eta: 0:01:13  lr: 0.000297  loss: 0.6836 (0.6844)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:27:59.277153] Epoch: [19]  [860/969]  eta: 0:01:01  lr: 0.000297  loss: 0.6816 (0.6843)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:28:10.583648] Epoch: [19]  [880/969]  eta: 0:00:50  lr: 0.000297  loss: 0.6767 (0.6842)  time: 0.5653  data: 0.0002  max mem: 6820\n[00:28:21.915137] Epoch: [19]  [900/969]  eta: 0:00:39  lr: 0.000297  loss: 0.6935 (0.6844)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:28:33.258263] Epoch: [19]  [920/969]  eta: 0:00:27  lr: 0.000297  loss: 0.6785 (0.6844)  time: 0.5671  data: 0.0002  max mem: 6820\n[00:28:44.592158] Epoch: [19]  [940/969]  eta: 0:00:16  lr: 0.000297  loss: 0.6639 (0.6842)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:28:55.914946] Epoch: [19]  [960/969]  eta: 0:00:05  lr: 0.000297  loss: 0.6985 (0.6846)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:29:00.442063] Epoch: [19]  [968/969]  eta: 0:00:00  lr: 0.000297  loss: 0.6904 (0.6845)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:29:00.601315] Epoch: [19] Total time: 0:09:10 (0.5677 s / it)\n[00:29:00.601433] Averaged stats: lr: 0.000297  loss: 0.6904 (0.6845)\n[00:29:01.737864] val:  [  0/139]  eta: 0:02:37  loss: 0.6776 (0.6776)  time: 1.1316  data: 0.9923  max mem: 6820\n[00:29:03.236799] val:  [ 10/139]  eta: 0:00:30  loss: 0.6288 (0.6170)  time: 0.2390  data: 0.0904  max mem: 6820\n[00:29:04.736959] val:  [ 20/139]  eta: 0:00:23  loss: 0.5853 (0.6111)  time: 0.1499  data: 0.0002  max mem: 6820\n[00:29:06.243412] val:  [ 30/139]  eta: 0:00:19  loss: 0.5937 (0.6122)  time: 0.1503  data: 0.0002  max mem: 6820\n[00:29:07.754795] val:  [ 40/139]  eta: 0:00:17  loss: 0.6339 (0.6211)  time: 0.1508  data: 0.0002  max mem: 6820\n[00:29:09.268380] val:  [ 50/139]  eta: 0:00:15  loss: 0.6342 (0.6266)  time: 0.1512  data: 0.0002  max mem: 6820\n[00:29:10.782657] val:  [ 60/139]  eta: 0:00:13  loss: 0.6390 (0.6304)  time: 0.1513  data: 0.0002  max mem: 6820\n[00:29:12.295124] val:  [ 70/139]  eta: 0:00:11  loss: 0.6633 (0.6356)  time: 0.1513  data: 0.0002  max mem: 6820\n[00:29:13.805724] val:  [ 80/139]  eta: 0:00:09  loss: 0.6779 (0.6446)  time: 0.1511  data: 0.0002  max mem: 6820\n[00:29:15.317801] val:  [ 90/139]  eta: 0:00:07  loss: 0.7108 (0.6537)  time: 0.1511  data: 0.0002  max mem: 6820\n[00:29:16.835011] val:  [100/139]  eta: 0:00:06  loss: 0.7320 (0.6616)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:29:18.346637] val:  [110/139]  eta: 0:00:04  loss: 0.7205 (0.6658)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:29:19.858065] val:  [120/139]  eta: 0:00:03  loss: 0.7054 (0.6681)  time: 0.1511  data: 0.0002  max mem: 6820\n[00:29:21.371781] val:  [130/139]  eta: 0:00:01  loss: 0.6684 (0.6675)  time: 0.1512  data: 0.0001  max mem: 6820\n[00:29:22.485599] val:  [138/139]  eta: 0:00:00  loss: 0.6634 (0.6679)  time: 0.1464  data: 0.0001  max mem: 6820\n[00:29:22.595048] val: Total time: 0:00:21 (0.1582 s / it)\n[00:29:22.655383] val loss: 0.6679161675542379\n[00:29:22.655440] Accuracy: 0.6175, F1 Score: 0.6173, ROC AUC: 0.6507, Hamming Loss: 0.3825,\n Jaccard Score: 0.4465, Precision: 0.6178, Recall: 0.6175,\n Average Precision: 0.6351, Kappa: 0.2351, Score: 0.5010\n[00:29:25.478067] Best epoch = 19, Best score = 0.5010\n[00:29:25.484165] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[00:29:27.068131] Epoch: [20]  [  0/969]  eta: 0:25:33  lr: 0.000297  loss: 0.6372 (0.6372)  time: 1.5822  data: 0.9869  max mem: 6820\n[00:29:38.323052] Epoch: [20]  [ 20/969]  eta: 0:09:40  lr: 0.000297  loss: 0.6722 (0.6768)  time: 0.5627  data: 0.0001  max mem: 6820\n[00:29:49.781823] Epoch: [20]  [ 40/969]  eta: 0:09:10  lr: 0.000297  loss: 0.6733 (0.6745)  time: 0.5729  data: 0.0002  max mem: 6820\n[00:30:01.306901] Epoch: [20]  [ 60/969]  eta: 0:08:53  lr: 0.000297  loss: 0.6827 (0.6770)  time: 0.5762  data: 0.0002  max mem: 6820\n[00:30:12.782592] Epoch: [20]  [ 80/969]  eta: 0:08:39  lr: 0.000297  loss: 0.6725 (0.6754)  time: 0.5737  data: 0.0002  max mem: 6820\n[00:30:24.148447] Epoch: [20]  [100/969]  eta: 0:08:24  lr: 0.000297  loss: 0.6860 (0.6781)  time: 0.5682  data: 0.0002  max mem: 6820\n[00:30:35.462280] Epoch: [20]  [120/969]  eta: 0:08:10  lr: 0.000297  loss: 0.6932 (0.6813)  time: 0.5656  data: 0.0002  max mem: 6820\n[00:30:46.728424] Epoch: [20]  [140/969]  eta: 0:07:57  lr: 0.000297  loss: 0.6807 (0.6817)  time: 0.5632  data: 0.0002  max mem: 6820\n[00:30:58.040873] Epoch: [20]  [160/969]  eta: 0:07:45  lr: 0.000297  loss: 0.6739 (0.6814)  time: 0.5656  data: 0.0002  max mem: 6820\n[00:31:09.365727] Epoch: [20]  [180/969]  eta: 0:07:32  lr: 0.000297  loss: 0.6737 (0.6817)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:31:20.707578] Epoch: [20]  [200/969]  eta: 0:07:20  lr: 0.000296  loss: 0.6879 (0.6828)  time: 0.5670  data: 0.0002  max mem: 6820\n[00:31:32.077740] Epoch: [20]  [220/969]  eta: 0:07:08  lr: 0.000296  loss: 0.6909 (0.6834)  time: 0.5685  data: 0.0002  max mem: 6820\n[00:31:43.438274] Epoch: [20]  [240/969]  eta: 0:06:57  lr: 0.000296  loss: 0.6851 (0.6836)  time: 0.5680  data: 0.0002  max mem: 6820\n[00:31:54.787135] Epoch: [20]  [260/969]  eta: 0:06:45  lr: 0.000296  loss: 0.6610 (0.6831)  time: 0.5674  data: 0.0002  max mem: 6820\n[00:32:06.116719] Epoch: [20]  [280/969]  eta: 0:06:33  lr: 0.000296  loss: 0.6923 (0.6832)  time: 0.5664  data: 0.0002  max mem: 6820\n[00:32:17.430779] Epoch: [20]  [300/969]  eta: 0:06:22  lr: 0.000296  loss: 0.6742 (0.6827)  time: 0.5656  data: 0.0002  max mem: 6820\n[00:32:28.754210] Epoch: [20]  [320/969]  eta: 0:06:10  lr: 0.000296  loss: 0.6775 (0.6825)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:32:40.069065] Epoch: [20]  [340/969]  eta: 0:05:58  lr: 0.000296  loss: 0.6616 (0.6818)  time: 0.5657  data: 0.0002  max mem: 6820\n[00:32:51.360552] Epoch: [20]  [360/969]  eta: 0:05:47  lr: 0.000296  loss: 0.6637 (0.6820)  time: 0.5645  data: 0.0003  max mem: 6820\n[00:33:02.688827] Epoch: [20]  [380/969]  eta: 0:05:35  lr: 0.000296  loss: 0.6765 (0.6822)  time: 0.5664  data: 0.0002  max mem: 6820\n[00:33:14.016796] Epoch: [20]  [400/969]  eta: 0:05:24  lr: 0.000296  loss: 0.6810 (0.6822)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:33:25.351377] Epoch: [20]  [420/969]  eta: 0:05:12  lr: 0.000296  loss: 0.6900 (0.6825)  time: 0.5667  data: 0.0002  max mem: 6820\n[00:33:36.704500] Epoch: [20]  [440/969]  eta: 0:05:01  lr: 0.000296  loss: 0.6729 (0.6825)  time: 0.5676  data: 0.0002  max mem: 6820\n[00:33:48.093453] Epoch: [20]  [460/969]  eta: 0:04:49  lr: 0.000296  loss: 0.6722 (0.6823)  time: 0.5694  data: 0.0002  max mem: 6820\n[00:33:59.480651] Epoch: [20]  [480/969]  eta: 0:04:38  lr: 0.000296  loss: 0.6677 (0.6821)  time: 0.5693  data: 0.0002  max mem: 6820\n[00:34:10.924048] Epoch: [20]  [500/969]  eta: 0:04:27  lr: 0.000295  loss: 0.6758 (0.6821)  time: 0.5721  data: 0.0002  max mem: 6820\n[00:34:22.348761] Epoch: [20]  [520/969]  eta: 0:04:15  lr: 0.000295  loss: 0.6721 (0.6817)  time: 0.5712  data: 0.0002  max mem: 6820\n[00:34:33.774262] Epoch: [20]  [540/969]  eta: 0:04:04  lr: 0.000295  loss: 0.6744 (0.6813)  time: 0.5712  data: 0.0002  max mem: 6820\n[00:34:45.187369] Epoch: [20]  [560/969]  eta: 0:03:53  lr: 0.000295  loss: 0.6693 (0.6807)  time: 0.5706  data: 0.0002  max mem: 6820\n[00:34:56.550375] Epoch: [20]  [580/969]  eta: 0:03:41  lr: 0.000295  loss: 0.6780 (0.6810)  time: 0.5681  data: 0.0002  max mem: 6820\n[00:35:07.880922] Epoch: [20]  [600/969]  eta: 0:03:30  lr: 0.000295  loss: 0.6944 (0.6816)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:35:19.204207] Epoch: [20]  [620/969]  eta: 0:03:18  lr: 0.000295  loss: 0.6965 (0.6820)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:35:30.502040] Epoch: [20]  [640/969]  eta: 0:03:07  lr: 0.000295  loss: 0.6729 (0.6820)  time: 0.5648  data: 0.0002  max mem: 6820\n[00:35:41.813798] Epoch: [20]  [660/969]  eta: 0:02:55  lr: 0.000295  loss: 0.6782 (0.6819)  time: 0.5655  data: 0.0002  max mem: 6820\n[00:35:53.141000] Epoch: [20]  [680/969]  eta: 0:02:44  lr: 0.000295  loss: 0.6933 (0.6825)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:36:04.459392] Epoch: [20]  [700/969]  eta: 0:02:33  lr: 0.000295  loss: 0.6688 (0.6823)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:36:15.768815] Epoch: [20]  [720/969]  eta: 0:02:21  lr: 0.000295  loss: 0.6851 (0.6823)  time: 0.5654  data: 0.0002  max mem: 6820\n[00:36:27.087849] Epoch: [20]  [740/969]  eta: 0:02:10  lr: 0.000295  loss: 0.6870 (0.6823)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:36:38.424733] Epoch: [20]  [760/969]  eta: 0:01:58  lr: 0.000295  loss: 0.6890 (0.6823)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:36:49.766620] Epoch: [20]  [780/969]  eta: 0:01:47  lr: 0.000295  loss: 0.6813 (0.6824)  time: 0.5670  data: 0.0002  max mem: 6820\n[00:37:01.105626] Epoch: [20]  [800/969]  eta: 0:01:36  lr: 0.000294  loss: 0.6861 (0.6826)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:37:12.476296] Epoch: [20]  [820/969]  eta: 0:01:24  lr: 0.000294  loss: 0.6814 (0.6824)  time: 0.5685  data: 0.0002  max mem: 6820\n[00:37:23.846277] Epoch: [20]  [840/969]  eta: 0:01:13  lr: 0.000294  loss: 0.6789 (0.6822)  time: 0.5684  data: 0.0002  max mem: 6820\n[00:37:35.241855] Epoch: [20]  [860/969]  eta: 0:01:01  lr: 0.000294  loss: 0.6703 (0.6820)  time: 0.5697  data: 0.0002  max mem: 6820\n[00:37:46.635389] Epoch: [20]  [880/969]  eta: 0:00:50  lr: 0.000294  loss: 0.6667 (0.6817)  time: 0.5696  data: 0.0002  max mem: 6820\n[00:37:57.999941] Epoch: [20]  [900/969]  eta: 0:00:39  lr: 0.000294  loss: 0.6877 (0.6817)  time: 0.5682  data: 0.0002  max mem: 6820\n[00:38:09.329035] Epoch: [20]  [920/969]  eta: 0:00:27  lr: 0.000294  loss: 0.6606 (0.6816)  time: 0.5664  data: 0.0002  max mem: 6820\n[00:38:20.667708] Epoch: [20]  [940/969]  eta: 0:00:16  lr: 0.000294  loss: 0.6724 (0.6815)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:38:31.984813] Epoch: [20]  [960/969]  eta: 0:00:05  lr: 0.000294  loss: 0.6935 (0.6818)  time: 0.5658  data: 0.0002  max mem: 6820\n[00:38:36.513434] Epoch: [20]  [968/969]  eta: 0:00:00  lr: 0.000294  loss: 0.6849 (0.6818)  time: 0.5652  data: 0.0002  max mem: 6820\n[00:38:36.630742] Epoch: [20] Total time: 0:09:11 (0.5688 s / it)\n[00:38:36.630852] Averaged stats: lr: 0.000294  loss: 0.6849 (0.6818)\n[00:38:37.543647] val:  [  0/139]  eta: 0:02:06  loss: 0.6707 (0.6707)  time: 0.9074  data: 0.7490  max mem: 6820\n[00:38:39.034991] val:  [ 10/139]  eta: 0:00:28  loss: 0.6505 (0.6267)  time: 0.2180  data: 0.0683  max mem: 6820\n[00:38:40.540419] val:  [ 20/139]  eta: 0:00:22  loss: 0.6032 (0.6214)  time: 0.1497  data: 0.0002  max mem: 6820\n[00:38:42.044368] val:  [ 30/139]  eta: 0:00:19  loss: 0.5921 (0.6203)  time: 0.1504  data: 0.0002  max mem: 6820\n[00:38:43.553021] val:  [ 40/139]  eta: 0:00:16  loss: 0.6367 (0.6291)  time: 0.1506  data: 0.0002  max mem: 6820\n[00:38:45.064958] val:  [ 50/139]  eta: 0:00:14  loss: 0.6435 (0.6341)  time: 0.1509  data: 0.0002  max mem: 6820\n[00:38:46.578038] val:  [ 60/139]  eta: 0:00:12  loss: 0.6444 (0.6365)  time: 0.1512  data: 0.0002  max mem: 6820\n[00:38:48.085861] val:  [ 70/139]  eta: 0:00:11  loss: 0.6558 (0.6403)  time: 0.1510  data: 0.0002  max mem: 6820\n[00:38:49.593926] val:  [ 80/139]  eta: 0:00:09  loss: 0.6638 (0.6464)  time: 0.1507  data: 0.0002  max mem: 6820\n[00:38:51.100535] val:  [ 90/139]  eta: 0:00:07  loss: 0.7363 (0.6563)  time: 0.1507  data: 0.0002  max mem: 6820\n[00:38:52.612082] val:  [100/139]  eta: 0:00:06  loss: 0.7387 (0.6638)  time: 0.1508  data: 0.0002  max mem: 6820\n[00:38:54.119728] val:  [110/139]  eta: 0:00:04  loss: 0.7352 (0.6674)  time: 0.1509  data: 0.0002  max mem: 6820\n[00:38:55.632460] val:  [120/139]  eta: 0:00:02  loss: 0.6883 (0.6680)  time: 0.1509  data: 0.0002  max mem: 6820\n[00:38:57.144561] val:  [130/139]  eta: 0:00:01  loss: 0.6349 (0.6640)  time: 0.1511  data: 0.0002  max mem: 6820\n[00:38:58.257010] val:  [138/139]  eta: 0:00:00  loss: 0.6200 (0.6620)  time: 0.1463  data: 0.0001  max mem: 6820\n[00:38:58.370156] val: Total time: 0:00:21 (0.1564 s / it)\n[00:38:58.427693] val loss: 0.6620351667884442\n[00:38:58.427761] Accuracy: 0.6225, F1 Score: 0.6225, ROC AUC: 0.6628, Hamming Loss: 0.3775,\n Jaccard Score: 0.4519, Precision: 0.6225, Recall: 0.6225,\n Average Precision: 0.6547, Kappa: 0.2450, Score: 0.5101\n[00:39:01.315519] Best epoch = 20, Best score = 0.5101\n[00:39:01.318241] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[00:39:02.910571] Epoch: [21]  [  0/969]  eta: 0:25:41  lr: 0.000294  loss: 0.6557 (0.6557)  time: 1.5910  data: 1.0012  max mem: 6820\n[00:39:14.108107] Epoch: [21]  [ 20/969]  eta: 0:09:37  lr: 0.000294  loss: 0.6803 (0.6710)  time: 0.5598  data: 0.0002  max mem: 6820\n[00:39:25.526555] Epoch: [21]  [ 40/969]  eta: 0:09:08  lr: 0.000294  loss: 0.6896 (0.6803)  time: 0.5709  data: 0.0002  max mem: 6820\n[00:39:37.011399] Epoch: [21]  [ 60/969]  eta: 0:08:51  lr: 0.000294  loss: 0.6800 (0.6817)  time: 0.5742  data: 0.0002  max mem: 6820\n[00:39:48.488789] Epoch: [21]  [ 80/969]  eta: 0:08:37  lr: 0.000294  loss: 0.6694 (0.6801)  time: 0.5738  data: 0.0002  max mem: 6820\n[00:39:59.875359] Epoch: [21]  [100/969]  eta: 0:08:23  lr: 0.000294  loss: 0.6712 (0.6807)  time: 0.5693  data: 0.0002  max mem: 6820\n[00:40:11.175021] Epoch: [21]  [120/969]  eta: 0:08:10  lr: 0.000293  loss: 0.6954 (0.6835)  time: 0.5649  data: 0.0002  max mem: 6820\n[00:40:22.428652] Epoch: [21]  [140/969]  eta: 0:07:56  lr: 0.000293  loss: 0.6834 (0.6837)  time: 0.5626  data: 0.0002  max mem: 6820\n[00:40:33.745377] Epoch: [21]  [160/969]  eta: 0:07:44  lr: 0.000293  loss: 0.6608 (0.6815)  time: 0.5658  data: 0.0002  max mem: 6820\n[00:40:45.116495] Epoch: [21]  [180/969]  eta: 0:07:32  lr: 0.000293  loss: 0.6866 (0.6816)  time: 0.5685  data: 0.0002  max mem: 6820\n[00:40:56.503211] Epoch: [21]  [200/969]  eta: 0:07:20  lr: 0.000293  loss: 0.6749 (0.6825)  time: 0.5693  data: 0.0002  max mem: 6820\n[00:41:07.900022] Epoch: [21]  [220/969]  eta: 0:07:08  lr: 0.000293  loss: 0.6814 (0.6826)  time: 0.5698  data: 0.0002  max mem: 6820\n[00:41:19.288285] Epoch: [21]  [240/969]  eta: 0:06:57  lr: 0.000293  loss: 0.6693 (0.6821)  time: 0.5694  data: 0.0002  max mem: 6820\n[00:41:30.691577] Epoch: [21]  [260/969]  eta: 0:06:45  lr: 0.000293  loss: 0.6704 (0.6821)  time: 0.5701  data: 0.0002  max mem: 6820\n[00:41:42.058472] Epoch: [21]  [280/969]  eta: 0:06:34  lr: 0.000293  loss: 0.6770 (0.6818)  time: 0.5683  data: 0.0002  max mem: 6820\n[00:41:53.412571] Epoch: [21]  [300/969]  eta: 0:06:22  lr: 0.000293  loss: 0.6719 (0.6818)  time: 0.5676  data: 0.0002  max mem: 6820\n[00:42:04.799257] Epoch: [21]  [320/969]  eta: 0:06:10  lr: 0.000293  loss: 0.6663 (0.6814)  time: 0.5693  data: 0.0002  max mem: 6820\n[00:42:16.060049] Epoch: [21]  [340/969]  eta: 0:05:59  lr: 0.000293  loss: 0.6774 (0.6812)  time: 0.5630  data: 0.0002  max mem: 6820\n[00:42:27.380624] Epoch: [21]  [360/969]  eta: 0:05:47  lr: 0.000293  loss: 0.6821 (0.6815)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:42:38.686714] Epoch: [21]  [380/969]  eta: 0:05:36  lr: 0.000293  loss: 0.6771 (0.6813)  time: 0.5653  data: 0.0002  max mem: 6820\n[00:42:49.977531] Epoch: [21]  [400/969]  eta: 0:05:24  lr: 0.000293  loss: 0.6762 (0.6809)  time: 0.5645  data: 0.0002  max mem: 6820\n[00:43:01.304806] Epoch: [21]  [420/969]  eta: 0:05:12  lr: 0.000292  loss: 0.6812 (0.6809)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:43:12.626133] Epoch: [21]  [440/969]  eta: 0:05:01  lr: 0.000292  loss: 0.6779 (0.6810)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:43:23.962364] Epoch: [21]  [460/969]  eta: 0:04:49  lr: 0.000292  loss: 0.6782 (0.6814)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:43:35.282910] Epoch: [21]  [480/969]  eta: 0:04:38  lr: 0.000292  loss: 0.6728 (0.6813)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:43:46.614259] Epoch: [21]  [500/969]  eta: 0:04:27  lr: 0.000292  loss: 0.6845 (0.6813)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:43:57.958617] Epoch: [21]  [520/969]  eta: 0:04:15  lr: 0.000292  loss: 0.6513 (0.6804)  time: 0.5672  data: 0.0002  max mem: 6820\n[00:44:09.333152] Epoch: [21]  [540/969]  eta: 0:04:04  lr: 0.000292  loss: 0.6575 (0.6798)  time: 0.5687  data: 0.0002  max mem: 6820\n[00:44:20.738620] Epoch: [21]  [560/969]  eta: 0:03:52  lr: 0.000292  loss: 0.6439 (0.6791)  time: 0.5702  data: 0.0002  max mem: 6820\n[00:44:32.135505] Epoch: [21]  [580/969]  eta: 0:03:41  lr: 0.000292  loss: 0.6637 (0.6786)  time: 0.5698  data: 0.0002  max mem: 6820\n[00:44:43.507233] Epoch: [21]  [600/969]  eta: 0:03:30  lr: 0.000292  loss: 0.6704 (0.6788)  time: 0.5685  data: 0.0002  max mem: 6820\n[00:44:54.857094] Epoch: [21]  [620/969]  eta: 0:03:18  lr: 0.000292  loss: 0.6935 (0.6793)  time: 0.5674  data: 0.0002  max mem: 6820\n[00:45:06.170914] Epoch: [21]  [640/969]  eta: 0:03:07  lr: 0.000292  loss: 0.6671 (0.6789)  time: 0.5656  data: 0.0002  max mem: 6820\n[00:45:17.512271] Epoch: [21]  [660/969]  eta: 0:02:55  lr: 0.000292  loss: 0.6678 (0.6787)  time: 0.5670  data: 0.0002  max mem: 6820\n[00:45:28.841346] Epoch: [21]  [680/969]  eta: 0:02:44  lr: 0.000292  loss: 0.7019 (0.6792)  time: 0.5664  data: 0.0002  max mem: 6820\n[00:45:40.160562] Epoch: [21]  [700/969]  eta: 0:02:33  lr: 0.000291  loss: 0.6615 (0.6787)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:45:51.485927] Epoch: [21]  [720/969]  eta: 0:02:21  lr: 0.000291  loss: 0.6997 (0.6794)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:46:02.792956] Epoch: [21]  [740/969]  eta: 0:02:10  lr: 0.000291  loss: 0.6663 (0.6790)  time: 0.5653  data: 0.0002  max mem: 6820\n[00:46:14.122416] Epoch: [21]  [760/969]  eta: 0:01:58  lr: 0.000291  loss: 0.6834 (0.6788)  time: 0.5664  data: 0.0002  max mem: 6820\n[00:46:25.455658] Epoch: [21]  [780/969]  eta: 0:01:47  lr: 0.000291  loss: 0.6810 (0.6790)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:46:36.800201] Epoch: [21]  [800/969]  eta: 0:01:36  lr: 0.000291  loss: 0.6674 (0.6791)  time: 0.5672  data: 0.0002  max mem: 6820\n[00:46:48.164920] Epoch: [21]  [820/969]  eta: 0:01:24  lr: 0.000291  loss: 0.6921 (0.6795)  time: 0.5682  data: 0.0002  max mem: 6820\n[00:46:59.535481] Epoch: [21]  [840/969]  eta: 0:01:13  lr: 0.000291  loss: 0.6839 (0.6796)  time: 0.5685  data: 0.0002  max mem: 6820\n[00:47:10.901721] Epoch: [21]  [860/969]  eta: 0:01:01  lr: 0.000291  loss: 0.6724 (0.6793)  time: 0.5683  data: 0.0002  max mem: 6820\n[00:47:22.282583] Epoch: [21]  [880/969]  eta: 0:00:50  lr: 0.000291  loss: 0.6740 (0.6793)  time: 0.5690  data: 0.0002  max mem: 6820\n[00:47:33.696695] Epoch: [21]  [900/969]  eta: 0:00:39  lr: 0.000291  loss: 0.6774 (0.6793)  time: 0.5707  data: 0.0002  max mem: 6820\n[00:47:45.108719] Epoch: [21]  [920/969]  eta: 0:00:27  lr: 0.000291  loss: 0.6615 (0.6790)  time: 0.5706  data: 0.0002  max mem: 6820\n[00:47:56.521061] Epoch: [21]  [940/969]  eta: 0:00:16  lr: 0.000291  loss: 0.6775 (0.6789)  time: 0.5706  data: 0.0002  max mem: 6820\n[00:48:07.908213] Epoch: [21]  [960/969]  eta: 0:00:05  lr: 0.000290  loss: 0.6741 (0.6789)  time: 0.5693  data: 0.0002  max mem: 6820\n[00:48:12.478179] Epoch: [21]  [968/969]  eta: 0:00:00  lr: 0.000290  loss: 0.6741 (0.6790)  time: 0.5702  data: 0.0001  max mem: 6820\n[00:48:12.609344] Epoch: [21] Total time: 0:09:11 (0.5689 s / it)\n[00:48:12.609459] Averaged stats: lr: 0.000290  loss: 0.6741 (0.6790)\n[00:48:13.450615] val:  [  0/139]  eta: 0:01:55  loss: 0.6615 (0.6615)  time: 0.8316  data: 0.6673  max mem: 6820\n[00:48:14.951204] val:  [ 10/139]  eta: 0:00:27  loss: 0.6141 (0.6021)  time: 0.2119  data: 0.0608  max mem: 6820\n[00:48:16.455349] val:  [ 20/139]  eta: 0:00:21  loss: 0.5717 (0.5899)  time: 0.1501  data: 0.0002  max mem: 6820\n[00:48:17.964685] val:  [ 30/139]  eta: 0:00:18  loss: 0.5955 (0.5940)  time: 0.1506  data: 0.0002  max mem: 6820\n[00:48:19.478340] val:  [ 40/139]  eta: 0:00:16  loss: 0.6547 (0.6170)  time: 0.1511  data: 0.0002  max mem: 6820\n[00:48:20.993900] val:  [ 50/139]  eta: 0:00:14  loss: 0.6812 (0.6287)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:48:22.511252] val:  [ 60/139]  eta: 0:00:12  loss: 0.6525 (0.6313)  time: 0.1516  data: 0.0002  max mem: 6820\n[00:48:24.022470] val:  [ 70/139]  eta: 0:00:11  loss: 0.6525 (0.6385)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:48:25.536843] val:  [ 80/139]  eta: 0:00:09  loss: 0.6982 (0.6514)  time: 0.1512  data: 0.0002  max mem: 6820\n[00:48:27.051213] val:  [ 90/139]  eta: 0:00:07  loss: 0.7210 (0.6610)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:48:28.574010] val:  [100/139]  eta: 0:00:06  loss: 0.7210 (0.6687)  time: 0.1518  data: 0.0002  max mem: 6820\n[00:48:30.101313] val:  [110/139]  eta: 0:00:04  loss: 0.7073 (0.6702)  time: 0.1524  data: 0.0002  max mem: 6820\n[00:48:31.610608] val:  [120/139]  eta: 0:00:02  loss: 0.6723 (0.6693)  time: 0.1518  data: 0.0002  max mem: 6820\n[00:48:33.132550] val:  [130/139]  eta: 0:00:01  loss: 0.6497 (0.6660)  time: 0.1515  data: 0.0001  max mem: 6820\n[00:48:34.253657] val:  [138/139]  eta: 0:00:00  loss: 0.6200 (0.6629)  time: 0.1472  data: 0.0001  max mem: 6820\n[00:48:34.359753] val: Total time: 0:00:21 (0.1565 s / it)\n[00:48:34.415059] val loss: 0.6628814258163781\n[00:48:34.415125] Accuracy: 0.5981, F1 Score: 0.5981, ROC AUC: 0.6396, Hamming Loss: 0.4019,\n Jaccard Score: 0.4266, Precision: 0.5981, Recall: 0.5981,\n Average Precision: 0.6243, Kappa: 0.1962, Score: 0.4779\n[00:48:34.416853] Best epoch = 20, Best score = 0.5101\n[00:48:34.419313] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[00:48:35.789191] Epoch: [22]  [  0/969]  eta: 0:22:06  lr: 0.000290  loss: 0.6642 (0.6642)  time: 1.3686  data: 0.8019  max mem: 6820\n[00:48:47.154493] Epoch: [22]  [ 20/969]  eta: 0:09:35  lr: 0.000290  loss: 0.6772 (0.6819)  time: 0.5682  data: 0.0002  max mem: 6820\n[00:48:58.568233] Epoch: [22]  [ 40/969]  eta: 0:09:07  lr: 0.000290  loss: 0.6736 (0.6805)  time: 0.5706  data: 0.0002  max mem: 6820\n[00:49:09.991950] Epoch: [22]  [ 60/969]  eta: 0:08:50  lr: 0.000290  loss: 0.6692 (0.6778)  time: 0.5711  data: 0.0002  max mem: 6820\n[00:49:21.385598] Epoch: [22]  [ 80/969]  eta: 0:08:35  lr: 0.000290  loss: 0.6840 (0.6744)  time: 0.5696  data: 0.0002  max mem: 6820\n[00:49:32.771137] Epoch: [22]  [100/969]  eta: 0:08:22  lr: 0.000290  loss: 0.6895 (0.6784)  time: 0.5692  data: 0.0002  max mem: 6820\n[00:49:44.129835] Epoch: [22]  [120/969]  eta: 0:08:09  lr: 0.000290  loss: 0.6890 (0.6797)  time: 0.5679  data: 0.0002  max mem: 6820\n[00:49:55.454056] Epoch: [22]  [140/969]  eta: 0:07:56  lr: 0.000290  loss: 0.6804 (0.6793)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:50:06.772207] Epoch: [22]  [160/969]  eta: 0:07:44  lr: 0.000290  loss: 0.6764 (0.6797)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:50:18.097267] Epoch: [22]  [180/969]  eta: 0:07:31  lr: 0.000290  loss: 0.6905 (0.6812)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:50:29.418522] Epoch: [22]  [200/969]  eta: 0:07:19  lr: 0.000290  loss: 0.6929 (0.6818)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:50:40.737509] Epoch: [22]  [220/969]  eta: 0:07:08  lr: 0.000290  loss: 0.6840 (0.6816)  time: 0.5659  data: 0.0002  max mem: 6820\n[00:50:52.075573] Epoch: [22]  [240/969]  eta: 0:06:56  lr: 0.000290  loss: 0.6690 (0.6809)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:51:03.398775] Epoch: [22]  [260/969]  eta: 0:06:44  lr: 0.000289  loss: 0.6859 (0.6809)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:51:14.734354] Epoch: [22]  [280/969]  eta: 0:06:33  lr: 0.000289  loss: 0.6545 (0.6801)  time: 0.5667  data: 0.0002  max mem: 6820\n[00:51:26.059565] Epoch: [22]  [300/969]  eta: 0:06:21  lr: 0.000289  loss: 0.6842 (0.6801)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:51:37.387110] Epoch: [22]  [320/969]  eta: 0:06:09  lr: 0.000289  loss: 0.6704 (0.6797)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:51:48.711110] Epoch: [22]  [340/969]  eta: 0:05:58  lr: 0.000289  loss: 0.6807 (0.6805)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:52:00.060644] Epoch: [22]  [360/969]  eta: 0:05:46  lr: 0.000289  loss: 0.6628 (0.6804)  time: 0.5674  data: 0.0002  max mem: 6820\n[00:52:11.387328] Epoch: [22]  [380/969]  eta: 0:05:35  lr: 0.000289  loss: 0.6786 (0.6803)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:52:22.712835] Epoch: [22]  [400/969]  eta: 0:05:23  lr: 0.000289  loss: 0.6565 (0.6794)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:52:34.063471] Epoch: [22]  [420/969]  eta: 0:05:12  lr: 0.000289  loss: 0.6857 (0.6798)  time: 0.5675  data: 0.0002  max mem: 6820\n[00:52:45.418860] Epoch: [22]  [440/969]  eta: 0:05:01  lr: 0.000289  loss: 0.6798 (0.6800)  time: 0.5677  data: 0.0002  max mem: 6820\n[00:52:56.755392] Epoch: [22]  [460/969]  eta: 0:04:49  lr: 0.000289  loss: 0.6584 (0.6801)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:53:08.089216] Epoch: [22]  [480/969]  eta: 0:04:38  lr: 0.000289  loss: 0.6854 (0.6804)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:53:19.411452] Epoch: [22]  [500/969]  eta: 0:04:26  lr: 0.000289  loss: 0.6853 (0.6807)  time: 0.5661  data: 0.0002  max mem: 6820\n[00:53:30.739216] Epoch: [22]  [520/969]  eta: 0:04:15  lr: 0.000288  loss: 0.6591 (0.6796)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:53:42.073734] Epoch: [22]  [540/969]  eta: 0:04:03  lr: 0.000288  loss: 0.6621 (0.6793)  time: 0.5667  data: 0.0002  max mem: 6820\n[00:53:53.404155] Epoch: [22]  [560/969]  eta: 0:03:52  lr: 0.000288  loss: 0.6280 (0.6788)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:54:04.738012] Epoch: [22]  [580/969]  eta: 0:03:41  lr: 0.000288  loss: 0.6800 (0.6787)  time: 0.5666  data: 0.0002  max mem: 6820\n[00:54:16.091770] Epoch: [22]  [600/969]  eta: 0:03:29  lr: 0.000288  loss: 0.6952 (0.6795)  time: 0.5676  data: 0.0002  max mem: 6820\n[00:54:27.419327] Epoch: [22]  [620/969]  eta: 0:03:18  lr: 0.000288  loss: 0.6722 (0.6794)  time: 0.5663  data: 0.0002  max mem: 6820\n[00:54:38.782360] Epoch: [22]  [640/969]  eta: 0:03:06  lr: 0.000288  loss: 0.6817 (0.6793)  time: 0.5681  data: 0.0002  max mem: 6820\n[00:54:50.139334] Epoch: [22]  [660/969]  eta: 0:02:55  lr: 0.000288  loss: 0.6799 (0.6792)  time: 0.5678  data: 0.0002  max mem: 6820\n[00:55:01.521112] Epoch: [22]  [680/969]  eta: 0:02:44  lr: 0.000288  loss: 0.6858 (0.6797)  time: 0.5690  data: 0.0002  max mem: 6820\n[00:55:12.860612] Epoch: [22]  [700/969]  eta: 0:02:32  lr: 0.000288  loss: 0.6686 (0.6798)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:55:24.213348] Epoch: [22]  [720/969]  eta: 0:02:21  lr: 0.000288  loss: 0.6762 (0.6798)  time: 0.5676  data: 0.0002  max mem: 6820\n[00:55:35.559495] Epoch: [22]  [740/969]  eta: 0:02:10  lr: 0.000288  loss: 0.6654 (0.6795)  time: 0.5672  data: 0.0002  max mem: 6820\n[00:55:46.911716] Epoch: [22]  [760/969]  eta: 0:01:58  lr: 0.000288  loss: 0.6796 (0.6796)  time: 0.5676  data: 0.0002  max mem: 6820\n[00:55:58.236013] Epoch: [22]  [780/969]  eta: 0:01:47  lr: 0.000287  loss: 0.6774 (0.6798)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:56:09.561821] Epoch: [22]  [800/969]  eta: 0:01:36  lr: 0.000287  loss: 0.6831 (0.6799)  time: 0.5662  data: 0.0002  max mem: 6820\n[00:56:20.898213] Epoch: [22]  [820/969]  eta: 0:01:24  lr: 0.000287  loss: 0.6786 (0.6796)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:56:32.237022] Epoch: [22]  [840/969]  eta: 0:01:13  lr: 0.000287  loss: 0.6736 (0.6795)  time: 0.5669  data: 0.0002  max mem: 6820\n[00:56:43.558169] Epoch: [22]  [860/969]  eta: 0:01:01  lr: 0.000287  loss: 0.6620 (0.6790)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:56:54.894994] Epoch: [22]  [880/969]  eta: 0:00:50  lr: 0.000287  loss: 0.6693 (0.6792)  time: 0.5668  data: 0.0002  max mem: 6820\n[00:57:06.229871] Epoch: [22]  [900/969]  eta: 0:00:39  lr: 0.000287  loss: 0.6619 (0.6790)  time: 0.5667  data: 0.0002  max mem: 6820\n[00:57:17.551335] Epoch: [22]  [920/969]  eta: 0:00:27  lr: 0.000287  loss: 0.6673 (0.6785)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:57:28.892613] Epoch: [22]  [940/969]  eta: 0:00:16  lr: 0.000287  loss: 0.6673 (0.6786)  time: 0.5670  data: 0.0001  max mem: 6820\n[00:57:40.213176] Epoch: [22]  [960/969]  eta: 0:00:05  lr: 0.000287  loss: 0.6959 (0.6789)  time: 0.5660  data: 0.0002  max mem: 6820\n[00:57:44.744801] Epoch: [22]  [968/969]  eta: 0:00:00  lr: 0.000287  loss: 0.6676 (0.6789)  time: 0.5663  data: 0.0001  max mem: 6820\n[00:57:44.859649] Epoch: [22] Total time: 0:09:10 (0.5680 s / it)\n[00:57:44.859757] Averaged stats: lr: 0.000287  loss: 0.6676 (0.6789)\n[00:57:45.777778] val:  [  0/139]  eta: 0:02:06  loss: 0.6549 (0.6549)  time: 0.9126  data: 0.7659  max mem: 6820\n[00:57:47.282400] val:  [ 10/139]  eta: 0:00:28  loss: 0.6011 (0.5778)  time: 0.2197  data: 0.0699  max mem: 6820\n[00:57:48.788507] val:  [ 20/139]  eta: 0:00:22  loss: 0.5830 (0.5770)  time: 0.1504  data: 0.0003  max mem: 6820\n[00:57:50.302033] val:  [ 30/139]  eta: 0:00:19  loss: 0.5830 (0.5797)  time: 0.1509  data: 0.0002  max mem: 6820\n[00:57:51.817550] val:  [ 40/139]  eta: 0:00:16  loss: 0.5999 (0.5903)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:57:53.334162] val:  [ 50/139]  eta: 0:00:14  loss: 0.6132 (0.5966)  time: 0.1515  data: 0.0002  max mem: 6820\n[00:57:54.848576] val:  [ 60/139]  eta: 0:00:12  loss: 0.6329 (0.6040)  time: 0.1515  data: 0.0002  max mem: 6820\n[00:57:56.366634] val:  [ 70/139]  eta: 0:00:11  loss: 0.6436 (0.6131)  time: 0.1515  data: 0.0002  max mem: 6820\n[00:57:57.883533] val:  [ 80/139]  eta: 0:00:09  loss: 0.6575 (0.6254)  time: 0.1517  data: 0.0002  max mem: 6820\n[00:57:59.404177] val:  [ 90/139]  eta: 0:00:07  loss: 0.7315 (0.6377)  time: 0.1518  data: 0.0002  max mem: 6820\n[00:58:00.918456] val:  [100/139]  eta: 0:00:06  loss: 0.7405 (0.6483)  time: 0.1517  data: 0.0002  max mem: 6820\n[00:58:02.437917] val:  [110/139]  eta: 0:00:04  loss: 0.7430 (0.6559)  time: 0.1516  data: 0.0002  max mem: 6820\n[00:58:03.948718] val:  [120/139]  eta: 0:00:02  loss: 0.7064 (0.6608)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:58:05.466943] val:  [130/139]  eta: 0:00:01  loss: 0.6847 (0.6607)  time: 0.1514  data: 0.0002  max mem: 6820\n[00:58:06.588481] val:  [138/139]  eta: 0:00:00  loss: 0.6623 (0.6609)  time: 0.1469  data: 0.0002  max mem: 6820\n[00:58:06.695747] val: Total time: 0:00:21 (0.1571 s / it)\n[00:58:06.751070] val loss: 0.6608700100466501\n[00:58:06.751137] Accuracy: 0.6130, F1 Score: 0.6100, ROC AUC: 0.6618, Hamming Loss: 0.3870,\n Jaccard Score: 0.4397, Precision: 0.6166, Recall: 0.6130,\n Average Precision: 0.6621, Kappa: 0.2260, Score: 0.4993\n[00:58:06.753015] Best epoch = 20, Best score = 0.5101\n[00:58:06.755611] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[00:58:08.074071] Epoch: [23]  [  0/969]  eta: 0:21:13  lr: 0.000287  loss: 0.7262 (0.7262)  time: 1.3137  data: 0.7237  max mem: 6820\n[00:58:19.406001] Epoch: [23]  [ 20/969]  eta: 0:09:31  lr: 0.000287  loss: 0.6875 (0.6906)  time: 0.5665  data: 0.0002  max mem: 6820\n[00:58:30.782553] Epoch: [23]  [ 40/969]  eta: 0:09:04  lr: 0.000287  loss: 0.6768 (0.6830)  time: 0.5688  data: 0.0002  max mem: 6820\n[00:58:42.193558] Epoch: [23]  [ 60/969]  eta: 0:08:47  lr: 0.000286  loss: 0.6760 (0.6825)  time: 0.5705  data: 0.0002  max mem: 6820\n[00:58:53.569871] Epoch: [23]  [ 80/969]  eta: 0:08:33  lr: 0.000286  loss: 0.6574 (0.6796)  time: 0.5688  data: 0.0002  max mem: 6820\n[00:59:04.968112] Epoch: [23]  [100/969]  eta: 0:08:20  lr: 0.000286  loss: 0.6812 (0.6796)  time: 0.5699  data: 0.0003  max mem: 6820\n[00:59:16.345890] Epoch: [23]  [120/969]  eta: 0:08:08  lr: 0.000286  loss: 0.6918 (0.6816)  time: 0.5688  data: 0.0002  max mem: 6820\n[00:59:27.730961] Epoch: [23]  [140/969]  eta: 0:07:56  lr: 0.000286  loss: 0.6801 (0.6808)  time: 0.5692  data: 0.0002  max mem: 6820\n[00:59:39.065949] Epoch: [23]  [160/969]  eta: 0:07:43  lr: 0.000286  loss: 0.6569 (0.6781)  time: 0.5667  data: 0.0003  max mem: 6820\n[00:59:50.405739] Epoch: [23]  [180/969]  eta: 0:07:31  lr: 0.000286  loss: 0.6888 (0.6810)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:00:01.737437] Epoch: [23]  [200/969]  eta: 0:07:19  lr: 0.000286  loss: 0.6896 (0.6821)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:00:13.061741] Epoch: [23]  [220/969]  eta: 0:07:08  lr: 0.000286  loss: 0.6823 (0.6820)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:00:24.389935] Epoch: [23]  [240/969]  eta: 0:06:56  lr: 0.000286  loss: 0.6807 (0.6824)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:00:35.721870] Epoch: [23]  [260/969]  eta: 0:06:44  lr: 0.000286  loss: 0.6569 (0.6812)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:00:47.059219] Epoch: [23]  [280/969]  eta: 0:06:33  lr: 0.000286  loss: 0.6713 (0.6810)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:00:58.382310] Epoch: [23]  [300/969]  eta: 0:06:21  lr: 0.000286  loss: 0.6719 (0.6814)  time: 0.5661  data: 0.0002  max mem: 6820\n[01:01:09.710575] Epoch: [23]  [320/969]  eta: 0:06:09  lr: 0.000285  loss: 0.6731 (0.6807)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:01:21.031631] Epoch: [23]  [340/969]  eta: 0:05:58  lr: 0.000285  loss: 0.6797 (0.6808)  time: 0.5660  data: 0.0002  max mem: 6820\n[01:01:32.362524] Epoch: [23]  [360/969]  eta: 0:05:46  lr: 0.000285  loss: 0.6816 (0.6810)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:01:43.711169] Epoch: [23]  [380/969]  eta: 0:05:35  lr: 0.000285  loss: 0.6724 (0.6806)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:01:54.962446] Epoch: [23]  [400/969]  eta: 0:05:23  lr: 0.000285  loss: 0.6619 (0.6795)  time: 0.5625  data: 0.0002  max mem: 6820\n[01:02:06.304516] Epoch: [23]  [420/969]  eta: 0:05:12  lr: 0.000285  loss: 0.6506 (0.6794)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:02:17.667037] Epoch: [23]  [440/969]  eta: 0:05:00  lr: 0.000285  loss: 0.6870 (0.6796)  time: 0.5681  data: 0.0002  max mem: 6820\n[01:02:28.998751] Epoch: [23]  [460/969]  eta: 0:04:49  lr: 0.000285  loss: 0.6680 (0.6792)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:02:40.332196] Epoch: [23]  [480/969]  eta: 0:04:38  lr: 0.000285  loss: 0.6811 (0.6796)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:02:51.664035] Epoch: [23]  [500/969]  eta: 0:04:26  lr: 0.000285  loss: 0.6851 (0.6799)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:03:03.007768] Epoch: [23]  [520/969]  eta: 0:04:15  lr: 0.000285  loss: 0.6561 (0.6795)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:03:14.346822] Epoch: [23]  [540/969]  eta: 0:04:03  lr: 0.000285  loss: 0.6655 (0.6794)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:03:25.671657] Epoch: [23]  [560/969]  eta: 0:03:52  lr: 0.000284  loss: 0.6646 (0.6794)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:03:36.995535] Epoch: [23]  [580/969]  eta: 0:03:41  lr: 0.000284  loss: 0.6704 (0.6794)  time: 0.5661  data: 0.0002  max mem: 6820\n[01:03:48.322630] Epoch: [23]  [600/969]  eta: 0:03:29  lr: 0.000284  loss: 0.6895 (0.6800)  time: 0.5663  data: 0.0002  max mem: 6820\n[01:03:59.655041] Epoch: [23]  [620/969]  eta: 0:03:18  lr: 0.000284  loss: 0.6946 (0.6805)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:04:10.986138] Epoch: [23]  [640/969]  eta: 0:03:06  lr: 0.000284  loss: 0.6889 (0.6806)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:04:22.336088] Epoch: [23]  [660/969]  eta: 0:02:55  lr: 0.000284  loss: 0.6740 (0.6807)  time: 0.5675  data: 0.0002  max mem: 6820\n[01:04:33.699355] Epoch: [23]  [680/969]  eta: 0:02:44  lr: 0.000284  loss: 0.6812 (0.6809)  time: 0.5681  data: 0.0002  max mem: 6820\n[01:04:45.098811] Epoch: [23]  [700/969]  eta: 0:02:32  lr: 0.000284  loss: 0.6791 (0.6807)  time: 0.5699  data: 0.0002  max mem: 6820\n[01:04:56.510117] Epoch: [23]  [720/969]  eta: 0:02:21  lr: 0.000284  loss: 0.6840 (0.6810)  time: 0.5705  data: 0.0002  max mem: 6820\n[01:05:07.903144] Epoch: [23]  [740/969]  eta: 0:02:10  lr: 0.000284  loss: 0.6708 (0.6809)  time: 0.5696  data: 0.0002  max mem: 6820\n[01:05:19.279804] Epoch: [23]  [760/969]  eta: 0:01:58  lr: 0.000284  loss: 0.6804 (0.6812)  time: 0.5688  data: 0.0003  max mem: 6820\n[01:05:30.627814] Epoch: [23]  [780/969]  eta: 0:01:47  lr: 0.000284  loss: 0.6631 (0.6809)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:05:41.993991] Epoch: [23]  [800/969]  eta: 0:01:36  lr: 0.000283  loss: 0.6881 (0.6811)  time: 0.5682  data: 0.0002  max mem: 6820\n[01:05:53.343913] Epoch: [23]  [820/969]  eta: 0:01:24  lr: 0.000283  loss: 0.6472 (0.6808)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:06:04.705801] Epoch: [23]  [840/969]  eta: 0:01:13  lr: 0.000283  loss: 0.6635 (0.6807)  time: 0.5680  data: 0.0003  max mem: 6820\n[01:06:16.093885] Epoch: [23]  [860/969]  eta: 0:01:01  lr: 0.000283  loss: 0.6540 (0.6800)  time: 0.5693  data: 0.0002  max mem: 6820\n[01:06:27.467562] Epoch: [23]  [880/969]  eta: 0:00:50  lr: 0.000283  loss: 0.6818 (0.6804)  time: 0.5686  data: 0.0002  max mem: 6820\n[01:06:38.833382] Epoch: [23]  [900/969]  eta: 0:00:39  lr: 0.000283  loss: 0.6600 (0.6801)  time: 0.5682  data: 0.0003  max mem: 6820\n[01:06:50.187449] Epoch: [23]  [920/969]  eta: 0:00:27  lr: 0.000283  loss: 0.6736 (0.6800)  time: 0.5676  data: 0.0002  max mem: 6820\n[01:07:01.528381] Epoch: [23]  [940/969]  eta: 0:00:16  lr: 0.000283  loss: 0.6833 (0.6799)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:07:12.865130] Epoch: [23]  [960/969]  eta: 0:00:05  lr: 0.000283  loss: 0.6794 (0.6802)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:07:17.405033] Epoch: [23]  [968/969]  eta: 0:00:00  lr: 0.000283  loss: 0.6638 (0.6801)  time: 0.5672  data: 0.0002  max mem: 6820\n[01:07:17.522221] Epoch: [23] Total time: 0:09:10 (0.5684 s / it)\n[01:07:17.522334] Averaged stats: lr: 0.000283  loss: 0.6638 (0.6801)\n[01:07:18.243411] val:  [  0/139]  eta: 0:01:39  loss: 0.7027 (0.7027)  time: 0.7166  data: 0.5737  max mem: 6820\n[01:07:19.748406] val:  [ 10/139]  eta: 0:00:26  loss: 0.6649 (0.6512)  time: 0.2019  data: 0.0538  max mem: 6820\n[01:07:21.253501] val:  [ 20/139]  eta: 0:00:21  loss: 0.6147 (0.6317)  time: 0.1504  data: 0.0010  max mem: 6820\n[01:07:22.756571] val:  [ 30/139]  eta: 0:00:18  loss: 0.6230 (0.6388)  time: 0.1503  data: 0.0002  max mem: 6820\n[01:07:24.267833] val:  [ 40/139]  eta: 0:00:16  loss: 0.7088 (0.6617)  time: 0.1506  data: 0.0002  max mem: 6820\n[01:07:25.775721] val:  [ 50/139]  eta: 0:00:14  loss: 0.7194 (0.6756)  time: 0.1509  data: 0.0002  max mem: 6820\n[01:07:27.288242] val:  [ 60/139]  eta: 0:00:12  loss: 0.7319 (0.6836)  time: 0.1509  data: 0.0002  max mem: 6820\n[01:07:28.802997] val:  [ 70/139]  eta: 0:00:10  loss: 0.7319 (0.6878)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:07:30.312866] val:  [ 80/139]  eta: 0:00:09  loss: 0.6767 (0.6859)  time: 0.1512  data: 0.0002  max mem: 6820\n[01:07:31.826005] val:  [ 90/139]  eta: 0:00:07  loss: 0.6725 (0.6842)  time: 0.1511  data: 0.0002  max mem: 6820\n[01:07:33.340508] val:  [100/139]  eta: 0:00:06  loss: 0.6653 (0.6813)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:07:34.853216] val:  [110/139]  eta: 0:00:04  loss: 0.6543 (0.6774)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:07:36.356245] val:  [120/139]  eta: 0:00:02  loss: 0.6298 (0.6709)  time: 0.1507  data: 0.0002  max mem: 6820\n[01:07:37.862705] val:  [130/139]  eta: 0:00:01  loss: 0.5930 (0.6622)  time: 0.1504  data: 0.0002  max mem: 6820\n[01:07:38.973219] val:  [138/139]  eta: 0:00:00  loss: 0.5484 (0.6537)  time: 0.1458  data: 0.0001  max mem: 6820\n[01:07:39.076285] val: Total time: 0:00:21 (0.1550 s / it)\n[01:07:39.133767] val loss: 0.6536955250252923\n[01:07:39.134188] Accuracy: 0.6257, F1 Score: 0.6256, ROC AUC: 0.6655, Hamming Loss: 0.3743,\n Jaccard Score: 0.4552, Precision: 0.6257, Recall: 0.6257,\n Average Precision: 0.6531, Kappa: 0.2514, Score: 0.5142\n[01:07:41.925160] Best epoch = 23, Best score = 0.5142\n[01:07:41.927902] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[01:07:43.505288] Epoch: [24]  [  0/969]  eta: 0:25:27  lr: 0.000283  loss: 0.6899 (0.6899)  time: 1.5759  data: 0.9477  max mem: 6820\n[01:07:54.780469] Epoch: [24]  [ 20/969]  eta: 0:09:40  lr: 0.000283  loss: 0.6595 (0.6718)  time: 0.5637  data: 0.0002  max mem: 6820\n[01:08:06.251714] Epoch: [24]  [ 40/969]  eta: 0:09:11  lr: 0.000283  loss: 0.6650 (0.6734)  time: 0.5735  data: 0.0002  max mem: 6820\n[01:08:17.769787] Epoch: [24]  [ 60/969]  eta: 0:08:54  lr: 0.000282  loss: 0.6632 (0.6702)  time: 0.5758  data: 0.0002  max mem: 6820\n[01:08:29.267089] Epoch: [24]  [ 80/969]  eta: 0:08:39  lr: 0.000282  loss: 0.6689 (0.6692)  time: 0.5748  data: 0.0002  max mem: 6820\n[01:08:40.645598] Epoch: [24]  [100/969]  eta: 0:08:25  lr: 0.000282  loss: 0.6512 (0.6709)  time: 0.5689  data: 0.0002  max mem: 6820\n[01:08:51.973581] Epoch: [24]  [120/969]  eta: 0:08:11  lr: 0.000282  loss: 0.6934 (0.6749)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:09:03.265662] Epoch: [24]  [140/969]  eta: 0:07:58  lr: 0.000282  loss: 0.6609 (0.6739)  time: 0.5646  data: 0.0002  max mem: 6820\n[01:09:14.570910] Epoch: [24]  [160/969]  eta: 0:07:45  lr: 0.000282  loss: 0.6587 (0.6736)  time: 0.5652  data: 0.0002  max mem: 6820\n[01:09:25.896348] Epoch: [24]  [180/969]  eta: 0:07:33  lr: 0.000282  loss: 0.6505 (0.6737)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:09:37.257862] Epoch: [24]  [200/969]  eta: 0:07:21  lr: 0.000282  loss: 0.6651 (0.6746)  time: 0.5680  data: 0.0002  max mem: 6820\n[01:09:48.641634] Epoch: [24]  [220/969]  eta: 0:07:09  lr: 0.000282  loss: 0.6769 (0.6747)  time: 0.5691  data: 0.0002  max mem: 6820\n[01:10:00.046410] Epoch: [24]  [240/969]  eta: 0:06:57  lr: 0.000282  loss: 0.6759 (0.6749)  time: 0.5702  data: 0.0002  max mem: 6820\n[01:10:11.426783] Epoch: [24]  [260/969]  eta: 0:06:46  lr: 0.000282  loss: 0.6753 (0.6750)  time: 0.5690  data: 0.0002  max mem: 6820\n[01:10:22.808175] Epoch: [24]  [280/969]  eta: 0:06:34  lr: 0.000282  loss: 0.6762 (0.6754)  time: 0.5690  data: 0.0002  max mem: 6820\n[01:10:34.178089] Epoch: [24]  [300/969]  eta: 0:06:22  lr: 0.000281  loss: 0.6814 (0.6760)  time: 0.5684  data: 0.0002  max mem: 6820\n[01:10:45.543858] Epoch: [24]  [320/969]  eta: 0:06:11  lr: 0.000281  loss: 0.6712 (0.6761)  time: 0.5682  data: 0.0002  max mem: 6820\n[01:10:56.884413] Epoch: [24]  [340/969]  eta: 0:05:59  lr: 0.000281  loss: 0.6698 (0.6761)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:11:08.227916] Epoch: [24]  [360/969]  eta: 0:05:47  lr: 0.000281  loss: 0.6977 (0.6770)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:11:19.569931] Epoch: [24]  [380/969]  eta: 0:05:36  lr: 0.000281  loss: 0.6650 (0.6767)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:11:30.895446] Epoch: [24]  [400/969]  eta: 0:05:24  lr: 0.000281  loss: 0.6793 (0.6767)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:11:42.247503] Epoch: [24]  [420/969]  eta: 0:05:13  lr: 0.000281  loss: 0.6514 (0.6764)  time: 0.5675  data: 0.0002  max mem: 6820\n[01:11:53.576831] Epoch: [24]  [440/969]  eta: 0:05:01  lr: 0.000281  loss: 0.6513 (0.6753)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:12:04.916703] Epoch: [24]  [460/969]  eta: 0:04:50  lr: 0.000281  loss: 0.6741 (0.6756)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:12:16.253315] Epoch: [24]  [480/969]  eta: 0:04:38  lr: 0.000281  loss: 0.6796 (0.6758)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:12:27.598548] Epoch: [24]  [500/969]  eta: 0:04:27  lr: 0.000281  loss: 0.6804 (0.6759)  time: 0.5672  data: 0.0002  max mem: 6820\n[01:12:38.921696] Epoch: [24]  [520/969]  eta: 0:04:15  lr: 0.000281  loss: 0.6457 (0.6751)  time: 0.5661  data: 0.0002  max mem: 6820\n[01:12:50.252175] Epoch: [24]  [540/969]  eta: 0:04:04  lr: 0.000280  loss: 0.6531 (0.6747)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:13:01.599232] Epoch: [24]  [560/969]  eta: 0:03:53  lr: 0.000280  loss: 0.6667 (0.6748)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:13:12.921908] Epoch: [24]  [580/969]  eta: 0:03:41  lr: 0.000280  loss: 0.6579 (0.6746)  time: 0.5661  data: 0.0002  max mem: 6820\n[01:13:24.252520] Epoch: [24]  [600/969]  eta: 0:03:30  lr: 0.000280  loss: 0.6643 (0.6748)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:13:35.579809] Epoch: [24]  [620/969]  eta: 0:03:18  lr: 0.000280  loss: 0.6821 (0.6751)  time: 0.5663  data: 0.0002  max mem: 6820\n[01:13:46.899020] Epoch: [24]  [640/969]  eta: 0:03:07  lr: 0.000280  loss: 0.6627 (0.6748)  time: 0.5659  data: 0.0002  max mem: 6820\n[01:13:58.227870] Epoch: [24]  [660/969]  eta: 0:02:55  lr: 0.000280  loss: 0.6701 (0.6750)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:14:09.555783] Epoch: [24]  [680/969]  eta: 0:02:44  lr: 0.000280  loss: 0.6845 (0.6755)  time: 0.5663  data: 0.0002  max mem: 6820\n[01:14:20.881465] Epoch: [24]  [700/969]  eta: 0:02:33  lr: 0.000280  loss: 0.6656 (0.6753)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:14:32.248278] Epoch: [24]  [720/969]  eta: 0:02:21  lr: 0.000280  loss: 0.6633 (0.6753)  time: 0.5683  data: 0.0002  max mem: 6820\n[01:14:43.617544] Epoch: [24]  [740/969]  eta: 0:02:10  lr: 0.000280  loss: 0.6573 (0.6748)  time: 0.5684  data: 0.0002  max mem: 6820\n[01:14:55.010155] Epoch: [24]  [760/969]  eta: 0:01:58  lr: 0.000279  loss: 0.6451 (0.6748)  time: 0.5696  data: 0.0002  max mem: 6820\n[01:15:06.404318] Epoch: [24]  [780/969]  eta: 0:01:47  lr: 0.000279  loss: 0.6767 (0.6750)  time: 0.5696  data: 0.0002  max mem: 6820\n[01:15:17.841378] Epoch: [24]  [800/969]  eta: 0:01:36  lr: 0.000279  loss: 0.6649 (0.6752)  time: 0.5718  data: 0.0003  max mem: 6820\n[01:15:29.227647] Epoch: [24]  [820/969]  eta: 0:01:24  lr: 0.000279  loss: 0.6540 (0.6752)  time: 0.5693  data: 0.0002  max mem: 6820\n[01:15:40.600117] Epoch: [24]  [840/969]  eta: 0:01:13  lr: 0.000279  loss: 0.6683 (0.6748)  time: 0.5686  data: 0.0002  max mem: 6820\n[01:15:51.960135] Epoch: [24]  [860/969]  eta: 0:01:02  lr: 0.000279  loss: 0.6675 (0.6746)  time: 0.5679  data: 0.0002  max mem: 6820\n[01:16:03.294779] Epoch: [24]  [880/969]  eta: 0:00:50  lr: 0.000279  loss: 0.6546 (0.6746)  time: 0.5667  data: 0.0002  max mem: 6820\n[01:16:14.602704] Epoch: [24]  [900/969]  eta: 0:00:39  lr: 0.000279  loss: 0.6774 (0.6747)  time: 0.5653  data: 0.0002  max mem: 6820\n[01:16:25.916604] Epoch: [24]  [920/969]  eta: 0:00:27  lr: 0.000279  loss: 0.6511 (0.6744)  time: 0.5656  data: 0.0002  max mem: 6820\n[01:16:37.248421] Epoch: [24]  [940/969]  eta: 0:00:16  lr: 0.000279  loss: 0.6606 (0.6741)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:16:48.594965] Epoch: [24]  [960/969]  eta: 0:00:05  lr: 0.000279  loss: 0.6799 (0.6744)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:16:53.122595] Epoch: [24]  [968/969]  eta: 0:00:00  lr: 0.000279  loss: 0.6805 (0.6745)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:16:53.250176] Epoch: [24] Total time: 0:09:11 (0.5690 s / it)\n[01:16:53.250286] Averaged stats: lr: 0.000279  loss: 0.6805 (0.6745)\n[01:16:54.053552] val:  [  0/139]  eta: 0:01:49  loss: 0.6125 (0.6125)  time: 0.7907  data: 0.6274  max mem: 6820\n[01:16:55.547349] val:  [ 10/139]  eta: 0:00:26  loss: 0.5868 (0.5358)  time: 0.2075  data: 0.0572  max mem: 6820\n[01:16:57.048794] val:  [ 20/139]  eta: 0:00:21  loss: 0.5237 (0.5307)  time: 0.1496  data: 0.0002  max mem: 6820\n[01:16:58.552865] val:  [ 30/139]  eta: 0:00:18  loss: 0.5237 (0.5345)  time: 0.1502  data: 0.0002  max mem: 6820\n[01:17:00.063217] val:  [ 40/139]  eta: 0:00:16  loss: 0.5811 (0.5512)  time: 0.1506  data: 0.0002  max mem: 6820\n[01:17:01.575234] val:  [ 50/139]  eta: 0:00:14  loss: 0.5850 (0.5591)  time: 0.1510  data: 0.0002  max mem: 6820\n[01:17:03.091436] val:  [ 60/139]  eta: 0:00:12  loss: 0.5917 (0.5667)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:17:04.609062] val:  [ 70/139]  eta: 0:00:11  loss: 0.6152 (0.5781)  time: 0.1516  data: 0.0002  max mem: 6820\n[01:17:06.118201] val:  [ 80/139]  eta: 0:00:09  loss: 0.6746 (0.5986)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:17:07.638051] val:  [ 90/139]  eta: 0:00:07  loss: 0.7713 (0.6185)  time: 0.1514  data: 0.0002  max mem: 6820\n[01:17:09.162806] val:  [100/139]  eta: 0:00:06  loss: 0.7866 (0.6344)  time: 0.1522  data: 0.0002  max mem: 6820\n[01:17:10.676108] val:  [110/139]  eta: 0:00:04  loss: 0.7840 (0.6460)  time: 0.1518  data: 0.0002  max mem: 6820\n[01:17:12.193610] val:  [120/139]  eta: 0:00:02  loss: 0.6951 (0.6499)  time: 0.1515  data: 0.0002  max mem: 6820\n[01:17:13.710226] val:  [130/139]  eta: 0:00:01  loss: 0.6674 (0.6474)  time: 0.1516  data: 0.0002  max mem: 6820\n[01:17:14.828787] val:  [138/139]  eta: 0:00:00  loss: 0.6338 (0.6449)  time: 0.1468  data: 0.0001  max mem: 6820\n[01:17:14.933425] val: Total time: 0:00:21 (0.1560 s / it)\n[01:17:14.988654] val loss: 0.6449499884955316\n[01:17:14.988701] Accuracy: 0.6248, F1 Score: 0.6182, ROC AUC: 0.6858, Hamming Loss: 0.3752,\n Jaccard Score: 0.4493, Precision: 0.6340, Recall: 0.6248,\n Average Precision: 0.6828, Kappa: 0.2495, Score: 0.5178\n[01:17:17.985255] Best epoch = 24, Best score = 0.5178\n[01:17:17.988244] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[01:17:19.413347] Epoch: [25]  [  0/969]  eta: 0:22:59  lr: 0.000279  loss: 0.6947 (0.6947)  time: 1.4237  data: 0.7752  max mem: 6820\n[01:17:30.684920] Epoch: [25]  [ 20/969]  eta: 0:09:33  lr: 0.000278  loss: 0.6531 (0.6613)  time: 0.5635  data: 0.0002  max mem: 6820\n[01:17:42.132631] Epoch: [25]  [ 40/969]  eta: 0:09:06  lr: 0.000278  loss: 0.6703 (0.6673)  time: 0.5723  data: 0.0002  max mem: 6820\n[01:17:53.628423] Epoch: [25]  [ 60/969]  eta: 0:08:51  lr: 0.000278  loss: 0.6537 (0.6670)  time: 0.5747  data: 0.0002  max mem: 6820\n[01:18:05.075085] Epoch: [25]  [ 80/969]  eta: 0:08:36  lr: 0.000278  loss: 0.6714 (0.6699)  time: 0.5723  data: 0.0002  max mem: 6820\n[01:18:16.431963] Epoch: [25]  [100/969]  eta: 0:08:22  lr: 0.000278  loss: 0.6716 (0.6716)  time: 0.5678  data: 0.0002  max mem: 6820\n[01:18:27.748456] Epoch: [25]  [120/969]  eta: 0:08:09  lr: 0.000278  loss: 0.6644 (0.6732)  time: 0.5658  data: 0.0002  max mem: 6820\n[01:18:39.056015] Epoch: [25]  [140/969]  eta: 0:07:56  lr: 0.000278  loss: 0.6799 (0.6749)  time: 0.5653  data: 0.0002  max mem: 6820\n[01:18:50.355566] Epoch: [25]  [160/969]  eta: 0:07:44  lr: 0.000278  loss: 0.6608 (0.6738)  time: 0.5649  data: 0.0002  max mem: 6820\n[01:19:01.686214] Epoch: [25]  [180/969]  eta: 0:07:31  lr: 0.000278  loss: 0.6742 (0.6749)  time: 0.5665  data: 0.0001  max mem: 6820\n[01:19:13.022035] Epoch: [25]  [200/969]  eta: 0:07:20  lr: 0.000278  loss: 0.6610 (0.6735)  time: 0.5667  data: 0.0002  max mem: 6820\n[01:19:24.392373] Epoch: [25]  [220/969]  eta: 0:07:08  lr: 0.000278  loss: 0.6703 (0.6740)  time: 0.5685  data: 0.0002  max mem: 6820\n[01:19:35.746171] Epoch: [25]  [240/969]  eta: 0:06:56  lr: 0.000277  loss: 0.6764 (0.6744)  time: 0.5676  data: 0.0002  max mem: 6820\n[01:19:47.101637] Epoch: [25]  [260/969]  eta: 0:06:45  lr: 0.000277  loss: 0.6635 (0.6740)  time: 0.5677  data: 0.0002  max mem: 6820\n[01:19:58.459061] Epoch: [25]  [280/969]  eta: 0:06:33  lr: 0.000277  loss: 0.6687 (0.6746)  time: 0.5678  data: 0.0002  max mem: 6820\n[01:20:09.799712] Epoch: [25]  [300/969]  eta: 0:06:21  lr: 0.000277  loss: 0.6885 (0.6755)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:20:21.124691] Epoch: [25]  [320/969]  eta: 0:06:10  lr: 0.000277  loss: 0.6649 (0.6749)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:20:32.446516] Epoch: [25]  [340/969]  eta: 0:05:58  lr: 0.000277  loss: 0.6613 (0.6747)  time: 0.5660  data: 0.0002  max mem: 6820\n[01:20:43.786228] Epoch: [25]  [360/969]  eta: 0:05:47  lr: 0.000277  loss: 0.6641 (0.6743)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:20:55.129043] Epoch: [25]  [380/969]  eta: 0:05:35  lr: 0.000277  loss: 0.6543 (0.6740)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:21:06.444837] Epoch: [25]  [400/969]  eta: 0:05:24  lr: 0.000277  loss: 0.6735 (0.6743)  time: 0.5657  data: 0.0002  max mem: 6820\n[01:21:17.784312] Epoch: [25]  [420/969]  eta: 0:05:12  lr: 0.000277  loss: 0.6546 (0.6738)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:21:29.113432] Epoch: [25]  [440/969]  eta: 0:05:01  lr: 0.000277  loss: 0.6728 (0.6747)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:21:40.446247] Epoch: [25]  [460/969]  eta: 0:04:49  lr: 0.000276  loss: 0.6475 (0.6746)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:21:51.689435] Epoch: [25]  [480/969]  eta: 0:04:38  lr: 0.000276  loss: 0.6772 (0.6749)  time: 0.5621  data: 0.0002  max mem: 6820\n[01:22:03.014018] Epoch: [25]  [500/969]  eta: 0:04:26  lr: 0.000276  loss: 0.6760 (0.6753)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:22:14.355894] Epoch: [25]  [520/969]  eta: 0:04:15  lr: 0.000276  loss: 0.6568 (0.6746)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:22:25.698787] Epoch: [25]  [540/969]  eta: 0:04:03  lr: 0.000276  loss: 0.6589 (0.6739)  time: 0.5671  data: 0.0001  max mem: 6820\n[01:22:37.046092] Epoch: [25]  [560/969]  eta: 0:03:52  lr: 0.000276  loss: 0.6660 (0.6736)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:22:48.391142] Epoch: [25]  [580/969]  eta: 0:03:41  lr: 0.000276  loss: 0.6815 (0.6737)  time: 0.5672  data: 0.0002  max mem: 6820\n[01:22:59.714373] Epoch: [25]  [600/969]  eta: 0:03:29  lr: 0.000276  loss: 0.6639 (0.6739)  time: 0.5661  data: 0.0002  max mem: 6820\n[01:23:11.045658] Epoch: [25]  [620/969]  eta: 0:03:18  lr: 0.000276  loss: 0.6614 (0.6736)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:23:22.381965] Epoch: [25]  [640/969]  eta: 0:03:07  lr: 0.000276  loss: 0.6705 (0.6735)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:23:33.731863] Epoch: [25]  [660/969]  eta: 0:02:55  lr: 0.000275  loss: 0.6380 (0.6731)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:23:45.072047] Epoch: [25]  [680/969]  eta: 0:02:44  lr: 0.000275  loss: 0.6918 (0.6736)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:23:56.401553] Epoch: [25]  [700/969]  eta: 0:02:32  lr: 0.000275  loss: 0.6550 (0.6733)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:24:07.719077] Epoch: [25]  [720/969]  eta: 0:02:21  lr: 0.000275  loss: 0.6820 (0.6736)  time: 0.5658  data: 0.0002  max mem: 6820\n[01:24:19.059157] Epoch: [25]  [740/969]  eta: 0:02:10  lr: 0.000275  loss: 0.6518 (0.6732)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:24:30.399026] Epoch: [25]  [760/969]  eta: 0:01:58  lr: 0.000275  loss: 0.6490 (0.6731)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:24:41.741480] Epoch: [25]  [780/969]  eta: 0:01:47  lr: 0.000275  loss: 0.6750 (0.6731)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:24:53.056073] Epoch: [25]  [800/969]  eta: 0:01:36  lr: 0.000275  loss: 0.6777 (0.6734)  time: 0.5657  data: 0.0002  max mem: 6820\n[01:25:04.403254] Epoch: [25]  [820/969]  eta: 0:01:24  lr: 0.000275  loss: 0.6556 (0.6732)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:25:15.735237] Epoch: [25]  [840/969]  eta: 0:01:13  lr: 0.000275  loss: 0.6627 (0.6731)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:25:27.060846] Epoch: [25]  [860/969]  eta: 0:01:01  lr: 0.000275  loss: 0.6748 (0.6733)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:25:38.393579] Epoch: [25]  [880/969]  eta: 0:00:50  lr: 0.000274  loss: 0.6573 (0.6732)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:25:49.735118] Epoch: [25]  [900/969]  eta: 0:00:39  lr: 0.000274  loss: 0.6788 (0.6734)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:26:01.084766] Epoch: [25]  [920/969]  eta: 0:00:27  lr: 0.000274  loss: 0.6752 (0.6734)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:26:12.409160] Epoch: [25]  [940/969]  eta: 0:00:16  lr: 0.000274  loss: 0.6778 (0.6735)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:26:23.743034] Epoch: [25]  [960/969]  eta: 0:00:05  lr: 0.000274  loss: 0.6713 (0.6736)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:26:28.286674] Epoch: [25]  [968/969]  eta: 0:00:00  lr: 0.000274  loss: 0.6713 (0.6737)  time: 0.5675  data: 0.0001  max mem: 6820\n[01:26:28.404098] Epoch: [25] Total time: 0:09:10 (0.5680 s / it)\n[01:26:28.404208] Averaged stats: lr: 0.000274  loss: 0.6713 (0.6737)\n[01:26:29.203790] val:  [  0/139]  eta: 0:01:50  loss: 0.6715 (0.6715)  time: 0.7950  data: 0.6495  max mem: 6820\n[01:26:30.706030] val:  [ 10/139]  eta: 0:00:26  loss: 0.6385 (0.6057)  time: 0.2088  data: 0.0593  max mem: 6820\n[01:26:32.203991] val:  [ 20/139]  eta: 0:00:21  loss: 0.5869 (0.5932)  time: 0.1499  data: 0.0002  max mem: 6820\n[01:26:33.712570] val:  [ 30/139]  eta: 0:00:18  loss: 0.5869 (0.5951)  time: 0.1503  data: 0.0002  max mem: 6820\n[01:26:35.219656] val:  [ 40/139]  eta: 0:00:16  loss: 0.6394 (0.6134)  time: 0.1507  data: 0.0002  max mem: 6820\n[01:26:36.731189] val:  [ 50/139]  eta: 0:00:14  loss: 0.6394 (0.6204)  time: 0.1509  data: 0.0002  max mem: 6820\n[01:26:38.246256] val:  [ 60/139]  eta: 0:00:12  loss: 0.6587 (0.6253)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:26:39.753702] val:  [ 70/139]  eta: 0:00:11  loss: 0.6645 (0.6312)  time: 0.1511  data: 0.0002  max mem: 6820\n[01:26:41.268503] val:  [ 80/139]  eta: 0:00:09  loss: 0.6571 (0.6377)  time: 0.1510  data: 0.0002  max mem: 6820\n[01:26:42.778831] val:  [ 90/139]  eta: 0:00:07  loss: 0.7302 (0.6483)  time: 0.1512  data: 0.0002  max mem: 6820\n[01:26:44.297054] val:  [100/139]  eta: 0:00:06  loss: 0.7364 (0.6564)  time: 0.1514  data: 0.0002  max mem: 6820\n[01:26:45.808465] val:  [110/139]  eta: 0:00:04  loss: 0.7282 (0.6604)  time: 0.1514  data: 0.0002  max mem: 6820\n[01:26:47.317588] val:  [120/139]  eta: 0:00:02  loss: 0.6448 (0.6578)  time: 0.1510  data: 0.0002  max mem: 6820\n[01:26:48.829524] val:  [130/139]  eta: 0:00:01  loss: 0.6065 (0.6488)  time: 0.1510  data: 0.0001  max mem: 6820\n[01:26:49.945466] val:  [138/139]  eta: 0:00:00  loss: 0.5673 (0.6421)  time: 0.1464  data: 0.0001  max mem: 6820\n[01:26:50.053450] val: Total time: 0:00:21 (0.1557 s / it)\n[01:26:50.107539] val loss: 0.642075271057568\n[01:26:50.107599] Accuracy: 0.6225, F1 Score: 0.6211, ROC AUC: 0.6832, Hamming Loss: 0.3775,\n Jaccard Score: 0.4508, Precision: 0.6244, Recall: 0.6225,\n Average Precision: 0.6802, Kappa: 0.2450, Score: 0.5164\n[01:26:50.109692] Best epoch = 24, Best score = 0.5178\n[01:26:50.112198] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[01:26:51.448725] Epoch: [26]  [  0/969]  eta: 0:21:33  lr: 0.000274  loss: 0.6169 (0.6169)  time: 1.3353  data: 0.7376  max mem: 6820\n[01:27:02.801669] Epoch: [26]  [ 20/969]  eta: 0:09:33  lr: 0.000274  loss: 0.6533 (0.6700)  time: 0.5676  data: 0.0002  max mem: 6820\n[01:27:14.165802] Epoch: [26]  [ 40/969]  eta: 0:09:04  lr: 0.000274  loss: 0.6611 (0.6649)  time: 0.5682  data: 0.0001  max mem: 6820\n[01:27:25.566303] Epoch: [26]  [ 60/969]  eta: 0:08:48  lr: 0.000274  loss: 0.6520 (0.6673)  time: 0.5700  data: 0.0002  max mem: 6820\n[01:27:36.931320] Epoch: [26]  [ 80/969]  eta: 0:08:33  lr: 0.000274  loss: 0.6860 (0.6689)  time: 0.5682  data: 0.0002  max mem: 6820\n[01:27:48.287207] Epoch: [26]  [100/969]  eta: 0:08:20  lr: 0.000274  loss: 0.6665 (0.6696)  time: 0.5677  data: 0.0002  max mem: 6820\n[01:27:59.621127] Epoch: [26]  [120/969]  eta: 0:08:07  lr: 0.000273  loss: 0.6711 (0.6703)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:28:10.989948] Epoch: [26]  [140/969]  eta: 0:07:55  lr: 0.000273  loss: 0.6837 (0.6734)  time: 0.5684  data: 0.0002  max mem: 6820\n[01:28:22.337407] Epoch: [26]  [160/969]  eta: 0:07:43  lr: 0.000273  loss: 0.6634 (0.6731)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:28:33.658173] Epoch: [26]  [180/969]  eta: 0:07:31  lr: 0.000273  loss: 0.6895 (0.6744)  time: 0.5660  data: 0.0002  max mem: 6820\n[01:28:44.989457] Epoch: [26]  [200/969]  eta: 0:07:19  lr: 0.000273  loss: 0.6750 (0.6752)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:28:56.319810] Epoch: [26]  [220/969]  eta: 0:07:07  lr: 0.000273  loss: 0.6712 (0.6753)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:29:07.666701] Epoch: [26]  [240/969]  eta: 0:06:56  lr: 0.000273  loss: 0.6735 (0.6749)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:29:19.006027] Epoch: [26]  [260/969]  eta: 0:06:44  lr: 0.000273  loss: 0.6801 (0.6756)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:29:30.361162] Epoch: [26]  [280/969]  eta: 0:06:32  lr: 0.000273  loss: 0.6755 (0.6763)  time: 0.5677  data: 0.0002  max mem: 6820\n[01:29:41.689026] Epoch: [26]  [300/969]  eta: 0:06:21  lr: 0.000273  loss: 0.6791 (0.6768)  time: 0.5663  data: 0.0002  max mem: 6820\n[01:29:53.012924] Epoch: [26]  [320/969]  eta: 0:06:09  lr: 0.000273  loss: 0.6713 (0.6773)  time: 0.5661  data: 0.0002  max mem: 6820\n[01:30:04.351958] Epoch: [26]  [340/969]  eta: 0:05:58  lr: 0.000272  loss: 0.6653 (0.6770)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:30:15.701219] Epoch: [26]  [360/969]  eta: 0:05:46  lr: 0.000272  loss: 0.6714 (0.6776)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:30:27.051742] Epoch: [26]  [380/969]  eta: 0:05:35  lr: 0.000272  loss: 0.6835 (0.6780)  time: 0.5675  data: 0.0002  max mem: 6820\n[01:30:38.399234] Epoch: [26]  [400/969]  eta: 0:05:23  lr: 0.000272  loss: 0.6546 (0.6775)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:30:49.743341] Epoch: [26]  [420/969]  eta: 0:05:12  lr: 0.000272  loss: 0.6678 (0.6769)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:31:01.069777] Epoch: [26]  [440/969]  eta: 0:05:01  lr: 0.000272  loss: 0.6679 (0.6772)  time: 0.5663  data: 0.0002  max mem: 6820\n[01:31:12.406176] Epoch: [26]  [460/969]  eta: 0:04:49  lr: 0.000272  loss: 0.6687 (0.6769)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:31:23.750717] Epoch: [26]  [480/969]  eta: 0:04:38  lr: 0.000272  loss: 0.6777 (0.6772)  time: 0.5672  data: 0.0002  max mem: 6820\n[01:31:35.070243] Epoch: [26]  [500/969]  eta: 0:04:26  lr: 0.000272  loss: 0.6742 (0.6773)  time: 0.5659  data: 0.0002  max mem: 6820\n[01:31:46.397101] Epoch: [26]  [520/969]  eta: 0:04:15  lr: 0.000272  loss: 0.6412 (0.6763)  time: 0.5663  data: 0.0002  max mem: 6820\n[01:31:57.734851] Epoch: [26]  [540/969]  eta: 0:04:03  lr: 0.000271  loss: 0.6499 (0.6753)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:32:09.054708] Epoch: [26]  [560/969]  eta: 0:03:52  lr: 0.000271  loss: 0.6680 (0.6750)  time: 0.5659  data: 0.0002  max mem: 6820\n[01:32:20.393461] Epoch: [26]  [580/969]  eta: 0:03:41  lr: 0.000271  loss: 0.6946 (0.6755)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:32:31.723192] Epoch: [26]  [600/969]  eta: 0:03:29  lr: 0.000271  loss: 0.6722 (0.6754)  time: 0.5664  data: 0.0001  max mem: 6820\n[01:32:43.053301] Epoch: [26]  [620/969]  eta: 0:03:18  lr: 0.000271  loss: 0.6670 (0.6753)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:32:54.410951] Epoch: [26]  [640/969]  eta: 0:03:06  lr: 0.000271  loss: 0.6672 (0.6751)  time: 0.5678  data: 0.0002  max mem: 6820\n[01:33:05.771458] Epoch: [26]  [660/969]  eta: 0:02:55  lr: 0.000271  loss: 0.6679 (0.6753)  time: 0.5680  data: 0.0002  max mem: 6820\n[01:33:17.122404] Epoch: [26]  [680/969]  eta: 0:02:44  lr: 0.000271  loss: 0.6891 (0.6758)  time: 0.5675  data: 0.0002  max mem: 6820\n[01:33:28.472051] Epoch: [26]  [700/969]  eta: 0:02:32  lr: 0.000271  loss: 0.6549 (0.6753)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:33:39.808494] Epoch: [26]  [720/969]  eta: 0:02:21  lr: 0.000271  loss: 0.6778 (0.6757)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:33:51.154688] Epoch: [26]  [740/969]  eta: 0:02:10  lr: 0.000270  loss: 0.6933 (0.6761)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:34:02.501749] Epoch: [26]  [760/969]  eta: 0:01:58  lr: 0.000270  loss: 0.6536 (0.6760)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:34:13.831205] Epoch: [26]  [780/969]  eta: 0:01:47  lr: 0.000270  loss: 0.6426 (0.6757)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:34:25.161632] Epoch: [26]  [800/969]  eta: 0:01:36  lr: 0.000270  loss: 0.6988 (0.6764)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:34:36.503124] Epoch: [26]  [820/969]  eta: 0:01:24  lr: 0.000270  loss: 0.6632 (0.6762)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:34:47.836522] Epoch: [26]  [840/969]  eta: 0:01:13  lr: 0.000270  loss: 0.6730 (0.6762)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:34:59.157827] Epoch: [26]  [860/969]  eta: 0:01:01  lr: 0.000270  loss: 0.6676 (0.6761)  time: 0.5660  data: 0.0002  max mem: 6820\n[01:35:10.504816] Epoch: [26]  [880/969]  eta: 0:00:50  lr: 0.000270  loss: 0.6626 (0.6763)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:35:21.847736] Epoch: [26]  [900/969]  eta: 0:00:39  lr: 0.000270  loss: 0.6821 (0.6762)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:35:33.208739] Epoch: [26]  [920/969]  eta: 0:00:27  lr: 0.000270  loss: 0.6721 (0.6760)  time: 0.5680  data: 0.0002  max mem: 6820\n[01:35:44.559156] Epoch: [26]  [940/969]  eta: 0:00:16  lr: 0.000269  loss: 0.6577 (0.6759)  time: 0.5675  data: 0.0002  max mem: 6820\n[01:35:55.962858] Epoch: [26]  [960/969]  eta: 0:00:05  lr: 0.000269  loss: 0.6810 (0.6762)  time: 0.5701  data: 0.0002  max mem: 6820\n[01:36:00.509026] Epoch: [26]  [968/969]  eta: 0:00:00  lr: 0.000269  loss: 0.6691 (0.6762)  time: 0.5693  data: 0.0002  max mem: 6820\n[01:36:00.629572] Epoch: [26] Total time: 0:09:10 (0.5681 s / it)\n[01:36:00.629684] Averaged stats: lr: 0.000269  loss: 0.6691 (0.6762)\n[01:36:01.437487] val:  [  0/139]  eta: 0:01:51  loss: 0.6738 (0.6738)  time: 0.8036  data: 0.6315  max mem: 6820\n[01:36:02.937397] val:  [ 10/139]  eta: 0:00:27  loss: 0.6427 (0.6122)  time: 0.2093  data: 0.0576  max mem: 6820\n[01:36:04.446507] val:  [ 20/139]  eta: 0:00:21  loss: 0.5704 (0.5961)  time: 0.1504  data: 0.0002  max mem: 6820\n[01:36:05.960783] val:  [ 30/139]  eta: 0:00:18  loss: 0.5693 (0.5963)  time: 0.1511  data: 0.0002  max mem: 6820\n[01:36:07.479328] val:  [ 40/139]  eta: 0:00:16  loss: 0.6600 (0.6178)  time: 0.1516  data: 0.0002  max mem: 6820\n[01:36:08.994229] val:  [ 50/139]  eta: 0:00:14  loss: 0.6812 (0.6273)  time: 0.1516  data: 0.0002  max mem: 6820\n[01:36:10.516840] val:  [ 60/139]  eta: 0:00:12  loss: 0.6730 (0.6318)  time: 0.1518  data: 0.0002  max mem: 6820\n[01:36:12.026473] val:  [ 70/139]  eta: 0:00:11  loss: 0.6578 (0.6367)  time: 0.1515  data: 0.0002  max mem: 6820\n[01:36:13.539348] val:  [ 80/139]  eta: 0:00:09  loss: 0.6557 (0.6409)  time: 0.1510  data: 0.0002  max mem: 6820\n[01:36:15.056444] val:  [ 90/139]  eta: 0:00:07  loss: 0.7209 (0.6514)  time: 0.1514  data: 0.0002  max mem: 6820\n[01:36:16.578337] val:  [100/139]  eta: 0:00:06  loss: 0.7307 (0.6583)  time: 0.1519  data: 0.0002  max mem: 6820\n[01:36:18.094988] val:  [110/139]  eta: 0:00:04  loss: 0.7226 (0.6607)  time: 0.1518  data: 0.0002  max mem: 6820\n[01:36:19.613183] val:  [120/139]  eta: 0:00:02  loss: 0.6253 (0.6568)  time: 0.1517  data: 0.0002  max mem: 6820\n[01:36:21.136142] val:  [130/139]  eta: 0:00:01  loss: 0.5880 (0.6466)  time: 0.1520  data: 0.0001  max mem: 6820\n[01:36:22.254964] val:  [138/139]  eta: 0:00:00  loss: 0.5078 (0.6396)  time: 0.1471  data: 0.0001  max mem: 6820\n[01:36:22.359123] val: Total time: 0:00:21 (0.1563 s / it)\n[01:36:22.416022] val loss: 0.6396214327366232\n[01:36:22.416078] Accuracy: 0.6347, F1 Score: 0.6347, ROC AUC: 0.6907, Hamming Loss: 0.3653,\n Jaccard Score: 0.4649, Precision: 0.6348, Recall: 0.6347,\n Average Precision: 0.6879, Kappa: 0.2694, Score: 0.5316\n[01:36:25.274845] Best epoch = 26, Best score = 0.5316\n[01:36:25.281654] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[01:36:26.762015] Epoch: [27]  [  0/969]  eta: 0:23:53  lr: 0.000269  loss: 0.6099 (0.6099)  time: 1.4790  data: 0.8134  max mem: 6820\n[01:36:38.065681] Epoch: [27]  [ 20/969]  eta: 0:09:37  lr: 0.000269  loss: 0.6630 (0.6620)  time: 0.5651  data: 0.0002  max mem: 6820\n[01:36:49.481588] Epoch: [27]  [ 40/969]  eta: 0:09:08  lr: 0.000269  loss: 0.6608 (0.6657)  time: 0.5707  data: 0.0002  max mem: 6820\n[01:37:00.977392] Epoch: [27]  [ 60/969]  eta: 0:08:51  lr: 0.000269  loss: 0.6778 (0.6694)  time: 0.5747  data: 0.0002  max mem: 6820\n[01:37:12.397271] Epoch: [27]  [ 80/969]  eta: 0:08:37  lr: 0.000269  loss: 0.6735 (0.6722)  time: 0.5709  data: 0.0002  max mem: 6820\n[01:37:23.750696] Epoch: [27]  [100/969]  eta: 0:08:23  lr: 0.000269  loss: 0.6878 (0.6743)  time: 0.5676  data: 0.0002  max mem: 6820\n[01:37:35.059851] Epoch: [27]  [120/969]  eta: 0:08:09  lr: 0.000269  loss: 0.6790 (0.6743)  time: 0.5654  data: 0.0002  max mem: 6820\n[01:37:46.368523] Epoch: [27]  [140/969]  eta: 0:07:56  lr: 0.000269  loss: 0.6816 (0.6750)  time: 0.5654  data: 0.0002  max mem: 6820\n[01:37:57.717213] Epoch: [27]  [160/969]  eta: 0:07:44  lr: 0.000269  loss: 0.6713 (0.6741)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:38:09.070944] Epoch: [27]  [180/969]  eta: 0:07:32  lr: 0.000268  loss: 0.6506 (0.6736)  time: 0.5676  data: 0.0002  max mem: 6820\n[01:38:20.472732] Epoch: [27]  [200/969]  eta: 0:07:20  lr: 0.000268  loss: 0.6646 (0.6746)  time: 0.5700  data: 0.0002  max mem: 6820\n[01:38:31.875040] Epoch: [27]  [220/969]  eta: 0:07:09  lr: 0.000268  loss: 0.6904 (0.6753)  time: 0.5701  data: 0.0002  max mem: 6820\n[01:38:43.289445] Epoch: [27]  [240/969]  eta: 0:06:57  lr: 0.000268  loss: 0.6829 (0.6756)  time: 0.5707  data: 0.0002  max mem: 6820\n[01:38:54.702304] Epoch: [27]  [260/969]  eta: 0:06:45  lr: 0.000268  loss: 0.6764 (0.6762)  time: 0.5706  data: 0.0002  max mem: 6820\n[01:39:06.082536] Epoch: [27]  [280/969]  eta: 0:06:34  lr: 0.000268  loss: 0.6619 (0.6764)  time: 0.5690  data: 0.0002  max mem: 6820\n[01:39:17.460312] Epoch: [27]  [300/969]  eta: 0:06:22  lr: 0.000268  loss: 0.6615 (0.6759)  time: 0.5688  data: 0.0002  max mem: 6820\n[01:39:28.833444] Epoch: [27]  [320/969]  eta: 0:06:11  lr: 0.000268  loss: 0.6533 (0.6736)  time: 0.5686  data: 0.0002  max mem: 6820\n[01:39:40.240029] Epoch: [27]  [340/969]  eta: 0:05:59  lr: 0.000268  loss: 0.6497 (0.6738)  time: 0.5703  data: 0.0003  max mem: 6820\n[01:39:51.623176] Epoch: [27]  [360/969]  eta: 0:05:48  lr: 0.000268  loss: 0.6893 (0.6755)  time: 0.5691  data: 0.0003  max mem: 6820\n[01:40:02.953112] Epoch: [27]  [380/969]  eta: 0:05:36  lr: 0.000267  loss: 0.6645 (0.6755)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:40:14.297092] Epoch: [27]  [400/969]  eta: 0:05:24  lr: 0.000267  loss: 0.6608 (0.6750)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:40:25.640802] Epoch: [27]  [420/969]  eta: 0:05:13  lr: 0.000267  loss: 0.6778 (0.6752)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:40:36.984324] Epoch: [27]  [440/969]  eta: 0:05:01  lr: 0.000267  loss: 0.6737 (0.6752)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:40:48.312699] Epoch: [27]  [460/969]  eta: 0:04:50  lr: 0.000267  loss: 0.6752 (0.6750)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:40:59.639260] Epoch: [27]  [480/969]  eta: 0:04:38  lr: 0.000267  loss: 0.6620 (0.6754)  time: 0.5663  data: 0.0002  max mem: 6820\n[01:41:10.973998] Epoch: [27]  [500/969]  eta: 0:04:27  lr: 0.000267  loss: 0.6622 (0.6752)  time: 0.5667  data: 0.0002  max mem: 6820\n[01:41:22.310400] Epoch: [27]  [520/969]  eta: 0:04:15  lr: 0.000267  loss: 0.6424 (0.6744)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:41:33.542740] Epoch: [27]  [540/969]  eta: 0:04:04  lr: 0.000267  loss: 0.6532 (0.6737)  time: 0.5616  data: 0.0002  max mem: 6820\n[01:41:44.891423] Epoch: [27]  [560/969]  eta: 0:03:52  lr: 0.000266  loss: 0.6252 (0.6727)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:41:56.219443] Epoch: [27]  [580/969]  eta: 0:03:41  lr: 0.000266  loss: 0.6331 (0.6721)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:42:07.540667] Epoch: [27]  [600/969]  eta: 0:03:30  lr: 0.000266  loss: 0.6865 (0.6728)  time: 0.5660  data: 0.0002  max mem: 6820\n[01:42:18.870644] Epoch: [27]  [620/969]  eta: 0:03:18  lr: 0.000266  loss: 0.6755 (0.6731)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:42:30.199867] Epoch: [27]  [640/969]  eta: 0:03:07  lr: 0.000266  loss: 0.6707 (0.6734)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:42:41.526873] Epoch: [27]  [660/969]  eta: 0:02:55  lr: 0.000266  loss: 0.6454 (0.6729)  time: 0.5663  data: 0.0002  max mem: 6820\n[01:42:52.847243] Epoch: [27]  [680/969]  eta: 0:02:44  lr: 0.000266  loss: 0.6771 (0.6731)  time: 0.5660  data: 0.0002  max mem: 6820\n[01:43:04.166666] Epoch: [27]  [700/969]  eta: 0:02:33  lr: 0.000266  loss: 0.6608 (0.6729)  time: 0.5659  data: 0.0002  max mem: 6820\n[01:43:15.508113] Epoch: [27]  [720/969]  eta: 0:02:21  lr: 0.000266  loss: 0.6768 (0.6730)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:43:26.853912] Epoch: [27]  [740/969]  eta: 0:02:10  lr: 0.000266  loss: 0.6703 (0.6725)  time: 0.5672  data: 0.0002  max mem: 6820\n[01:43:38.194237] Epoch: [27]  [760/969]  eta: 0:01:58  lr: 0.000265  loss: 0.6723 (0.6726)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:43:49.529302] Epoch: [27]  [780/969]  eta: 0:01:47  lr: 0.000265  loss: 0.6704 (0.6729)  time: 0.5667  data: 0.0002  max mem: 6820\n[01:44:00.872097] Epoch: [27]  [800/969]  eta: 0:01:36  lr: 0.000265  loss: 0.6839 (0.6735)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:44:12.214065] Epoch: [27]  [820/969]  eta: 0:01:24  lr: 0.000265  loss: 0.6506 (0.6732)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:44:23.575097] Epoch: [27]  [840/969]  eta: 0:01:13  lr: 0.000265  loss: 0.6705 (0.6730)  time: 0.5680  data: 0.0003  max mem: 6820\n[01:44:34.944550] Epoch: [27]  [860/969]  eta: 0:01:01  lr: 0.000265  loss: 0.6818 (0.6731)  time: 0.5684  data: 0.0002  max mem: 6820\n[01:44:46.284721] Epoch: [27]  [880/969]  eta: 0:00:50  lr: 0.000265  loss: 0.6514 (0.6728)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:44:57.601063] Epoch: [27]  [900/969]  eta: 0:00:39  lr: 0.000265  loss: 0.6557 (0.6727)  time: 0.5658  data: 0.0002  max mem: 6820\n[01:45:08.954182] Epoch: [27]  [920/969]  eta: 0:00:27  lr: 0.000265  loss: 0.6681 (0.6726)  time: 0.5676  data: 0.0002  max mem: 6820\n[01:45:20.304597] Epoch: [27]  [940/969]  eta: 0:00:16  lr: 0.000265  loss: 0.6676 (0.6726)  time: 0.5675  data: 0.0002  max mem: 6820\n[01:45:31.663123] Epoch: [27]  [960/969]  eta: 0:00:05  lr: 0.000264  loss: 0.6636 (0.6726)  time: 0.5679  data: 0.0002  max mem: 6820\n[01:45:36.220855] Epoch: [27]  [968/969]  eta: 0:00:00  lr: 0.000264  loss: 0.6653 (0.6727)  time: 0.5693  data: 0.0001  max mem: 6820\n[01:45:36.336335] Epoch: [27] Total time: 0:09:11 (0.5687 s / it)\n[01:45:36.336443] Averaged stats: lr: 0.000264  loss: 0.6653 (0.6727)\n[01:45:37.219524] val:  [  0/139]  eta: 0:02:02  loss: 0.6242 (0.6242)  time: 0.8788  data: 0.7352  max mem: 6820\n[01:45:38.724511] val:  [ 10/139]  eta: 0:00:27  loss: 0.5762 (0.5547)  time: 0.2166  data: 0.0670  max mem: 6820\n[01:45:40.233686] val:  [ 20/139]  eta: 0:00:22  loss: 0.5231 (0.5482)  time: 0.1506  data: 0.0002  max mem: 6820\n[01:45:41.739392] val:  [ 30/139]  eta: 0:00:18  loss: 0.5132 (0.5433)  time: 0.1507  data: 0.0002  max mem: 6820\n[01:45:43.248988] val:  [ 40/139]  eta: 0:00:16  loss: 0.5729 (0.5570)  time: 0.1507  data: 0.0002  max mem: 6820\n[01:45:44.763310] val:  [ 50/139]  eta: 0:00:14  loss: 0.5783 (0.5624)  time: 0.1511  data: 0.0002  max mem: 6820\n[01:45:46.280526] val:  [ 60/139]  eta: 0:00:12  loss: 0.5783 (0.5654)  time: 0.1515  data: 0.0002  max mem: 6820\n[01:45:47.790301] val:  [ 70/139]  eta: 0:00:11  loss: 0.6008 (0.5741)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:45:49.300953] val:  [ 80/139]  eta: 0:00:09  loss: 0.6409 (0.5930)  time: 0.1509  data: 0.0002  max mem: 6820\n[01:45:50.823516] val:  [ 90/139]  eta: 0:00:07  loss: 0.7958 (0.6172)  time: 0.1516  data: 0.0002  max mem: 6820\n[01:45:52.344462] val:  [100/139]  eta: 0:00:06  loss: 0.8056 (0.6358)  time: 0.1521  data: 0.0002  max mem: 6820\n[01:45:53.854168] val:  [110/139]  eta: 0:00:04  loss: 0.8056 (0.6473)  time: 0.1515  data: 0.0002  max mem: 6820\n[01:45:55.368678] val:  [120/139]  eta: 0:00:02  loss: 0.6892 (0.6489)  time: 0.1511  data: 0.0002  max mem: 6820\n[01:45:56.879513] val:  [130/139]  eta: 0:00:01  loss: 0.6263 (0.6430)  time: 0.1512  data: 0.0001  max mem: 6820\n[01:45:57.998729] val:  [138/139]  eta: 0:00:00  loss: 0.5968 (0.6410)  time: 0.1465  data: 0.0001  max mem: 6820\n[01:45:58.108698] val: Total time: 0:00:21 (0.1566 s / it)\n[01:45:58.161748] val loss: 0.6409812762582903\n[01:45:58.161796] Accuracy: 0.6261, F1 Score: 0.6235, ROC AUC: 0.6926, Hamming Loss: 0.3739,\n Jaccard Score: 0.4537, Precision: 0.6298, Recall: 0.6261,\n Average Precision: 0.6907, Kappa: 0.2523, Score: 0.5228\n[01:45:58.163660] Best epoch = 26, Best score = 0.5316\n[01:45:58.166142] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[01:45:59.589018] Epoch: [28]  [  0/969]  eta: 0:22:57  lr: 0.000264  loss: 0.5796 (0.5796)  time: 1.4216  data: 0.8334  max mem: 6820\n[01:46:10.928518] Epoch: [28]  [ 20/969]  eta: 0:09:36  lr: 0.000264  loss: 0.6441 (0.6478)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:46:22.331672] Epoch: [28]  [ 40/969]  eta: 0:09:07  lr: 0.000264  loss: 0.6809 (0.6603)  time: 0.5701  data: 0.0002  max mem: 6820\n[01:46:33.770478] Epoch: [28]  [ 60/969]  eta: 0:08:50  lr: 0.000264  loss: 0.6901 (0.6702)  time: 0.5719  data: 0.0002  max mem: 6820\n[01:46:45.202401] Epoch: [28]  [ 80/969]  eta: 0:08:36  lr: 0.000264  loss: 0.6707 (0.6681)  time: 0.5715  data: 0.0002  max mem: 6820\n[01:46:56.619479] Epoch: [28]  [100/969]  eta: 0:08:22  lr: 0.000264  loss: 0.6654 (0.6678)  time: 0.5708  data: 0.0002  max mem: 6820\n[01:47:08.025471] Epoch: [28]  [120/969]  eta: 0:08:10  lr: 0.000264  loss: 0.6886 (0.6710)  time: 0.5702  data: 0.0002  max mem: 6820\n[01:47:19.403852] Epoch: [28]  [140/969]  eta: 0:07:57  lr: 0.000264  loss: 0.6764 (0.6715)  time: 0.5689  data: 0.0002  max mem: 6820\n[01:47:30.754915] Epoch: [28]  [160/969]  eta: 0:07:45  lr: 0.000264  loss: 0.6545 (0.6699)  time: 0.5675  data: 0.0002  max mem: 6820\n[01:47:42.110901] Epoch: [28]  [180/969]  eta: 0:07:33  lr: 0.000263  loss: 0.7004 (0.6734)  time: 0.5677  data: 0.0002  max mem: 6820\n[01:47:53.414837] Epoch: [28]  [200/969]  eta: 0:07:20  lr: 0.000263  loss: 0.6946 (0.6751)  time: 0.5651  data: 0.0002  max mem: 6820\n[01:48:04.730873] Epoch: [28]  [220/969]  eta: 0:07:08  lr: 0.000263  loss: 0.6809 (0.6750)  time: 0.5658  data: 0.0002  max mem: 6820\n[01:48:16.048524] Epoch: [28]  [240/969]  eta: 0:06:57  lr: 0.000263  loss: 0.6872 (0.6756)  time: 0.5658  data: 0.0002  max mem: 6820\n[01:48:27.378179] Epoch: [28]  [260/969]  eta: 0:06:45  lr: 0.000263  loss: 0.6667 (0.6752)  time: 0.5664  data: 0.0002  max mem: 6820\n[01:48:38.753174] Epoch: [28]  [280/969]  eta: 0:06:33  lr: 0.000263  loss: 0.6701 (0.6756)  time: 0.5687  data: 0.0002  max mem: 6820\n[01:48:50.161913] Epoch: [28]  [300/969]  eta: 0:06:22  lr: 0.000263  loss: 0.6600 (0.6749)  time: 0.5704  data: 0.0001  max mem: 6820\n[01:49:01.600684] Epoch: [28]  [320/969]  eta: 0:06:10  lr: 0.000263  loss: 0.6370 (0.6731)  time: 0.5719  data: 0.0002  max mem: 6820\n[01:49:13.035109] Epoch: [28]  [340/969]  eta: 0:05:59  lr: 0.000263  loss: 0.6638 (0.6739)  time: 0.5717  data: 0.0002  max mem: 6820\n[01:49:24.438672] Epoch: [28]  [360/969]  eta: 0:05:47  lr: 0.000262  loss: 0.6789 (0.6739)  time: 0.5701  data: 0.0002  max mem: 6820\n[01:49:35.856999] Epoch: [28]  [380/969]  eta: 0:05:36  lr: 0.000262  loss: 0.6756 (0.6744)  time: 0.5709  data: 0.0002  max mem: 6820\n[01:49:47.247616] Epoch: [28]  [400/969]  eta: 0:05:25  lr: 0.000262  loss: 0.6667 (0.6740)  time: 0.5695  data: 0.0002  max mem: 6820\n[01:49:58.619408] Epoch: [28]  [420/969]  eta: 0:05:13  lr: 0.000262  loss: 0.6659 (0.6732)  time: 0.5685  data: 0.0002  max mem: 6820\n[01:50:09.989422] Epoch: [28]  [440/969]  eta: 0:05:02  lr: 0.000262  loss: 0.6746 (0.6739)  time: 0.5685  data: 0.0002  max mem: 6820\n[01:50:21.341537] Epoch: [28]  [460/969]  eta: 0:04:50  lr: 0.000262  loss: 0.6649 (0.6732)  time: 0.5676  data: 0.0002  max mem: 6820\n[01:50:32.698693] Epoch: [28]  [480/969]  eta: 0:04:39  lr: 0.000262  loss: 0.6603 (0.6730)  time: 0.5678  data: 0.0002  max mem: 6820\n[01:50:44.057097] Epoch: [28]  [500/969]  eta: 0:04:27  lr: 0.000262  loss: 0.6598 (0.6724)  time: 0.5679  data: 0.0002  max mem: 6820\n[01:50:55.442338] Epoch: [28]  [520/969]  eta: 0:04:16  lr: 0.000262  loss: 0.6613 (0.6715)  time: 0.5692  data: 0.0002  max mem: 6820\n[01:51:06.826784] Epoch: [28]  [540/969]  eta: 0:04:04  lr: 0.000262  loss: 0.6807 (0.6716)  time: 0.5692  data: 0.0002  max mem: 6820\n[01:51:18.193154] Epoch: [28]  [560/969]  eta: 0:03:53  lr: 0.000261  loss: 0.6659 (0.6714)  time: 0.5683  data: 0.0002  max mem: 6820\n[01:51:29.542878] Epoch: [28]  [580/969]  eta: 0:03:41  lr: 0.000261  loss: 0.6704 (0.6716)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:51:40.893916] Epoch: [28]  [600/969]  eta: 0:03:30  lr: 0.000261  loss: 0.6785 (0.6720)  time: 0.5675  data: 0.0002  max mem: 6820\n[01:51:52.248049] Epoch: [28]  [620/969]  eta: 0:03:18  lr: 0.000261  loss: 0.6674 (0.6722)  time: 0.5677  data: 0.0002  max mem: 6820\n[01:52:03.604369] Epoch: [28]  [640/969]  eta: 0:03:07  lr: 0.000261  loss: 0.6746 (0.6721)  time: 0.5678  data: 0.0002  max mem: 6820\n[01:52:14.951315] Epoch: [28]  [660/969]  eta: 0:02:56  lr: 0.000261  loss: 0.6717 (0.6723)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:52:26.281517] Epoch: [28]  [680/969]  eta: 0:02:44  lr: 0.000261  loss: 0.6863 (0.6730)  time: 0.5665  data: 0.0002  max mem: 6820\n[01:52:37.616145] Epoch: [28]  [700/969]  eta: 0:02:33  lr: 0.000261  loss: 0.6581 (0.6725)  time: 0.5667  data: 0.0002  max mem: 6820\n[01:52:48.957190] Epoch: [28]  [720/969]  eta: 0:02:21  lr: 0.000261  loss: 0.6959 (0.6730)  time: 0.5670  data: 0.0002  max mem: 6820\n[01:53:00.304449] Epoch: [28]  [740/969]  eta: 0:02:10  lr: 0.000260  loss: 0.6513 (0.6727)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:53:11.669732] Epoch: [28]  [760/969]  eta: 0:01:59  lr: 0.000260  loss: 0.6731 (0.6730)  time: 0.5682  data: 0.0002  max mem: 6820\n[01:53:23.012076] Epoch: [28]  [780/969]  eta: 0:01:47  lr: 0.000260  loss: 0.6667 (0.6731)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:53:34.346766] Epoch: [28]  [800/969]  eta: 0:01:36  lr: 0.000260  loss: 0.6759 (0.6732)  time: 0.5667  data: 0.0002  max mem: 6820\n[01:53:45.679653] Epoch: [28]  [820/969]  eta: 0:01:24  lr: 0.000260  loss: 0.6584 (0.6732)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:53:57.039295] Epoch: [28]  [840/969]  eta: 0:01:13  lr: 0.000260  loss: 0.6494 (0.6728)  time: 0.5679  data: 0.0001  max mem: 6820\n[01:54:08.404525] Epoch: [28]  [860/969]  eta: 0:01:02  lr: 0.000260  loss: 0.6294 (0.6726)  time: 0.5682  data: 0.0001  max mem: 6820\n[01:54:19.749541] Epoch: [28]  [880/969]  eta: 0:00:50  lr: 0.000260  loss: 0.6419 (0.6721)  time: 0.5672  data: 0.0002  max mem: 6820\n[01:54:31.119349] Epoch: [28]  [900/969]  eta: 0:00:39  lr: 0.000260  loss: 0.6584 (0.6721)  time: 0.5684  data: 0.0002  max mem: 6820\n[01:54:42.456955] Epoch: [28]  [920/969]  eta: 0:00:27  lr: 0.000259  loss: 0.6506 (0.6718)  time: 0.5668  data: 0.0002  max mem: 6820\n[01:54:53.802656] Epoch: [28]  [940/969]  eta: 0:00:16  lr: 0.000259  loss: 0.6650 (0.6717)  time: 0.5672  data: 0.0002  max mem: 6820\n[01:55:05.149634] Epoch: [28]  [960/969]  eta: 0:00:05  lr: 0.000259  loss: 0.6376 (0.6714)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:55:09.691077] Epoch: [28]  [968/969]  eta: 0:00:00  lr: 0.000259  loss: 0.6518 (0.6716)  time: 0.5671  data: 0.0001  max mem: 6820\n[01:55:09.812648] Epoch: [28] Total time: 0:09:11 (0.5693 s / it)\n[01:55:09.812756] Averaged stats: lr: 0.000259  loss: 0.6518 (0.6716)\n[01:55:10.589011] val:  [  0/139]  eta: 0:01:46  loss: 0.6265 (0.6265)  time: 0.7683  data: 0.6300  max mem: 6820\n[01:55:12.087376] val:  [ 10/139]  eta: 0:00:26  loss: 0.5977 (0.5555)  time: 0.2055  data: 0.0577  max mem: 6820\n[01:55:13.582370] val:  [ 20/139]  eta: 0:00:21  loss: 0.5177 (0.5402)  time: 0.1493  data: 0.0003  max mem: 6820\n[01:55:15.094529] val:  [ 30/139]  eta: 0:00:18  loss: 0.5015 (0.5444)  time: 0.1503  data: 0.0002  max mem: 6820\n[01:55:16.604830] val:  [ 40/139]  eta: 0:00:16  loss: 0.6234 (0.5695)  time: 0.1510  data: 0.0002  max mem: 6820\n[01:55:18.107769] val:  [ 50/139]  eta: 0:00:14  loss: 0.6378 (0.5823)  time: 0.1506  data: 0.0002  max mem: 6820\n[01:55:19.625201] val:  [ 60/139]  eta: 0:00:12  loss: 0.6327 (0.5936)  time: 0.1509  data: 0.0002  max mem: 6820\n[01:55:21.131293] val:  [ 70/139]  eta: 0:00:10  loss: 0.6744 (0.6047)  time: 0.1511  data: 0.0002  max mem: 6820\n[01:55:22.644492] val:  [ 80/139]  eta: 0:00:09  loss: 0.6744 (0.6173)  time: 0.1509  data: 0.0002  max mem: 6820\n[01:55:24.158235] val:  [ 90/139]  eta: 0:00:07  loss: 0.7148 (0.6298)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:55:25.673451] val:  [100/139]  eta: 0:00:06  loss: 0.7391 (0.6409)  time: 0.1514  data: 0.0002  max mem: 6820\n[01:55:27.185656] val:  [110/139]  eta: 0:00:04  loss: 0.7322 (0.6468)  time: 0.1513  data: 0.0002  max mem: 6820\n[01:55:28.695991] val:  [120/139]  eta: 0:00:02  loss: 0.6600 (0.6453)  time: 0.1510  data: 0.0002  max mem: 6820\n[01:55:30.212846] val:  [130/139]  eta: 0:00:01  loss: 0.5965 (0.6381)  time: 0.1513  data: 0.0001  max mem: 6820\n[01:55:31.326463] val:  [138/139]  eta: 0:00:00  loss: 0.5380 (0.6328)  time: 0.1465  data: 0.0001  max mem: 6820\n[01:55:31.429642] val: Total time: 0:00:21 (0.1555 s / it)\n[01:55:31.483159] val loss: 0.6327532469797478\n[01:55:31.483208] Accuracy: 0.6383, F1 Score: 0.6382, ROC AUC: 0.6983, Hamming Loss: 0.3617,\n Jaccard Score: 0.4687, Precision: 0.6385, Recall: 0.6383,\n Average Precision: 0.6986, Kappa: 0.2767, Score: 0.5377\n[01:55:34.326427] Best epoch = 28, Best score = 0.5377\n[01:55:34.329220] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[01:55:35.842469] Epoch: [29]  [  0/969]  eta: 0:24:25  lr: 0.000259  loss: 0.6374 (0.6374)  time: 1.5119  data: 0.9386  max mem: 6820\n[01:55:47.103961] Epoch: [29]  [ 20/969]  eta: 0:09:37  lr: 0.000259  loss: 0.6630 (0.6623)  time: 0.5630  data: 0.0002  max mem: 6820\n[01:55:58.577673] Epoch: [29]  [ 40/969]  eta: 0:09:09  lr: 0.000259  loss: 0.6855 (0.6705)  time: 0.5736  data: 0.0002  max mem: 6820\n[01:56:10.172544] Epoch: [29]  [ 60/969]  eta: 0:08:54  lr: 0.000259  loss: 0.6752 (0.6728)  time: 0.5797  data: 0.0002  max mem: 6820\n[01:56:21.661503] Epoch: [29]  [ 80/969]  eta: 0:08:39  lr: 0.000259  loss: 0.6756 (0.6713)  time: 0.5744  data: 0.0002  max mem: 6820\n[01:56:33.039936] Epoch: [29]  [100/969]  eta: 0:08:25  lr: 0.000259  loss: 0.6628 (0.6712)  time: 0.5689  data: 0.0002  max mem: 6820\n[01:56:44.364371] Epoch: [29]  [120/969]  eta: 0:08:11  lr: 0.000259  loss: 0.6868 (0.6723)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:56:55.690293] Epoch: [29]  [140/969]  eta: 0:07:58  lr: 0.000258  loss: 0.6509 (0.6709)  time: 0.5662  data: 0.0002  max mem: 6820\n[01:57:07.023120] Epoch: [29]  [160/969]  eta: 0:07:45  lr: 0.000258  loss: 0.6552 (0.6707)  time: 0.5666  data: 0.0002  max mem: 6820\n[01:57:18.401959] Epoch: [29]  [180/969]  eta: 0:07:33  lr: 0.000258  loss: 0.6637 (0.6699)  time: 0.5689  data: 0.0002  max mem: 6820\n[01:57:29.799158] Epoch: [29]  [200/969]  eta: 0:07:21  lr: 0.000258  loss: 0.6550 (0.6701)  time: 0.5698  data: 0.0002  max mem: 6820\n[01:57:41.217139] Epoch: [29]  [220/969]  eta: 0:07:10  lr: 0.000258  loss: 0.6769 (0.6714)  time: 0.5709  data: 0.0002  max mem: 6820\n[01:57:52.627098] Epoch: [29]  [240/969]  eta: 0:06:58  lr: 0.000258  loss: 0.6777 (0.6717)  time: 0.5705  data: 0.0002  max mem: 6820\n[01:58:04.018727] Epoch: [29]  [260/969]  eta: 0:06:46  lr: 0.000258  loss: 0.6657 (0.6716)  time: 0.5695  data: 0.0002  max mem: 6820\n[01:58:15.365571] Epoch: [29]  [280/969]  eta: 0:06:34  lr: 0.000258  loss: 0.6772 (0.6723)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:58:26.709499] Epoch: [29]  [300/969]  eta: 0:06:23  lr: 0.000258  loss: 0.6620 (0.6726)  time: 0.5671  data: 0.0002  max mem: 6820\n[01:58:38.048444] Epoch: [29]  [320/969]  eta: 0:06:11  lr: 0.000257  loss: 0.6708 (0.6718)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:58:49.371107] Epoch: [29]  [340/969]  eta: 0:05:59  lr: 0.000257  loss: 0.6627 (0.6714)  time: 0.5661  data: 0.0002  max mem: 6820\n[01:59:00.711280] Epoch: [29]  [360/969]  eta: 0:05:48  lr: 0.000257  loss: 0.6894 (0.6733)  time: 0.5669  data: 0.0002  max mem: 6820\n[01:59:12.066321] Epoch: [29]  [380/969]  eta: 0:05:36  lr: 0.000257  loss: 0.6912 (0.6734)  time: 0.5677  data: 0.0002  max mem: 6820\n[01:59:23.414335] Epoch: [29]  [400/969]  eta: 0:05:25  lr: 0.000257  loss: 0.6793 (0.6735)  time: 0.5674  data: 0.0002  max mem: 6820\n[01:59:34.762259] Epoch: [29]  [420/969]  eta: 0:05:13  lr: 0.000257  loss: 0.6469 (0.6727)  time: 0.5673  data: 0.0002  max mem: 6820\n[01:59:46.146541] Epoch: [29]  [440/969]  eta: 0:05:02  lr: 0.000257  loss: 0.6438 (0.6717)  time: 0.5692  data: 0.0002  max mem: 6820\n[01:59:57.534097] Epoch: [29]  [460/969]  eta: 0:04:50  lr: 0.000257  loss: 0.6463 (0.6719)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:00:08.956331] Epoch: [29]  [480/969]  eta: 0:04:39  lr: 0.000257  loss: 0.6716 (0.6723)  time: 0.5711  data: 0.0002  max mem: 6820\n[02:00:20.348085] Epoch: [29]  [500/969]  eta: 0:04:27  lr: 0.000256  loss: 0.6575 (0.6721)  time: 0.5695  data: 0.0001  max mem: 6820\n[02:00:31.755062] Epoch: [29]  [520/969]  eta: 0:04:16  lr: 0.000256  loss: 0.6518 (0.6713)  time: 0.5703  data: 0.0002  max mem: 6820\n[02:00:43.198709] Epoch: [29]  [540/969]  eta: 0:04:04  lr: 0.000256  loss: 0.6431 (0.6711)  time: 0.5721  data: 0.0002  max mem: 6820\n[02:00:54.629411] Epoch: [29]  [560/969]  eta: 0:03:53  lr: 0.000256  loss: 0.6275 (0.6703)  time: 0.5715  data: 0.0002  max mem: 6820\n[02:01:06.058626] Epoch: [29]  [580/969]  eta: 0:03:42  lr: 0.000256  loss: 0.6632 (0.6700)  time: 0.5714  data: 0.0002  max mem: 6820\n[02:01:17.383905] Epoch: [29]  [600/969]  eta: 0:03:30  lr: 0.000256  loss: 0.6691 (0.6706)  time: 0.5662  data: 0.0002  max mem: 6820\n[02:01:28.783435] Epoch: [29]  [620/969]  eta: 0:03:19  lr: 0.000256  loss: 0.6613 (0.6705)  time: 0.5699  data: 0.0002  max mem: 6820\n[02:01:40.165154] Epoch: [29]  [640/969]  eta: 0:03:07  lr: 0.000256  loss: 0.6555 (0.6703)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:01:51.530633] Epoch: [29]  [660/969]  eta: 0:02:56  lr: 0.000256  loss: 0.6578 (0.6699)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:02:02.911229] Epoch: [29]  [680/969]  eta: 0:02:44  lr: 0.000255  loss: 0.6476 (0.6696)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:02:14.296326] Epoch: [29]  [700/969]  eta: 0:02:33  lr: 0.000255  loss: 0.6443 (0.6691)  time: 0.5692  data: 0.0002  max mem: 6820\n[02:02:25.661128] Epoch: [29]  [720/969]  eta: 0:02:22  lr: 0.000255  loss: 0.6732 (0.6697)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:02:36.998651] Epoch: [29]  [740/969]  eta: 0:02:10  lr: 0.000255  loss: 0.6656 (0.6694)  time: 0.5668  data: 0.0002  max mem: 6820\n[02:02:48.346572] Epoch: [29]  [760/969]  eta: 0:01:59  lr: 0.000255  loss: 0.6810 (0.6692)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:02:59.678001] Epoch: [29]  [780/969]  eta: 0:01:47  lr: 0.000255  loss: 0.6710 (0.6692)  time: 0.5665  data: 0.0002  max mem: 6820\n[02:03:11.014787] Epoch: [29]  [800/969]  eta: 0:01:36  lr: 0.000255  loss: 0.6636 (0.6693)  time: 0.5668  data: 0.0002  max mem: 6820\n[02:03:22.346212] Epoch: [29]  [820/969]  eta: 0:01:24  lr: 0.000255  loss: 0.6659 (0.6689)  time: 0.5665  data: 0.0002  max mem: 6820\n[02:03:33.679898] Epoch: [29]  [840/969]  eta: 0:01:13  lr: 0.000255  loss: 0.6546 (0.6683)  time: 0.5666  data: 0.0002  max mem: 6820\n[02:03:45.064670] Epoch: [29]  [860/969]  eta: 0:01:02  lr: 0.000254  loss: 0.6745 (0.6683)  time: 0.5692  data: 0.0001  max mem: 6820\n[02:03:56.446847] Epoch: [29]  [880/969]  eta: 0:00:50  lr: 0.000254  loss: 0.6400 (0.6679)  time: 0.5691  data: 0.0002  max mem: 6820\n[02:04:07.826903] Epoch: [29]  [900/969]  eta: 0:00:39  lr: 0.000254  loss: 0.6417 (0.6679)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:04:19.214495] Epoch: [29]  [920/969]  eta: 0:00:27  lr: 0.000254  loss: 0.6654 (0.6677)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:04:30.594931] Epoch: [29]  [940/969]  eta: 0:00:16  lr: 0.000254  loss: 0.6803 (0.6678)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:04:41.967414] Epoch: [29]  [960/969]  eta: 0:00:05  lr: 0.000254  loss: 0.6685 (0.6679)  time: 0.5686  data: 0.0002  max mem: 6820\n[02:04:46.518330] Epoch: [29]  [968/969]  eta: 0:00:00  lr: 0.000254  loss: 0.6588 (0.6679)  time: 0.5688  data: 0.0001  max mem: 6820\n[02:04:46.637719] Epoch: [29] Total time: 0:09:12 (0.5700 s / it)\n[02:04:46.637831] Averaged stats: lr: 0.000254  loss: 0.6588 (0.6679)\n[02:04:47.477523] val:  [  0/139]  eta: 0:01:56  loss: 0.6233 (0.6233)  time: 0.8351  data: 0.6355  max mem: 6820\n[02:04:48.993113] val:  [ 10/139]  eta: 0:00:27  loss: 0.6101 (0.5832)  time: 0.2131  data: 0.0593  max mem: 6820\n[02:04:50.496834] val:  [ 20/139]  eta: 0:00:21  loss: 0.5675 (0.5788)  time: 0.1506  data: 0.0009  max mem: 6820\n[02:04:52.000432] val:  [ 30/139]  eta: 0:00:18  loss: 0.5630 (0.5781)  time: 0.1503  data: 0.0002  max mem: 6820\n[02:04:53.506589] val:  [ 40/139]  eta: 0:00:16  loss: 0.6196 (0.5945)  time: 0.1504  data: 0.0002  max mem: 6820\n[02:04:55.015878] val:  [ 50/139]  eta: 0:00:14  loss: 0.6196 (0.5988)  time: 0.1507  data: 0.0002  max mem: 6820\n[02:04:56.526443] val:  [ 60/139]  eta: 0:00:12  loss: 0.6115 (0.6036)  time: 0.1509  data: 0.0002  max mem: 6820\n[02:04:58.035526] val:  [ 70/139]  eta: 0:00:11  loss: 0.6318 (0.6097)  time: 0.1509  data: 0.0002  max mem: 6820\n[02:04:59.547116] val:  [ 80/139]  eta: 0:00:09  loss: 0.6497 (0.6183)  time: 0.1510  data: 0.0002  max mem: 6820\n[02:05:01.061649] val:  [ 90/139]  eta: 0:00:07  loss: 0.7288 (0.6326)  time: 0.1512  data: 0.0002  max mem: 6820\n[02:05:02.576236] val:  [100/139]  eta: 0:00:06  loss: 0.7539 (0.6437)  time: 0.1514  data: 0.0002  max mem: 6820\n[02:05:04.086453] val:  [110/139]  eta: 0:00:04  loss: 0.7532 (0.6516)  time: 0.1512  data: 0.0002  max mem: 6820\n[02:05:05.598170] val:  [120/139]  eta: 0:00:02  loss: 0.6401 (0.6497)  time: 0.1510  data: 0.0002  max mem: 6820\n[02:05:07.113226] val:  [130/139]  eta: 0:00:01  loss: 0.6093 (0.6406)  time: 0.1513  data: 0.0001  max mem: 6820\n[02:05:08.230771] val:  [138/139]  eta: 0:00:00  loss: 0.5352 (0.6337)  time: 0.1467  data: 0.0001  max mem: 6820\n[02:05:08.333318] val: Total time: 0:00:21 (0.1561 s / it)\n[02:05:08.387702] val loss: 0.6337081030976001\n[02:05:08.387764] Accuracy: 0.6392, F1 Score: 0.6381, ROC AUC: 0.6960, Hamming Loss: 0.3608,\n Jaccard Score: 0.4688, Precision: 0.6411, Recall: 0.6392,\n Average Precision: 0.6924, Kappa: 0.2785, Score: 0.5375\n[02:05:08.389650] Best epoch = 28, Best score = 0.5377\n[02:05:08.396962] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[02:05:09.874349] Epoch: [30]  [  0/969]  eta: 0:23:50  lr: 0.000254  loss: 0.6295 (0.6295)  time: 1.4761  data: 0.8987  max mem: 6820\n[02:05:21.203607] Epoch: [30]  [ 20/969]  eta: 0:09:38  lr: 0.000254  loss: 0.6505 (0.6635)  time: 0.5664  data: 0.0002  max mem: 6820\n[02:05:32.550385] Epoch: [30]  [ 40/969]  eta: 0:09:07  lr: 0.000254  loss: 0.6697 (0.6625)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:05:43.933624] Epoch: [30]  [ 60/969]  eta: 0:08:49  lr: 0.000254  loss: 0.6501 (0.6610)  time: 0.5691  data: 0.0002  max mem: 6820\n[02:05:55.347570] Epoch: [30]  [ 80/969]  eta: 0:08:35  lr: 0.000253  loss: 0.6649 (0.6625)  time: 0.5706  data: 0.0002  max mem: 6820\n[02:06:06.721053] Epoch: [30]  [100/969]  eta: 0:08:21  lr: 0.000253  loss: 0.6783 (0.6646)  time: 0.5686  data: 0.0002  max mem: 6820\n[02:06:18.100636] Epoch: [30]  [120/969]  eta: 0:08:09  lr: 0.000253  loss: 0.6690 (0.6679)  time: 0.5689  data: 0.0002  max mem: 6820\n[02:06:29.480340] Epoch: [30]  [140/969]  eta: 0:07:56  lr: 0.000253  loss: 0.6571 (0.6677)  time: 0.5689  data: 0.0002  max mem: 6820\n[02:06:40.845440] Epoch: [30]  [160/969]  eta: 0:07:44  lr: 0.000253  loss: 0.6640 (0.6675)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:06:52.198185] Epoch: [30]  [180/969]  eta: 0:07:32  lr: 0.000253  loss: 0.6696 (0.6682)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:07:03.549906] Epoch: [30]  [200/969]  eta: 0:07:20  lr: 0.000253  loss: 0.6799 (0.6699)  time: 0.5675  data: 0.0002  max mem: 6820\n[02:07:14.882703] Epoch: [30]  [220/969]  eta: 0:07:08  lr: 0.000253  loss: 0.6940 (0.6716)  time: 0.5666  data: 0.0002  max mem: 6820\n[02:07:26.220015] Epoch: [30]  [240/969]  eta: 0:06:56  lr: 0.000252  loss: 0.6756 (0.6719)  time: 0.5668  data: 0.0002  max mem: 6820\n[02:07:37.540219] Epoch: [30]  [260/969]  eta: 0:06:45  lr: 0.000252  loss: 0.6635 (0.6718)  time: 0.5660  data: 0.0002  max mem: 6820\n[02:07:48.868118] Epoch: [30]  [280/969]  eta: 0:06:33  lr: 0.000252  loss: 0.6700 (0.6721)  time: 0.5663  data: 0.0002  max mem: 6820\n[02:08:00.200609] Epoch: [30]  [300/969]  eta: 0:06:21  lr: 0.000252  loss: 0.6665 (0.6720)  time: 0.5666  data: 0.0002  max mem: 6820\n[02:08:11.536209] Epoch: [30]  [320/969]  eta: 0:06:10  lr: 0.000252  loss: 0.6431 (0.6702)  time: 0.5667  data: 0.0002  max mem: 6820\n[02:08:22.872103] Epoch: [30]  [340/969]  eta: 0:05:58  lr: 0.000252  loss: 0.6466 (0.6704)  time: 0.5667  data: 0.0002  max mem: 6820\n[02:08:34.238921] Epoch: [30]  [360/969]  eta: 0:05:47  lr: 0.000252  loss: 0.6628 (0.6697)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:08:45.592844] Epoch: [30]  [380/969]  eta: 0:05:35  lr: 0.000252  loss: 0.6570 (0.6699)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:08:56.950618] Epoch: [30]  [400/969]  eta: 0:05:24  lr: 0.000252  loss: 0.6559 (0.6700)  time: 0.5678  data: 0.0002  max mem: 6820\n[02:09:08.294004] Epoch: [30]  [420/969]  eta: 0:05:12  lr: 0.000251  loss: 0.6433 (0.6694)  time: 0.5671  data: 0.0002  max mem: 6820\n[02:09:19.658428] Epoch: [30]  [440/969]  eta: 0:05:01  lr: 0.000251  loss: 0.6774 (0.6686)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:09:31.023512] Epoch: [30]  [460/969]  eta: 0:04:49  lr: 0.000251  loss: 0.6366 (0.6679)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:09:42.371821] Epoch: [30]  [480/969]  eta: 0:04:38  lr: 0.000251  loss: 0.6508 (0.6678)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:09:53.712100] Epoch: [30]  [500/969]  eta: 0:04:27  lr: 0.000251  loss: 0.6718 (0.6680)  time: 0.5670  data: 0.0002  max mem: 6820\n[02:10:05.068428] Epoch: [30]  [520/969]  eta: 0:04:15  lr: 0.000251  loss: 0.6557 (0.6675)  time: 0.5678  data: 0.0002  max mem: 6820\n[02:10:16.397771] Epoch: [30]  [540/969]  eta: 0:04:04  lr: 0.000251  loss: 0.6449 (0.6672)  time: 0.5664  data: 0.0002  max mem: 6820\n[02:10:27.730795] Epoch: [30]  [560/969]  eta: 0:03:52  lr: 0.000251  loss: 0.6664 (0.6679)  time: 0.5666  data: 0.0002  max mem: 6820\n[02:10:39.070854] Epoch: [30]  [580/969]  eta: 0:03:41  lr: 0.000251  loss: 0.6793 (0.6689)  time: 0.5670  data: 0.0002  max mem: 6820\n[02:10:50.400053] Epoch: [30]  [600/969]  eta: 0:03:29  lr: 0.000250  loss: 0.6736 (0.6690)  time: 0.5664  data: 0.0002  max mem: 6820\n[02:11:01.726691] Epoch: [30]  [620/969]  eta: 0:03:18  lr: 0.000250  loss: 0.6674 (0.6693)  time: 0.5663  data: 0.0002  max mem: 6820\n[02:11:13.066590] Epoch: [30]  [640/969]  eta: 0:03:07  lr: 0.000250  loss: 0.6733 (0.6695)  time: 0.5669  data: 0.0002  max mem: 6820\n[02:11:24.414190] Epoch: [30]  [660/969]  eta: 0:02:55  lr: 0.000250  loss: 0.6723 (0.6697)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:11:35.771353] Epoch: [30]  [680/969]  eta: 0:02:44  lr: 0.000250  loss: 0.6870 (0.6700)  time: 0.5678  data: 0.0002  max mem: 6820\n[02:11:47.149798] Epoch: [30]  [700/969]  eta: 0:02:33  lr: 0.000250  loss: 0.6495 (0.6694)  time: 0.5689  data: 0.0002  max mem: 6820\n[02:11:58.548007] Epoch: [30]  [720/969]  eta: 0:02:21  lr: 0.000250  loss: 0.6848 (0.6697)  time: 0.5699  data: 0.0002  max mem: 6820\n[02:12:09.934373] Epoch: [30]  [740/969]  eta: 0:02:10  lr: 0.000250  loss: 0.6775 (0.6698)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:12:21.327481] Epoch: [30]  [760/969]  eta: 0:01:58  lr: 0.000250  loss: 0.6703 (0.6700)  time: 0.5696  data: 0.0002  max mem: 6820\n[02:12:32.703307] Epoch: [30]  [780/969]  eta: 0:01:47  lr: 0.000249  loss: 0.6596 (0.6699)  time: 0.5687  data: 0.0002  max mem: 6820\n[02:12:44.065194] Epoch: [30]  [800/969]  eta: 0:01:36  lr: 0.000249  loss: 0.6675 (0.6702)  time: 0.5680  data: 0.0002  max mem: 6820\n[02:12:55.411544] Epoch: [30]  [820/969]  eta: 0:01:24  lr: 0.000249  loss: 0.6473 (0.6698)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:13:06.795131] Epoch: [30]  [840/969]  eta: 0:01:13  lr: 0.000249  loss: 0.6266 (0.6690)  time: 0.5691  data: 0.0002  max mem: 6820\n[02:13:18.141453] Epoch: [30]  [860/969]  eta: 0:01:01  lr: 0.000249  loss: 0.6771 (0.6691)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:13:29.518850] Epoch: [30]  [880/969]  eta: 0:00:50  lr: 0.000249  loss: 0.6492 (0.6688)  time: 0.5688  data: 0.0002  max mem: 6820\n[02:13:40.884054] Epoch: [30]  [900/969]  eta: 0:00:39  lr: 0.000249  loss: 0.6881 (0.6692)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:13:52.237854] Epoch: [30]  [920/969]  eta: 0:00:27  lr: 0.000249  loss: 0.6518 (0.6687)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:14:03.581844] Epoch: [30]  [940/969]  eta: 0:00:16  lr: 0.000248  loss: 0.6426 (0.6685)  time: 0.5672  data: 0.0002  max mem: 6820\n[02:14:14.946056] Epoch: [30]  [960/969]  eta: 0:00:05  lr: 0.000248  loss: 0.6678 (0.6690)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:14:19.476300] Epoch: [30]  [968/969]  eta: 0:00:00  lr: 0.000248  loss: 0.6704 (0.6692)  time: 0.5678  data: 0.0001  max mem: 6820\n[02:14:19.595626] Epoch: [30] Total time: 0:09:11 (0.5688 s / it)\n[02:14:19.595721] Averaged stats: lr: 0.000248  loss: 0.6704 (0.6692)\n[02:14:20.456545] val:  [  0/139]  eta: 0:01:59  loss: 0.6376 (0.6376)  time: 0.8562  data: 0.7251  max mem: 6820\n[02:14:21.942891] val:  [ 10/139]  eta: 0:00:27  loss: 0.6028 (0.5780)  time: 0.2129  data: 0.0661  max mem: 6820\n[02:14:23.437761] val:  [ 20/139]  eta: 0:00:21  loss: 0.5450 (0.5687)  time: 0.1490  data: 0.0002  max mem: 6820\n[02:14:24.940915] val:  [ 30/139]  eta: 0:00:18  loss: 0.5297 (0.5681)  time: 0.1498  data: 0.0002  max mem: 6820\n[02:14:26.448823] val:  [ 40/139]  eta: 0:00:16  loss: 0.6206 (0.5898)  time: 0.1505  data: 0.0002  max mem: 6820\n[02:14:27.952743] val:  [ 50/139]  eta: 0:00:14  loss: 0.6392 (0.5981)  time: 0.1505  data: 0.0002  max mem: 6820\n[02:14:29.458518] val:  [ 60/139]  eta: 0:00:12  loss: 0.6378 (0.6048)  time: 0.1504  data: 0.0002  max mem: 6820\n[02:14:30.969620] val:  [ 70/139]  eta: 0:00:11  loss: 0.6514 (0.6120)  time: 0.1508  data: 0.0002  max mem: 6820\n[02:14:32.476462] val:  [ 80/139]  eta: 0:00:09  loss: 0.6645 (0.6225)  time: 0.1508  data: 0.0002  max mem: 6820\n[02:14:33.993134] val:  [ 90/139]  eta: 0:00:07  loss: 0.7145 (0.6346)  time: 0.1511  data: 0.0002  max mem: 6820\n[02:14:35.507110] val:  [100/139]  eta: 0:00:06  loss: 0.7443 (0.6439)  time: 0.1515  data: 0.0002  max mem: 6820\n[02:14:37.021234] val:  [110/139]  eta: 0:00:04  loss: 0.7276 (0.6502)  time: 0.1513  data: 0.0002  max mem: 6820\n[02:14:38.533404] val:  [120/139]  eta: 0:00:02  loss: 0.6438 (0.6487)  time: 0.1512  data: 0.0002  max mem: 6820\n[02:14:40.047471] val:  [130/139]  eta: 0:00:01  loss: 0.5883 (0.6406)  time: 0.1512  data: 0.0002  max mem: 6820\n[02:14:41.159942] val:  [138/139]  eta: 0:00:00  loss: 0.5488 (0.6336)  time: 0.1463  data: 0.0001  max mem: 6820\n[02:14:41.272073] val: Total time: 0:00:21 (0.1559 s / it)\n[02:14:41.326828] val loss: 0.6336257715019391\n[02:14:41.327320] Accuracy: 0.6415, F1 Score: 0.6406, ROC AUC: 0.6988, Hamming Loss: 0.3585,\n Jaccard Score: 0.4715, Precision: 0.6429, Recall: 0.6415,\n Average Precision: 0.6938, Kappa: 0.2830, Score: 0.5408\n[02:14:44.295262] Best epoch = 30, Best score = 0.5408\n[02:14:44.297941] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[02:14:45.753431] Epoch: [31]  [  0/969]  eta: 0:23:29  lr: 0.000248  loss: 0.5812 (0.5812)  time: 1.4541  data: 0.8459  max mem: 6820\n[02:14:57.023154] Epoch: [31]  [ 20/969]  eta: 0:09:34  lr: 0.000248  loss: 0.6607 (0.6596)  time: 0.5634  data: 0.0002  max mem: 6820\n[02:15:08.512625] Epoch: [31]  [ 40/969]  eta: 0:09:08  lr: 0.000248  loss: 0.6593 (0.6639)  time: 0.5744  data: 0.0002  max mem: 6820\n[02:15:20.092544] Epoch: [31]  [ 60/969]  eta: 0:08:53  lr: 0.000248  loss: 0.6479 (0.6653)  time: 0.5789  data: 0.0002  max mem: 6820\n[02:15:31.588595] Epoch: [31]  [ 80/969]  eta: 0:08:38  lr: 0.000248  loss: 0.6586 (0.6649)  time: 0.5748  data: 0.0002  max mem: 6820\n[02:15:42.970039] Epoch: [31]  [100/969]  eta: 0:08:24  lr: 0.000248  loss: 0.6748 (0.6674)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:15:54.274327] Epoch: [31]  [120/969]  eta: 0:08:10  lr: 0.000248  loss: 0.6972 (0.6713)  time: 0.5652  data: 0.0002  max mem: 6820\n[02:16:05.570410] Epoch: [31]  [140/969]  eta: 0:07:57  lr: 0.000247  loss: 0.6662 (0.6698)  time: 0.5648  data: 0.0002  max mem: 6820\n[02:16:16.908356] Epoch: [31]  [160/969]  eta: 0:07:45  lr: 0.000247  loss: 0.6568 (0.6690)  time: 0.5668  data: 0.0002  max mem: 6820\n[02:16:28.274790] Epoch: [31]  [180/969]  eta: 0:07:33  lr: 0.000247  loss: 0.6634 (0.6699)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:16:39.656045] Epoch: [31]  [200/969]  eta: 0:07:21  lr: 0.000247  loss: 0.6881 (0.6717)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:16:51.051215] Epoch: [31]  [220/969]  eta: 0:07:09  lr: 0.000247  loss: 0.6714 (0.6720)  time: 0.5697  data: 0.0002  max mem: 6820\n[02:17:02.451807] Epoch: [31]  [240/969]  eta: 0:06:57  lr: 0.000247  loss: 0.6868 (0.6734)  time: 0.5700  data: 0.0002  max mem: 6820\n[02:17:13.845487] Epoch: [31]  [260/969]  eta: 0:06:46  lr: 0.000247  loss: 0.6441 (0.6725)  time: 0.5696  data: 0.0002  max mem: 6820\n[02:17:25.260097] Epoch: [31]  [280/969]  eta: 0:06:34  lr: 0.000247  loss: 0.6570 (0.6728)  time: 0.5707  data: 0.0002  max mem: 6820\n[02:17:36.688858] Epoch: [31]  [300/969]  eta: 0:06:23  lr: 0.000247  loss: 0.6723 (0.6734)  time: 0.5714  data: 0.0002  max mem: 6820\n[02:17:48.134376] Epoch: [31]  [320/969]  eta: 0:06:11  lr: 0.000246  loss: 0.6445 (0.6721)  time: 0.5722  data: 0.0002  max mem: 6820\n[02:17:59.593448] Epoch: [31]  [340/969]  eta: 0:06:00  lr: 0.000246  loss: 0.6635 (0.6710)  time: 0.5729  data: 0.0002  max mem: 6820\n[02:18:11.012137] Epoch: [31]  [360/969]  eta: 0:05:48  lr: 0.000246  loss: 0.6771 (0.6724)  time: 0.5709  data: 0.0002  max mem: 6820\n[02:18:22.413227] Epoch: [31]  [380/969]  eta: 0:05:37  lr: 0.000246  loss: 0.6323 (0.6716)  time: 0.5700  data: 0.0002  max mem: 6820\n[02:18:33.835988] Epoch: [31]  [400/969]  eta: 0:05:25  lr: 0.000246  loss: 0.6529 (0.6710)  time: 0.5711  data: 0.0002  max mem: 6820\n[02:18:45.251403] Epoch: [31]  [420/969]  eta: 0:05:14  lr: 0.000246  loss: 0.6730 (0.6705)  time: 0.5707  data: 0.0002  max mem: 6820\n[02:18:56.667102] Epoch: [31]  [440/969]  eta: 0:05:02  lr: 0.000246  loss: 0.6608 (0.6704)  time: 0.5707  data: 0.0002  max mem: 6820\n[02:19:08.081586] Epoch: [31]  [460/969]  eta: 0:04:51  lr: 0.000246  loss: 0.6469 (0.6701)  time: 0.5707  data: 0.0002  max mem: 6820\n[02:19:19.467306] Epoch: [31]  [480/969]  eta: 0:04:39  lr: 0.000245  loss: 0.6697 (0.6702)  time: 0.5692  data: 0.0002  max mem: 6820\n[02:19:30.832152] Epoch: [31]  [500/969]  eta: 0:04:28  lr: 0.000245  loss: 0.6508 (0.6703)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:19:42.228753] Epoch: [31]  [520/969]  eta: 0:04:16  lr: 0.000245  loss: 0.6377 (0.6694)  time: 0.5698  data: 0.0002  max mem: 6820\n[02:19:53.602108] Epoch: [31]  [540/969]  eta: 0:04:05  lr: 0.000245  loss: 0.6635 (0.6693)  time: 0.5686  data: 0.0002  max mem: 6820\n[02:20:04.967128] Epoch: [31]  [560/969]  eta: 0:03:53  lr: 0.000245  loss: 0.6343 (0.6687)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:20:16.317023] Epoch: [31]  [580/969]  eta: 0:03:42  lr: 0.000245  loss: 0.6482 (0.6687)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:20:27.670191] Epoch: [31]  [600/969]  eta: 0:03:30  lr: 0.000245  loss: 0.6617 (0.6690)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:20:38.983943] Epoch: [31]  [620/969]  eta: 0:03:19  lr: 0.000245  loss: 0.6842 (0.6695)  time: 0.5656  data: 0.0002  max mem: 6820\n[02:20:50.318061] Epoch: [31]  [640/969]  eta: 0:03:07  lr: 0.000245  loss: 0.6656 (0.6696)  time: 0.5667  data: 0.0002  max mem: 6820\n[02:21:01.682482] Epoch: [31]  [660/969]  eta: 0:02:56  lr: 0.000244  loss: 0.6688 (0.6697)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:21:13.072721] Epoch: [31]  [680/969]  eta: 0:02:44  lr: 0.000244  loss: 0.6785 (0.6702)  time: 0.5695  data: 0.0002  max mem: 6820\n[02:21:24.449777] Epoch: [31]  [700/969]  eta: 0:02:33  lr: 0.000244  loss: 0.6751 (0.6707)  time: 0.5688  data: 0.0002  max mem: 6820\n[02:21:35.845029] Epoch: [31]  [720/969]  eta: 0:02:22  lr: 0.000244  loss: 0.6535 (0.6704)  time: 0.5697  data: 0.0002  max mem: 6820\n[02:21:47.151067] Epoch: [31]  [740/969]  eta: 0:02:10  lr: 0.000244  loss: 0.6796 (0.6706)  time: 0.5652  data: 0.0002  max mem: 6820\n[02:21:58.568844] Epoch: [31]  [760/969]  eta: 0:01:59  lr: 0.000244  loss: 0.6828 (0.6709)  time: 0.5708  data: 0.0002  max mem: 6820\n[02:22:09.985324] Epoch: [31]  [780/969]  eta: 0:01:47  lr: 0.000244  loss: 0.6848 (0.6713)  time: 0.5708  data: 0.0002  max mem: 6820\n[02:22:21.391118] Epoch: [31]  [800/969]  eta: 0:01:36  lr: 0.000244  loss: 0.6987 (0.6719)  time: 0.5702  data: 0.0002  max mem: 6820\n[02:22:32.762186] Epoch: [31]  [820/969]  eta: 0:01:25  lr: 0.000243  loss: 0.6609 (0.6717)  time: 0.5685  data: 0.0002  max mem: 6820\n[02:22:44.132129] Epoch: [31]  [840/969]  eta: 0:01:13  lr: 0.000243  loss: 0.6631 (0.6716)  time: 0.5684  data: 0.0002  max mem: 6820\n[02:22:55.478056] Epoch: [31]  [860/969]  eta: 0:01:02  lr: 0.000243  loss: 0.6586 (0.6715)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:23:06.848653] Epoch: [31]  [880/969]  eta: 0:00:50  lr: 0.000243  loss: 0.6783 (0.6715)  time: 0.5685  data: 0.0002  max mem: 6820\n[02:23:18.235542] Epoch: [31]  [900/969]  eta: 0:00:39  lr: 0.000243  loss: 0.6514 (0.6711)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:23:29.579123] Epoch: [31]  [920/969]  eta: 0:00:27  lr: 0.000243  loss: 0.6622 (0.6711)  time: 0.5671  data: 0.0002  max mem: 6820\n[02:23:40.927421] Epoch: [31]  [940/969]  eta: 0:00:16  lr: 0.000243  loss: 0.6817 (0.6711)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:23:52.254699] Epoch: [31]  [960/969]  eta: 0:00:05  lr: 0.000243  loss: 0.6532 (0.6712)  time: 0.5663  data: 0.0002  max mem: 6820\n[02:23:56.791077] Epoch: [31]  [968/969]  eta: 0:00:00  lr: 0.000243  loss: 0.6471 (0.6714)  time: 0.5666  data: 0.0001  max mem: 6820\n[02:23:56.915321] Epoch: [31] Total time: 0:09:12 (0.5703 s / it)\n[02:23:56.915435] Averaged stats: lr: 0.000243  loss: 0.6471 (0.6714)\n[02:23:57.724889] val:  [  0/139]  eta: 0:01:51  loss: 0.6397 (0.6397)  time: 0.8047  data: 0.6680  max mem: 6820\n[02:23:59.217755] val:  [ 10/139]  eta: 0:00:26  loss: 0.6195 (0.5880)  time: 0.2088  data: 0.0612  max mem: 6820\n[02:24:00.712245] val:  [ 20/139]  eta: 0:00:21  loss: 0.5664 (0.5794)  time: 0.1493  data: 0.0004  max mem: 6820\n[02:24:02.210478] val:  [ 30/139]  eta: 0:00:18  loss: 0.5289 (0.5738)  time: 0.1496  data: 0.0002  max mem: 6820\n[02:24:03.716760] val:  [ 40/139]  eta: 0:00:16  loss: 0.6190 (0.5919)  time: 0.1501  data: 0.0002  max mem: 6820\n[02:24:05.222183] val:  [ 50/139]  eta: 0:00:14  loss: 0.6252 (0.5980)  time: 0.1505  data: 0.0002  max mem: 6820\n[02:24:06.726434] val:  [ 60/139]  eta: 0:00:12  loss: 0.6193 (0.6039)  time: 0.1504  data: 0.0002  max mem: 6820\n[02:24:08.235444] val:  [ 70/139]  eta: 0:00:10  loss: 0.6520 (0.6102)  time: 0.1506  data: 0.0002  max mem: 6820\n[02:24:09.741707] val:  [ 80/139]  eta: 0:00:09  loss: 0.6520 (0.6183)  time: 0.1507  data: 0.0002  max mem: 6820\n[02:24:11.253928] val:  [ 90/139]  eta: 0:00:07  loss: 0.7300 (0.6327)  time: 0.1509  data: 0.0002  max mem: 6820\n[02:24:12.768608] val:  [100/139]  eta: 0:00:06  loss: 0.7448 (0.6430)  time: 0.1513  data: 0.0002  max mem: 6820\n[02:24:14.283202] val:  [110/139]  eta: 0:00:04  loss: 0.7379 (0.6495)  time: 0.1514  data: 0.0002  max mem: 6820\n[02:24:15.794630] val:  [120/139]  eta: 0:00:02  loss: 0.6474 (0.6471)  time: 0.1512  data: 0.0002  max mem: 6820\n[02:24:17.311814] val:  [130/139]  eta: 0:00:01  loss: 0.5770 (0.6390)  time: 0.1514  data: 0.0001  max mem: 6820\n[02:24:18.425587] val:  [138/139]  eta: 0:00:00  loss: 0.5443 (0.6332)  time: 0.1464  data: 0.0001  max mem: 6820\n[02:24:18.531692] val: Total time: 0:00:21 (0.1555 s / it)\n[02:24:18.586214] val loss: 0.6332056608131463\n[02:24:18.586266] Accuracy: 0.6429, F1 Score: 0.6424, ROC AUC: 0.6987, Hamming Loss: 0.3571,\n Jaccard Score: 0.4733, Precision: 0.6436, Recall: 0.6429,\n Average Precision: 0.6970, Kappa: 0.2857, Score: 0.5423\n[02:24:21.453362] Best epoch = 31, Best score = 0.5423\n[02:24:21.456136] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[02:24:23.028814] Epoch: [32]  [  0/969]  eta: 0:25:22  lr: 0.000243  loss: 0.5941 (0.5941)  time: 1.5713  data: 0.9691  max mem: 6820\n[02:24:34.307169] Epoch: [32]  [ 20/969]  eta: 0:09:40  lr: 0.000242  loss: 0.6523 (0.6569)  time: 0.5639  data: 0.0002  max mem: 6820\n[02:24:45.817738] Epoch: [32]  [ 40/969]  eta: 0:09:11  lr: 0.000242  loss: 0.6599 (0.6619)  time: 0.5755  data: 0.0002  max mem: 6820\n[02:24:57.389747] Epoch: [32]  [ 60/969]  eta: 0:08:55  lr: 0.000242  loss: 0.6538 (0.6624)  time: 0.5785  data: 0.0002  max mem: 6820\n[02:25:08.896174] Epoch: [32]  [ 80/969]  eta: 0:08:40  lr: 0.000242  loss: 0.6847 (0.6660)  time: 0.5753  data: 0.0002  max mem: 6820\n[02:25:20.274236] Epoch: [32]  [100/969]  eta: 0:08:26  lr: 0.000242  loss: 0.6660 (0.6679)  time: 0.5689  data: 0.0002  max mem: 6820\n[02:25:31.613582] Epoch: [32]  [120/969]  eta: 0:08:12  lr: 0.000242  loss: 0.6859 (0.6693)  time: 0.5669  data: 0.0002  max mem: 6820\n[02:25:42.906859] Epoch: [32]  [140/969]  eta: 0:07:58  lr: 0.000242  loss: 0.7045 (0.6733)  time: 0.5646  data: 0.0002  max mem: 6820\n[02:25:54.226404] Epoch: [32]  [160/969]  eta: 0:07:46  lr: 0.000242  loss: 0.6605 (0.6712)  time: 0.5659  data: 0.0002  max mem: 6820\n[02:26:05.562674] Epoch: [32]  [180/969]  eta: 0:07:33  lr: 0.000241  loss: 0.6699 (0.6711)  time: 0.5668  data: 0.0002  max mem: 6820\n[02:26:16.949417] Epoch: [32]  [200/969]  eta: 0:07:21  lr: 0.000241  loss: 0.7082 (0.6734)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:26:28.355050] Epoch: [32]  [220/969]  eta: 0:07:10  lr: 0.000241  loss: 0.6762 (0.6739)  time: 0.5702  data: 0.0002  max mem: 6820\n[02:26:39.782260] Epoch: [32]  [240/969]  eta: 0:06:58  lr: 0.000241  loss: 0.6605 (0.6737)  time: 0.5713  data: 0.0002  max mem: 6820\n[02:26:51.209202] Epoch: [32]  [260/969]  eta: 0:06:46  lr: 0.000241  loss: 0.6422 (0.6724)  time: 0.5713  data: 0.0002  max mem: 6820\n[02:27:02.654353] Epoch: [32]  [280/969]  eta: 0:06:35  lr: 0.000241  loss: 0.6658 (0.6722)  time: 0.5722  data: 0.0002  max mem: 6820\n[02:27:14.078149] Epoch: [32]  [300/969]  eta: 0:06:23  lr: 0.000241  loss: 0.6649 (0.6717)  time: 0.5711  data: 0.0002  max mem: 6820\n[02:27:25.485108] Epoch: [32]  [320/969]  eta: 0:06:12  lr: 0.000241  loss: 0.6337 (0.6701)  time: 0.5703  data: 0.0002  max mem: 6820\n[02:27:36.909440] Epoch: [32]  [340/969]  eta: 0:06:00  lr: 0.000240  loss: 0.6602 (0.6703)  time: 0.5712  data: 0.0002  max mem: 6820\n[02:27:48.303416] Epoch: [32]  [360/969]  eta: 0:05:48  lr: 0.000240  loss: 0.6629 (0.6703)  time: 0.5696  data: 0.0002  max mem: 6820\n[02:27:59.671124] Epoch: [32]  [380/969]  eta: 0:05:37  lr: 0.000240  loss: 0.6807 (0.6708)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:28:11.042350] Epoch: [32]  [400/969]  eta: 0:05:25  lr: 0.000240  loss: 0.6578 (0.6705)  time: 0.5685  data: 0.0002  max mem: 6820\n[02:28:22.397031] Epoch: [32]  [420/969]  eta: 0:05:14  lr: 0.000240  loss: 0.6792 (0.6705)  time: 0.5677  data: 0.0002  max mem: 6820\n[02:28:33.735612] Epoch: [32]  [440/969]  eta: 0:05:02  lr: 0.000240  loss: 0.6763 (0.6705)  time: 0.5669  data: 0.0002  max mem: 6820\n[02:28:45.069028] Epoch: [32]  [460/969]  eta: 0:04:51  lr: 0.000240  loss: 0.6654 (0.6702)  time: 0.5666  data: 0.0002  max mem: 6820\n[02:28:56.404196] Epoch: [32]  [480/969]  eta: 0:04:39  lr: 0.000240  loss: 0.6467 (0.6702)  time: 0.5667  data: 0.0002  max mem: 6820\n[02:29:07.742793] Epoch: [32]  [500/969]  eta: 0:04:27  lr: 0.000240  loss: 0.6275 (0.6689)  time: 0.5669  data: 0.0002  max mem: 6820\n[02:29:19.092148] Epoch: [32]  [520/969]  eta: 0:04:16  lr: 0.000239  loss: 0.6369 (0.6679)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:29:30.459013] Epoch: [32]  [540/969]  eta: 0:04:05  lr: 0.000239  loss: 0.6691 (0.6679)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:29:41.806882] Epoch: [32]  [560/969]  eta: 0:03:53  lr: 0.000239  loss: 0.6777 (0.6682)  time: 0.5673  data: 0.0001  max mem: 6820\n[02:29:53.173190] Epoch: [32]  [580/969]  eta: 0:03:42  lr: 0.000239  loss: 0.6611 (0.6685)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:30:04.572890] Epoch: [32]  [600/969]  eta: 0:03:30  lr: 0.000239  loss: 0.6655 (0.6691)  time: 0.5699  data: 0.0002  max mem: 6820\n[02:30:15.970850] Epoch: [32]  [620/969]  eta: 0:03:19  lr: 0.000239  loss: 0.6459 (0.6689)  time: 0.5698  data: 0.0002  max mem: 6820\n[02:30:27.366323] Epoch: [32]  [640/969]  eta: 0:03:07  lr: 0.000239  loss: 0.6696 (0.6688)  time: 0.5697  data: 0.0002  max mem: 6820\n[02:30:38.759384] Epoch: [32]  [660/969]  eta: 0:02:56  lr: 0.000239  loss: 0.6457 (0.6683)  time: 0.5696  data: 0.0002  max mem: 6820\n[02:30:50.182319] Epoch: [32]  [680/969]  eta: 0:02:44  lr: 0.000238  loss: 0.6963 (0.6692)  time: 0.5711  data: 0.0002  max mem: 6820\n[02:31:01.583422] Epoch: [32]  [700/969]  eta: 0:02:33  lr: 0.000238  loss: 0.6629 (0.6692)  time: 0.5700  data: 0.0002  max mem: 6820\n[02:31:12.969608] Epoch: [32]  [720/969]  eta: 0:02:22  lr: 0.000238  loss: 0.6889 (0.6695)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:31:24.345362] Epoch: [32]  [740/969]  eta: 0:02:10  lr: 0.000238  loss: 0.6595 (0.6693)  time: 0.5687  data: 0.0002  max mem: 6820\n[02:31:35.722801] Epoch: [32]  [760/969]  eta: 0:01:59  lr: 0.000238  loss: 0.6609 (0.6695)  time: 0.5688  data: 0.0002  max mem: 6820\n[02:31:47.073756] Epoch: [32]  [780/969]  eta: 0:01:47  lr: 0.000238  loss: 0.6711 (0.6693)  time: 0.5675  data: 0.0002  max mem: 6820\n[02:31:58.426807] Epoch: [32]  [800/969]  eta: 0:01:36  lr: 0.000238  loss: 0.6676 (0.6695)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:32:09.800517] Epoch: [32]  [820/969]  eta: 0:01:24  lr: 0.000238  loss: 0.6605 (0.6693)  time: 0.5686  data: 0.0002  max mem: 6820\n[02:32:21.143513] Epoch: [32]  [840/969]  eta: 0:01:13  lr: 0.000237  loss: 0.6664 (0.6694)  time: 0.5671  data: 0.0002  max mem: 6820\n[02:32:32.466404] Epoch: [32]  [860/969]  eta: 0:01:02  lr: 0.000237  loss: 0.6661 (0.6692)  time: 0.5661  data: 0.0002  max mem: 6820\n[02:32:43.821635] Epoch: [32]  [880/969]  eta: 0:00:50  lr: 0.000237  loss: 0.6762 (0.6693)  time: 0.5677  data: 0.0002  max mem: 6820\n[02:32:55.158216] Epoch: [32]  [900/969]  eta: 0:00:39  lr: 0.000237  loss: 0.6584 (0.6691)  time: 0.5668  data: 0.0002  max mem: 6820\n[02:33:06.488585] Epoch: [32]  [920/969]  eta: 0:00:27  lr: 0.000237  loss: 0.6482 (0.6688)  time: 0.5665  data: 0.0002  max mem: 6820\n[02:33:17.826498] Epoch: [32]  [940/969]  eta: 0:00:16  lr: 0.000237  loss: 0.6449 (0.6684)  time: 0.5668  data: 0.0002  max mem: 6820\n[02:33:29.212504] Epoch: [32]  [960/969]  eta: 0:00:05  lr: 0.000237  loss: 0.6430 (0.6682)  time: 0.5692  data: 0.0002  max mem: 6820\n[02:33:33.779562] Epoch: [32]  [968/969]  eta: 0:00:00  lr: 0.000237  loss: 0.6458 (0.6684)  time: 0.5701  data: 0.0002  max mem: 6820\n[02:33:33.904309] Epoch: [32] Total time: 0:09:12 (0.5701 s / it)\n[02:33:33.904422] Averaged stats: lr: 0.000237  loss: 0.6458 (0.6684)\n[02:33:34.827481] val:  [  0/139]  eta: 0:02:07  loss: 0.6411 (0.6411)  time: 0.9152  data: 0.7665  max mem: 6820\n[02:33:36.324873] val:  [ 10/139]  eta: 0:00:28  loss: 0.6207 (0.5963)  time: 0.2191  data: 0.0699  max mem: 6820\n[02:33:37.832898] val:  [ 20/139]  eta: 0:00:22  loss: 0.5678 (0.5801)  time: 0.1501  data: 0.0002  max mem: 6820\n[02:33:39.344242] val:  [ 30/139]  eta: 0:00:19  loss: 0.5293 (0.5749)  time: 0.1509  data: 0.0002  max mem: 6820\n[02:33:40.860963] val:  [ 40/139]  eta: 0:00:16  loss: 0.6618 (0.6057)  time: 0.1513  data: 0.0002  max mem: 6820\n[02:33:42.377919] val:  [ 50/139]  eta: 0:00:14  loss: 0.6627 (0.6165)  time: 0.1516  data: 0.0002  max mem: 6820\n[02:33:43.892651] val:  [ 60/139]  eta: 0:00:12  loss: 0.6627 (0.6221)  time: 0.1515  data: 0.0002  max mem: 6820\n[02:33:45.414210] val:  [ 70/139]  eta: 0:00:11  loss: 0.6701 (0.6269)  time: 0.1517  data: 0.0002  max mem: 6820\n[02:33:46.930037] val:  [ 80/139]  eta: 0:00:09  loss: 0.6361 (0.6313)  time: 0.1518  data: 0.0002  max mem: 6820\n[02:33:48.445422] val:  [ 90/139]  eta: 0:00:07  loss: 0.7371 (0.6451)  time: 0.1515  data: 0.0002  max mem: 6820\n[02:33:49.961307] val:  [100/139]  eta: 0:00:06  loss: 0.7565 (0.6543)  time: 0.1515  data: 0.0002  max mem: 6820\n[02:33:51.480513] val:  [110/139]  eta: 0:00:04  loss: 0.7462 (0.6579)  time: 0.1517  data: 0.0002  max mem: 6820\n[02:33:52.999577] val:  [120/139]  eta: 0:00:02  loss: 0.5942 (0.6504)  time: 0.1518  data: 0.0002  max mem: 6820\n[02:33:54.515615] val:  [130/139]  eta: 0:00:01  loss: 0.5244 (0.6348)  time: 0.1517  data: 0.0002  max mem: 6820\n[02:33:55.632541] val:  [138/139]  eta: 0:00:00  loss: 0.4190 (0.6239)  time: 0.1466  data: 0.0001  max mem: 6820\n[02:33:55.744657] val: Total time: 0:00:21 (0.1571 s / it)\n[02:33:55.802893] val loss: 0.6238890997797465\n[02:33:55.802942] Accuracy: 0.6460, F1 Score: 0.6460, ROC AUC: 0.7055, Hamming Loss: 0.3540,\n Jaccard Score: 0.4771, Precision: 0.6460, Recall: 0.6460,\n Average Precision: 0.7029, Kappa: 0.2920, Score: 0.5478\n[02:33:58.767777] Best epoch = 32, Best score = 0.5478\n[02:33:58.774338] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[02:34:00.290348] Epoch: [33]  [  0/969]  eta: 0:24:27  lr: 0.000237  loss: 0.6040 (0.6040)  time: 1.5147  data: 0.9322  max mem: 6820\n[02:34:11.585851] Epoch: [33]  [ 20/969]  eta: 0:09:38  lr: 0.000237  loss: 0.6800 (0.6653)  time: 0.5647  data: 0.0002  max mem: 6820\n[02:34:23.086549] Epoch: [33]  [ 40/969]  eta: 0:09:10  lr: 0.000236  loss: 0.6779 (0.6714)  time: 0.5750  data: 0.0002  max mem: 6820\n[02:34:34.602130] Epoch: [33]  [ 60/969]  eta: 0:08:53  lr: 0.000236  loss: 0.6782 (0.6762)  time: 0.5757  data: 0.0002  max mem: 6820\n[02:34:46.047597] Epoch: [33]  [ 80/969]  eta: 0:08:38  lr: 0.000236  loss: 0.6581 (0.6758)  time: 0.5722  data: 0.0002  max mem: 6820\n[02:34:57.420909] Epoch: [33]  [100/969]  eta: 0:08:24  lr: 0.000236  loss: 0.6874 (0.6786)  time: 0.5686  data: 0.0002  max mem: 6820\n[02:35:08.736172] Epoch: [33]  [120/969]  eta: 0:08:10  lr: 0.000236  loss: 0.6757 (0.6766)  time: 0.5657  data: 0.0002  max mem: 6820\n[02:35:20.046862] Epoch: [33]  [140/969]  eta: 0:07:57  lr: 0.000236  loss: 0.6616 (0.6764)  time: 0.5655  data: 0.0002  max mem: 6820\n[02:35:31.368096] Epoch: [33]  [160/969]  eta: 0:07:45  lr: 0.000236  loss: 0.6665 (0.6749)  time: 0.5660  data: 0.0002  max mem: 6820\n[02:35:42.764695] Epoch: [33]  [180/969]  eta: 0:07:33  lr: 0.000236  loss: 0.6454 (0.6730)  time: 0.5698  data: 0.0002  max mem: 6820\n[02:35:54.208777] Epoch: [33]  [200/969]  eta: 0:07:21  lr: 0.000235  loss: 0.6574 (0.6728)  time: 0.5721  data: 0.0002  max mem: 6820\n[02:36:05.651117] Epoch: [33]  [220/969]  eta: 0:07:09  lr: 0.000235  loss: 0.6654 (0.6724)  time: 0.5721  data: 0.0002  max mem: 6820\n[02:36:17.065765] Epoch: [33]  [240/969]  eta: 0:06:58  lr: 0.000235  loss: 0.6788 (0.6730)  time: 0.5707  data: 0.0002  max mem: 6820\n[02:36:28.426615] Epoch: [33]  [260/969]  eta: 0:06:46  lr: 0.000235  loss: 0.6458 (0.6711)  time: 0.5680  data: 0.0002  max mem: 6820\n[02:36:39.811149] Epoch: [33]  [280/969]  eta: 0:06:34  lr: 0.000235  loss: 0.6506 (0.6694)  time: 0.5692  data: 0.0002  max mem: 6820\n[02:36:51.191545] Epoch: [33]  [300/969]  eta: 0:06:23  lr: 0.000235  loss: 0.6568 (0.6699)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:37:02.550609] Epoch: [33]  [320/969]  eta: 0:06:11  lr: 0.000235  loss: 0.6140 (0.6681)  time: 0.5679  data: 0.0002  max mem: 6820\n[02:37:13.933681] Epoch: [33]  [340/969]  eta: 0:05:59  lr: 0.000235  loss: 0.6681 (0.6683)  time: 0.5691  data: 0.0002  max mem: 6820\n[02:37:25.302883] Epoch: [33]  [360/969]  eta: 0:05:48  lr: 0.000234  loss: 0.6823 (0.6694)  time: 0.5684  data: 0.0002  max mem: 6820\n[02:37:36.655058] Epoch: [33]  [380/969]  eta: 0:05:36  lr: 0.000234  loss: 0.6636 (0.6688)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:37:48.011570] Epoch: [33]  [400/969]  eta: 0:05:25  lr: 0.000234  loss: 0.6484 (0.6682)  time: 0.5678  data: 0.0002  max mem: 6820\n[02:37:59.360521] Epoch: [33]  [420/969]  eta: 0:05:13  lr: 0.000234  loss: 0.6389 (0.6675)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:38:10.739022] Epoch: [33]  [440/969]  eta: 0:05:02  lr: 0.000234  loss: 0.6507 (0.6672)  time: 0.5689  data: 0.0002  max mem: 6820\n[02:38:22.077856] Epoch: [33]  [460/969]  eta: 0:04:50  lr: 0.000234  loss: 0.6633 (0.6669)  time: 0.5669  data: 0.0001  max mem: 6820\n[02:38:33.423091] Epoch: [33]  [480/969]  eta: 0:04:39  lr: 0.000234  loss: 0.6721 (0.6673)  time: 0.5672  data: 0.0002  max mem: 6820\n[02:38:44.772180] Epoch: [33]  [500/969]  eta: 0:04:27  lr: 0.000234  loss: 0.6515 (0.6676)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:38:56.089005] Epoch: [33]  [520/969]  eta: 0:04:16  lr: 0.000233  loss: 0.6468 (0.6668)  time: 0.5658  data: 0.0002  max mem: 6820\n[02:39:07.436307] Epoch: [33]  [540/969]  eta: 0:04:04  lr: 0.000233  loss: 0.6678 (0.6669)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:39:18.780339] Epoch: [33]  [560/969]  eta: 0:03:53  lr: 0.000233  loss: 0.6537 (0.6668)  time: 0.5672  data: 0.0002  max mem: 6820\n[02:39:30.138422] Epoch: [33]  [580/969]  eta: 0:03:41  lr: 0.000233  loss: 0.6631 (0.6668)  time: 0.5679  data: 0.0002  max mem: 6820\n[02:39:41.529737] Epoch: [33]  [600/969]  eta: 0:03:30  lr: 0.000233  loss: 0.6785 (0.6673)  time: 0.5695  data: 0.0002  max mem: 6820\n[02:39:52.912839] Epoch: [33]  [620/969]  eta: 0:03:19  lr: 0.000233  loss: 0.6757 (0.6675)  time: 0.5691  data: 0.0002  max mem: 6820\n[02:40:04.305618] Epoch: [33]  [640/969]  eta: 0:03:07  lr: 0.000233  loss: 0.6671 (0.6676)  time: 0.5696  data: 0.0005  max mem: 6820\n[02:40:15.710758] Epoch: [33]  [660/969]  eta: 0:02:56  lr: 0.000233  loss: 0.6511 (0.6674)  time: 0.5702  data: 0.0002  max mem: 6820\n[02:40:27.138385] Epoch: [33]  [680/969]  eta: 0:02:44  lr: 0.000232  loss: 0.6719 (0.6679)  time: 0.5713  data: 0.0002  max mem: 6820\n[02:40:38.556288] Epoch: [33]  [700/969]  eta: 0:02:33  lr: 0.000232  loss: 0.6429 (0.6673)  time: 0.5708  data: 0.0002  max mem: 6820\n[02:40:49.957774] Epoch: [33]  [720/969]  eta: 0:02:21  lr: 0.000232  loss: 0.6863 (0.6679)  time: 0.5700  data: 0.0002  max mem: 6820\n[02:41:01.356142] Epoch: [33]  [740/969]  eta: 0:02:10  lr: 0.000232  loss: 0.6642 (0.6678)  time: 0.5699  data: 0.0002  max mem: 6820\n[02:41:12.743900] Epoch: [33]  [760/969]  eta: 0:01:59  lr: 0.000232  loss: 0.6641 (0.6680)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:41:24.134625] Epoch: [33]  [780/969]  eta: 0:01:47  lr: 0.000232  loss: 0.6762 (0.6684)  time: 0.5695  data: 0.0002  max mem: 6820\n[02:41:35.536783] Epoch: [33]  [800/969]  eta: 0:01:36  lr: 0.000232  loss: 0.6861 (0.6690)  time: 0.5701  data: 0.0002  max mem: 6820\n[02:41:46.806965] Epoch: [33]  [820/969]  eta: 0:01:24  lr: 0.000231  loss: 0.6558 (0.6689)  time: 0.5635  data: 0.0002  max mem: 6820\n[02:41:58.158507] Epoch: [33]  [840/969]  eta: 0:01:13  lr: 0.000231  loss: 0.6392 (0.6685)  time: 0.5675  data: 0.0002  max mem: 6820\n[02:42:09.504115] Epoch: [33]  [860/969]  eta: 0:01:02  lr: 0.000231  loss: 0.6366 (0.6682)  time: 0.5672  data: 0.0002  max mem: 6820\n[02:42:20.871407] Epoch: [33]  [880/969]  eta: 0:00:50  lr: 0.000231  loss: 0.6649 (0.6679)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:42:32.231956] Epoch: [33]  [900/969]  eta: 0:00:39  lr: 0.000231  loss: 0.6656 (0.6679)  time: 0.5680  data: 0.0002  max mem: 6820\n[02:42:43.586051] Epoch: [33]  [920/969]  eta: 0:00:27  lr: 0.000231  loss: 0.6601 (0.6679)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:42:54.932043] Epoch: [33]  [940/969]  eta: 0:00:16  lr: 0.000231  loss: 0.6808 (0.6681)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:43:06.271097] Epoch: [33]  [960/969]  eta: 0:00:05  lr: 0.000231  loss: 0.6478 (0.6679)  time: 0.5669  data: 0.0002  max mem: 6820\n[02:43:10.810001] Epoch: [33]  [968/969]  eta: 0:00:00  lr: 0.000231  loss: 0.6536 (0.6682)  time: 0.5672  data: 0.0002  max mem: 6820\n[02:43:10.929601] Epoch: [33] Total time: 0:09:12 (0.5698 s / it)\n[02:43:10.929709] Averaged stats: lr: 0.000231  loss: 0.6536 (0.6682)\n[02:43:11.770407] val:  [  0/139]  eta: 0:01:55  loss: 0.6471 (0.6471)  time: 0.8342  data: 0.7032  max mem: 6820\n[02:43:13.265516] val:  [ 10/139]  eta: 0:00:27  loss: 0.5986 (0.5917)  time: 0.2116  data: 0.0641  max mem: 6820\n[02:43:14.767486] val:  [ 20/139]  eta: 0:00:21  loss: 0.5758 (0.5758)  time: 0.1497  data: 0.0002  max mem: 6820\n[02:43:16.281042] val:  [ 30/139]  eta: 0:00:18  loss: 0.5190 (0.5711)  time: 0.1507  data: 0.0002  max mem: 6820\n[02:43:17.785435] val:  [ 40/139]  eta: 0:00:16  loss: 0.6433 (0.5979)  time: 0.1508  data: 0.0002  max mem: 6820\n[02:43:19.290259] val:  [ 50/139]  eta: 0:00:14  loss: 0.6496 (0.6061)  time: 0.1504  data: 0.0002  max mem: 6820\n[02:43:20.798790] val:  [ 60/139]  eta: 0:00:12  loss: 0.6353 (0.6108)  time: 0.1506  data: 0.0002  max mem: 6820\n[02:43:22.309139] val:  [ 70/139]  eta: 0:00:11  loss: 0.6375 (0.6156)  time: 0.1509  data: 0.0002  max mem: 6820\n[02:43:23.822036] val:  [ 80/139]  eta: 0:00:09  loss: 0.6490 (0.6217)  time: 0.1511  data: 0.0002  max mem: 6820\n[02:43:25.334838] val:  [ 90/139]  eta: 0:00:07  loss: 0.7413 (0.6365)  time: 0.1512  data: 0.0002  max mem: 6820\n[02:43:26.847313] val:  [100/139]  eta: 0:00:06  loss: 0.7568 (0.6469)  time: 0.1512  data: 0.0002  max mem: 6820\n[02:43:28.356029] val:  [110/139]  eta: 0:00:04  loss: 0.7510 (0.6523)  time: 0.1510  data: 0.0002  max mem: 6820\n[02:43:29.872948] val:  [120/139]  eta: 0:00:02  loss: 0.6044 (0.6467)  time: 0.1512  data: 0.0002  max mem: 6820\n[02:43:31.386259] val:  [130/139]  eta: 0:00:01  loss: 0.5472 (0.6324)  time: 0.1514  data: 0.0002  max mem: 6820\n[02:43:32.497666] val:  [138/139]  eta: 0:00:00  loss: 0.4393 (0.6223)  time: 0.1462  data: 0.0001  max mem: 6820\n[02:43:32.603908] val: Total time: 0:00:21 (0.1559 s / it)\n[02:43:32.658528] val loss: 0.6223379725174938\n[02:43:32.658579] Accuracy: 0.6451, F1 Score: 0.6444, ROC AUC: 0.7075, Hamming Loss: 0.3549,\n Jaccard Score: 0.4755, Precision: 0.6463, Recall: 0.6451,\n Average Precision: 0.7056, Kappa: 0.2902, Score: 0.5474\n[02:43:32.660362] Best epoch = 32, Best score = 0.5478\n[02:43:32.662810] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[02:43:34.154384] Epoch: [34]  [  0/969]  eta: 0:24:04  lr: 0.000231  loss: 0.5900 (0.5900)  time: 1.4903  data: 0.8835  max mem: 6820\n[02:43:45.520746] Epoch: [34]  [ 20/969]  eta: 0:09:40  lr: 0.000230  loss: 0.6496 (0.6473)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:43:56.896357] Epoch: [34]  [ 40/969]  eta: 0:09:09  lr: 0.000230  loss: 0.6819 (0.6555)  time: 0.5687  data: 0.0002  max mem: 6820\n[02:44:08.333421] Epoch: [34]  [ 60/969]  eta: 0:08:51  lr: 0.000230  loss: 0.6821 (0.6633)  time: 0.5718  data: 0.0002  max mem: 6820\n[02:44:19.739570] Epoch: [34]  [ 80/969]  eta: 0:08:36  lr: 0.000230  loss: 0.6299 (0.6603)  time: 0.5703  data: 0.0002  max mem: 6820\n[02:44:31.170386] Epoch: [34]  [100/969]  eta: 0:08:23  lr: 0.000230  loss: 0.6857 (0.6651)  time: 0.5715  data: 0.0002  max mem: 6820\n[02:44:42.565774] Epoch: [34]  [120/969]  eta: 0:08:10  lr: 0.000230  loss: 0.7016 (0.6698)  time: 0.5697  data: 0.0002  max mem: 6820\n[02:44:53.918560] Epoch: [34]  [140/969]  eta: 0:07:57  lr: 0.000230  loss: 0.6789 (0.6720)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:45:05.241175] Epoch: [34]  [160/969]  eta: 0:07:45  lr: 0.000230  loss: 0.6599 (0.6701)  time: 0.5661  data: 0.0002  max mem: 6820\n[02:45:16.573324] Epoch: [34]  [180/969]  eta: 0:07:32  lr: 0.000229  loss: 0.6632 (0.6702)  time: 0.5666  data: 0.0002  max mem: 6820\n[02:45:27.899521] Epoch: [34]  [200/969]  eta: 0:07:20  lr: 0.000229  loss: 0.6842 (0.6722)  time: 0.5663  data: 0.0002  max mem: 6820\n[02:45:39.234166] Epoch: [34]  [220/969]  eta: 0:07:08  lr: 0.000229  loss: 0.6817 (0.6734)  time: 0.5667  data: 0.0002  max mem: 6820\n[02:45:50.583395] Epoch: [34]  [240/969]  eta: 0:06:57  lr: 0.000229  loss: 0.6725 (0.6734)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:46:01.942153] Epoch: [34]  [260/969]  eta: 0:06:45  lr: 0.000229  loss: 0.6620 (0.6726)  time: 0.5679  data: 0.0002  max mem: 6820\n[02:46:13.344487] Epoch: [34]  [280/969]  eta: 0:06:33  lr: 0.000229  loss: 0.6932 (0.6735)  time: 0.5701  data: 0.0002  max mem: 6820\n[02:46:24.722779] Epoch: [34]  [300/969]  eta: 0:06:22  lr: 0.000229  loss: 0.6530 (0.6728)  time: 0.5689  data: 0.0002  max mem: 6820\n[02:46:36.133091] Epoch: [34]  [320/969]  eta: 0:06:10  lr: 0.000229  loss: 0.6558 (0.6718)  time: 0.5705  data: 0.0002  max mem: 6820\n[02:46:47.543631] Epoch: [34]  [340/969]  eta: 0:05:59  lr: 0.000228  loss: 0.6750 (0.6726)  time: 0.5705  data: 0.0002  max mem: 6820\n[02:46:58.957313] Epoch: [34]  [360/969]  eta: 0:05:47  lr: 0.000228  loss: 0.6716 (0.6729)  time: 0.5706  data: 0.0002  max mem: 6820\n[02:47:10.350852] Epoch: [34]  [380/969]  eta: 0:05:36  lr: 0.000228  loss: 0.6667 (0.6722)  time: 0.5696  data: 0.0002  max mem: 6820\n[02:47:21.719344] Epoch: [34]  [400/969]  eta: 0:05:24  lr: 0.000228  loss: 0.6497 (0.6717)  time: 0.5684  data: 0.0002  max mem: 6820\n[02:47:33.066157] Epoch: [34]  [420/969]  eta: 0:05:13  lr: 0.000228  loss: 0.6567 (0.6712)  time: 0.5673  data: 0.0002  max mem: 6820\n[02:47:44.377755] Epoch: [34]  [440/969]  eta: 0:05:01  lr: 0.000228  loss: 0.6570 (0.6710)  time: 0.5655  data: 0.0001  max mem: 6820\n[02:47:55.704549] Epoch: [34]  [460/969]  eta: 0:04:50  lr: 0.000228  loss: 0.6512 (0.6703)  time: 0.5663  data: 0.0002  max mem: 6820\n[02:48:07.050155] Epoch: [34]  [480/969]  eta: 0:04:38  lr: 0.000227  loss: 0.6418 (0.6701)  time: 0.5672  data: 0.0002  max mem: 6820\n[02:48:18.406410] Epoch: [34]  [500/969]  eta: 0:04:27  lr: 0.000227  loss: 0.6624 (0.6703)  time: 0.5678  data: 0.0002  max mem: 6820\n[02:48:29.778035] Epoch: [34]  [520/969]  eta: 0:04:16  lr: 0.000227  loss: 0.6398 (0.6693)  time: 0.5685  data: 0.0002  max mem: 6820\n[02:48:41.161270] Epoch: [34]  [540/969]  eta: 0:04:04  lr: 0.000227  loss: 0.6191 (0.6679)  time: 0.5691  data: 0.0002  max mem: 6820\n[02:48:52.552026] Epoch: [34]  [560/969]  eta: 0:03:53  lr: 0.000227  loss: 0.6574 (0.6678)  time: 0.5695  data: 0.0002  max mem: 6820\n[02:49:03.938277] Epoch: [34]  [580/969]  eta: 0:03:41  lr: 0.000227  loss: 0.6635 (0.6678)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:49:15.332210] Epoch: [34]  [600/969]  eta: 0:03:30  lr: 0.000227  loss: 0.6902 (0.6686)  time: 0.5696  data: 0.0002  max mem: 6820\n[02:49:26.716233] Epoch: [34]  [620/969]  eta: 0:03:18  lr: 0.000227  loss: 0.6516 (0.6686)  time: 0.5692  data: 0.0002  max mem: 6820\n[02:49:38.114432] Epoch: [34]  [640/969]  eta: 0:03:07  lr: 0.000226  loss: 0.6885 (0.6691)  time: 0.5698  data: 0.0002  max mem: 6820\n[02:49:49.528106] Epoch: [34]  [660/969]  eta: 0:02:56  lr: 0.000226  loss: 0.6612 (0.6690)  time: 0.5706  data: 0.0002  max mem: 6820\n[02:50:00.929402] Epoch: [34]  [680/969]  eta: 0:02:44  lr: 0.000226  loss: 0.6734 (0.6695)  time: 0.5700  data: 0.0002  max mem: 6820\n[02:50:12.317202] Epoch: [34]  [700/969]  eta: 0:02:33  lr: 0.000226  loss: 0.6677 (0.6690)  time: 0.5693  data: 0.0002  max mem: 6820\n[02:50:23.700521] Epoch: [34]  [720/969]  eta: 0:02:21  lr: 0.000226  loss: 0.6727 (0.6689)  time: 0.5691  data: 0.0002  max mem: 6820\n[02:50:35.088929] Epoch: [34]  [740/969]  eta: 0:02:10  lr: 0.000226  loss: 0.6583 (0.6687)  time: 0.5694  data: 0.0002  max mem: 6820\n[02:50:46.459243] Epoch: [34]  [760/969]  eta: 0:01:59  lr: 0.000226  loss: 0.6326 (0.6683)  time: 0.5685  data: 0.0002  max mem: 6820\n[02:50:57.813008] Epoch: [34]  [780/969]  eta: 0:01:47  lr: 0.000226  loss: 0.6634 (0.6684)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:51:09.166737] Epoch: [34]  [800/969]  eta: 0:01:36  lr: 0.000225  loss: 0.6683 (0.6684)  time: 0.5676  data: 0.0002  max mem: 6820\n[02:51:20.534723] Epoch: [34]  [820/969]  eta: 0:01:24  lr: 0.000225  loss: 0.6385 (0.6678)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:51:31.890147] Epoch: [34]  [840/969]  eta: 0:01:13  lr: 0.000225  loss: 0.6425 (0.6675)  time: 0.5677  data: 0.0002  max mem: 6820\n[02:51:43.218108] Epoch: [34]  [860/969]  eta: 0:01:02  lr: 0.000225  loss: 0.6143 (0.6669)  time: 0.5663  data: 0.0002  max mem: 6820\n[02:51:54.540436] Epoch: [34]  [880/969]  eta: 0:00:50  lr: 0.000225  loss: 0.6780 (0.6670)  time: 0.5661  data: 0.0002  max mem: 6820\n[02:52:05.876037] Epoch: [34]  [900/969]  eta: 0:00:39  lr: 0.000225  loss: 0.6498 (0.6670)  time: 0.5667  data: 0.0002  max mem: 6820\n[02:52:17.203213] Epoch: [34]  [920/969]  eta: 0:00:27  lr: 0.000225  loss: 0.6521 (0.6668)  time: 0.5663  data: 0.0002  max mem: 6820\n[02:52:28.552307] Epoch: [34]  [940/969]  eta: 0:00:16  lr: 0.000225  loss: 0.6416 (0.6666)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:52:39.927290] Epoch: [34]  [960/969]  eta: 0:00:05  lr: 0.000224  loss: 0.6634 (0.6669)  time: 0.5687  data: 0.0002  max mem: 6820\n[02:52:44.474127] Epoch: [34]  [968/969]  eta: 0:00:00  lr: 0.000224  loss: 0.6615 (0.6672)  time: 0.5686  data: 0.0002  max mem: 6820\n[02:52:44.594403] Epoch: [34] Total time: 0:09:11 (0.5696 s / it)\n[02:52:44.594518] Averaged stats: lr: 0.000224  loss: 0.6615 (0.6672)\n[02:52:45.476057] val:  [  0/139]  eta: 0:02:01  loss: 0.6098 (0.6098)  time: 0.8746  data: 0.7332  max mem: 6820\n[02:52:46.975540] val:  [ 10/139]  eta: 0:00:27  loss: 0.5900 (0.5631)  time: 0.2158  data: 0.0668  max mem: 6820\n[02:52:48.481644] val:  [ 20/139]  eta: 0:00:21  loss: 0.5282 (0.5503)  time: 0.1502  data: 0.0002  max mem: 6820\n[02:52:49.994381] val:  [ 30/139]  eta: 0:00:18  loss: 0.5075 (0.5478)  time: 0.1509  data: 0.0002  max mem: 6820\n[02:52:51.508863] val:  [ 40/139]  eta: 0:00:16  loss: 0.6054 (0.5704)  time: 0.1513  data: 0.0002  max mem: 6820\n[02:52:53.021351] val:  [ 50/139]  eta: 0:00:14  loss: 0.6145 (0.5792)  time: 0.1513  data: 0.0002  max mem: 6820\n[02:52:54.540960] val:  [ 60/139]  eta: 0:00:12  loss: 0.6027 (0.5845)  time: 0.1515  data: 0.0002  max mem: 6820\n[02:52:56.059719] val:  [ 70/139]  eta: 0:00:11  loss: 0.6229 (0.5915)  time: 0.1518  data: 0.0002  max mem: 6820\n[02:52:57.578717] val:  [ 80/139]  eta: 0:00:09  loss: 0.6451 (0.6065)  time: 0.1518  data: 0.0002  max mem: 6820\n[02:52:59.098966] val:  [ 90/139]  eta: 0:00:07  loss: 0.7467 (0.6242)  time: 0.1519  data: 0.0002  max mem: 6820\n[02:53:00.621538] val:  [100/139]  eta: 0:00:06  loss: 0.7626 (0.6374)  time: 0.1521  data: 0.0002  max mem: 6820\n[02:53:02.143579] val:  [110/139]  eta: 0:00:04  loss: 0.7619 (0.6454)  time: 0.1522  data: 0.0002  max mem: 6820\n[02:53:03.659179] val:  [120/139]  eta: 0:00:02  loss: 0.6504 (0.6444)  time: 0.1518  data: 0.0002  max mem: 6820\n[02:53:05.173698] val:  [130/139]  eta: 0:00:01  loss: 0.6187 (0.6360)  time: 0.1514  data: 0.0002  max mem: 6820\n[02:53:06.296227] val:  [138/139]  eta: 0:00:00  loss: 0.5306 (0.6285)  time: 0.1469  data: 0.0001  max mem: 6820\n[02:53:06.399714] val: Total time: 0:00:21 (0.1568 s / it)\n[02:53:06.455270] val loss: 0.6284519442551427\n[02:53:06.455335] Accuracy: 0.6469, F1 Score: 0.6445, ROC AUC: 0.7071, Hamming Loss: 0.3531,\n Jaccard Score: 0.4762, Precision: 0.6510, Recall: 0.6469,\n Average Precision: 0.7044, Kappa: 0.2939, Score: 0.5485\n[02:53:09.357854] Best epoch = 34, Best score = 0.5485\n[02:53:09.364572] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[02:53:10.737505] Epoch: [35]  [  0/969]  eta: 0:22:08  lr: 0.000224  loss: 0.6294 (0.6294)  time: 1.3713  data: 0.7130  max mem: 6820\n[02:53:22.052095] Epoch: [35]  [ 20/969]  eta: 0:09:33  lr: 0.000224  loss: 0.6449 (0.6536)  time: 0.5657  data: 0.0002  max mem: 6820\n[02:53:33.482632] Epoch: [35]  [ 40/969]  eta: 0:09:06  lr: 0.000224  loss: 0.6694 (0.6629)  time: 0.5715  data: 0.0002  max mem: 6820\n[02:53:44.971992] Epoch: [35]  [ 60/969]  eta: 0:08:50  lr: 0.000224  loss: 0.6665 (0.6667)  time: 0.5744  data: 0.0002  max mem: 6820\n[02:53:56.442358] Epoch: [35]  [ 80/969]  eta: 0:08:36  lr: 0.000224  loss: 0.6778 (0.6698)  time: 0.5735  data: 0.0002  max mem: 6820\n[02:54:07.808749] Epoch: [35]  [100/969]  eta: 0:08:22  lr: 0.000224  loss: 0.6582 (0.6701)  time: 0.5683  data: 0.0002  max mem: 6820\n[02:54:19.156864] Epoch: [35]  [120/969]  eta: 0:08:09  lr: 0.000224  loss: 0.6867 (0.6726)  time: 0.5674  data: 0.0002  max mem: 6820\n[02:54:30.470470] Epoch: [35]  [140/969]  eta: 0:07:56  lr: 0.000223  loss: 0.6667 (0.6717)  time: 0.5656  data: 0.0002  max mem: 6820\n[02:54:41.781104] Epoch: [35]  [160/969]  eta: 0:07:44  lr: 0.000223  loss: 0.6658 (0.6706)  time: 0.5655  data: 0.0002  max mem: 6820\n[02:54:53.113801] Epoch: [35]  [180/969]  eta: 0:07:32  lr: 0.000223  loss: 0.6742 (0.6716)  time: 0.5666  data: 0.0002  max mem: 6820\n[02:55:04.487617] Epoch: [35]  [200/969]  eta: 0:07:20  lr: 0.000223  loss: 0.6524 (0.6707)  time: 0.5686  data: 0.0002  max mem: 6820\n[02:55:15.815439] Epoch: [35]  [220/969]  eta: 0:07:08  lr: 0.000223  loss: 0.6479 (0.6685)  time: 0.5663  data: 0.0002  max mem: 6820\n[02:55:27.178498] Epoch: [35]  [240/969]  eta: 0:06:56  lr: 0.000223  loss: 0.6522 (0.6678)  time: 0.5681  data: 0.0002  max mem: 6820\n[02:55:38.559435] Epoch: [35]  [260/969]  eta: 0:06:45  lr: 0.000223  loss: 0.6606 (0.6685)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:55:49.948728] Epoch: [35]  [280/969]  eta: 0:06:33  lr: 0.000223  loss: 0.6829 (0.6697)  time: 0.5694  data: 0.0002  max mem: 6820\n[02:56:01.354360] Epoch: [35]  [300/969]  eta: 0:06:22  lr: 0.000222  loss: 0.6727 (0.6700)  time: 0.5702  data: 0.0002  max mem: 6820\n[02:56:12.749417] Epoch: [35]  [320/969]  eta: 0:06:10  lr: 0.000222  loss: 0.6463 (0.6686)  time: 0.5697  data: 0.0002  max mem: 6820\n[02:56:24.145384] Epoch: [35]  [340/969]  eta: 0:05:59  lr: 0.000222  loss: 0.6653 (0.6686)  time: 0.5698  data: 0.0003  max mem: 6820\n[02:56:35.536371] Epoch: [35]  [360/969]  eta: 0:05:47  lr: 0.000222  loss: 0.6864 (0.6695)  time: 0.5695  data: 0.0002  max mem: 6820\n[02:56:46.945631] Epoch: [35]  [380/969]  eta: 0:05:36  lr: 0.000222  loss: 0.6627 (0.6691)  time: 0.5704  data: 0.0002  max mem: 6820\n[02:56:58.353005] Epoch: [35]  [400/969]  eta: 0:05:24  lr: 0.000222  loss: 0.6569 (0.6691)  time: 0.5703  data: 0.0002  max mem: 6820\n[02:57:09.755782] Epoch: [35]  [420/969]  eta: 0:05:13  lr: 0.000222  loss: 0.6684 (0.6689)  time: 0.5701  data: 0.0002  max mem: 6820\n[02:57:21.186080] Epoch: [35]  [440/969]  eta: 0:05:02  lr: 0.000221  loss: 0.6460 (0.6678)  time: 0.5715  data: 0.0002  max mem: 6820\n[02:57:32.585836] Epoch: [35]  [460/969]  eta: 0:04:50  lr: 0.000221  loss: 0.6528 (0.6675)  time: 0.5699  data: 0.0002  max mem: 6820\n[02:57:43.965396] Epoch: [35]  [480/969]  eta: 0:04:39  lr: 0.000221  loss: 0.6555 (0.6671)  time: 0.5689  data: 0.0002  max mem: 6820\n[02:57:55.346470] Epoch: [35]  [500/969]  eta: 0:04:27  lr: 0.000221  loss: 0.6545 (0.6668)  time: 0.5690  data: 0.0002  max mem: 6820\n[02:58:06.740862] Epoch: [35]  [520/969]  eta: 0:04:16  lr: 0.000221  loss: 0.6580 (0.6662)  time: 0.5697  data: 0.0002  max mem: 6820\n[02:58:18.105893] Epoch: [35]  [540/969]  eta: 0:04:04  lr: 0.000221  loss: 0.6820 (0.6666)  time: 0.5682  data: 0.0002  max mem: 6820\n[02:58:29.477492] Epoch: [35]  [560/969]  eta: 0:03:53  lr: 0.000221  loss: 0.6665 (0.6666)  time: 0.5685  data: 0.0002  max mem: 6820\n[02:58:40.827792] Epoch: [35]  [580/969]  eta: 0:03:41  lr: 0.000221  loss: 0.6701 (0.6666)  time: 0.5675  data: 0.0002  max mem: 6820\n[02:58:52.172690] Epoch: [35]  [600/969]  eta: 0:03:30  lr: 0.000220  loss: 0.6453 (0.6667)  time: 0.5672  data: 0.0002  max mem: 6820\n[02:59:03.509184] Epoch: [35]  [620/969]  eta: 0:03:19  lr: 0.000220  loss: 0.6482 (0.6664)  time: 0.5668  data: 0.0002  max mem: 6820\n[02:59:14.851131] Epoch: [35]  [640/969]  eta: 0:03:07  lr: 0.000220  loss: 0.6523 (0.6662)  time: 0.5670  data: 0.0002  max mem: 6820\n[02:59:26.181613] Epoch: [35]  [660/969]  eta: 0:02:56  lr: 0.000220  loss: 0.6603 (0.6663)  time: 0.5665  data: 0.0002  max mem: 6820\n[02:59:37.527622] Epoch: [35]  [680/969]  eta: 0:02:44  lr: 0.000220  loss: 0.6816 (0.6667)  time: 0.5672  data: 0.0002  max mem: 6820\n[02:59:48.838373] Epoch: [35]  [700/969]  eta: 0:02:33  lr: 0.000220  loss: 0.6444 (0.6663)  time: 0.5655  data: 0.0002  max mem: 6820\n[03:00:00.165266] Epoch: [35]  [720/969]  eta: 0:02:21  lr: 0.000220  loss: 0.6916 (0.6669)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:00:11.504156] Epoch: [35]  [740/969]  eta: 0:02:10  lr: 0.000219  loss: 0.6324 (0.6665)  time: 0.5669  data: 0.0002  max mem: 6820\n[03:00:22.849059] Epoch: [35]  [760/969]  eta: 0:01:59  lr: 0.000219  loss: 0.6628 (0.6666)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:00:34.175183] Epoch: [35]  [780/969]  eta: 0:01:47  lr: 0.000219  loss: 0.6600 (0.6667)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:00:45.505251] Epoch: [35]  [800/969]  eta: 0:01:36  lr: 0.000219  loss: 0.6706 (0.6672)  time: 0.5664  data: 0.0002  max mem: 6820\n[03:00:56.840287] Epoch: [35]  [820/969]  eta: 0:01:24  lr: 0.000219  loss: 0.6420 (0.6669)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:01:08.197406] Epoch: [35]  [840/969]  eta: 0:01:13  lr: 0.000219  loss: 0.6610 (0.6670)  time: 0.5678  data: 0.0002  max mem: 6820\n[03:01:19.534902] Epoch: [35]  [860/969]  eta: 0:01:02  lr: 0.000219  loss: 0.6628 (0.6670)  time: 0.5668  data: 0.0002  max mem: 6820\n[03:01:30.770083] Epoch: [35]  [880/969]  eta: 0:00:50  lr: 0.000219  loss: 0.6419 (0.6671)  time: 0.5617  data: 0.0002  max mem: 6820\n[03:01:42.099359] Epoch: [35]  [900/969]  eta: 0:00:39  lr: 0.000218  loss: 0.6584 (0.6669)  time: 0.5664  data: 0.0002  max mem: 6820\n[03:01:53.445768] Epoch: [35]  [920/969]  eta: 0:00:27  lr: 0.000218  loss: 0.6469 (0.6666)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:02:04.794241] Epoch: [35]  [940/969]  eta: 0:00:16  lr: 0.000218  loss: 0.6638 (0.6664)  time: 0.5674  data: 0.0002  max mem: 6820\n[03:02:16.158434] Epoch: [35]  [960/969]  eta: 0:00:05  lr: 0.000218  loss: 0.6582 (0.6664)  time: 0.5682  data: 0.0002  max mem: 6820\n[03:02:20.705231] Epoch: [35]  [968/969]  eta: 0:00:00  lr: 0.000218  loss: 0.6539 (0.6665)  time: 0.5680  data: 0.0001  max mem: 6820\n[03:02:20.836622] Epoch: [35] Total time: 0:09:11 (0.5691 s / it)\n[03:02:20.836745] Averaged stats: lr: 0.000218  loss: 0.6539 (0.6665)\n[03:02:21.686244] val:  [  0/139]  eta: 0:01:57  loss: 0.5669 (0.5669)  time: 0.8445  data: 0.6901  max mem: 6820\n[03:02:23.190164] val:  [ 10/139]  eta: 0:00:27  loss: 0.5315 (0.5137)  time: 0.2134  data: 0.0631  max mem: 6820\n[03:02:24.692195] val:  [ 20/139]  eta: 0:00:21  loss: 0.4509 (0.4959)  time: 0.1502  data: 0.0003  max mem: 6820\n[03:02:26.205386] val:  [ 30/139]  eta: 0:00:18  loss: 0.4500 (0.5011)  time: 0.1507  data: 0.0002  max mem: 6820\n[03:02:27.721722] val:  [ 40/139]  eta: 0:00:16  loss: 0.5942 (0.5269)  time: 0.1514  data: 0.0002  max mem: 6820\n[03:02:29.232501] val:  [ 50/139]  eta: 0:00:14  loss: 0.6042 (0.5422)  time: 0.1513  data: 0.0002  max mem: 6820\n[03:02:30.751378] val:  [ 60/139]  eta: 0:00:12  loss: 0.6057 (0.5498)  time: 0.1514  data: 0.0002  max mem: 6820\n[03:02:32.258962] val:  [ 70/139]  eta: 0:00:11  loss: 0.5996 (0.5608)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:02:33.772021] val:  [ 80/139]  eta: 0:00:09  loss: 0.6619 (0.5874)  time: 0.1510  data: 0.0002  max mem: 6820\n[03:02:35.285133] val:  [ 90/139]  eta: 0:00:07  loss: 0.7832 (0.6102)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:02:36.801866] val:  [100/139]  eta: 0:00:06  loss: 0.7888 (0.6284)  time: 0.1514  data: 0.0002  max mem: 6820\n[03:02:38.310672] val:  [110/139]  eta: 0:00:04  loss: 0.7848 (0.6397)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:02:39.824081] val:  [120/139]  eta: 0:00:02  loss: 0.7001 (0.6425)  time: 0.1510  data: 0.0002  max mem: 6820\n[03:02:41.337687] val:  [130/139]  eta: 0:00:01  loss: 0.6361 (0.6374)  time: 0.1513  data: 0.0001  max mem: 6820\n[03:02:42.458084] val:  [138/139]  eta: 0:00:00  loss: 0.5856 (0.6310)  time: 0.1466  data: 0.0001  max mem: 6820\n[03:02:42.563101] val: Total time: 0:00:21 (0.1563 s / it)\n[03:02:42.619322] val loss: 0.6309951672451102\n[03:02:42.619395] Accuracy: 0.6410, F1 Score: 0.6355, ROC AUC: 0.7012, Hamming Loss: 0.3590,\n Jaccard Score: 0.4673, Precision: 0.6503, Recall: 0.6410,\n Average Precision: 0.6990, Kappa: 0.2821, Score: 0.5396\n[03:02:42.621121] Best epoch = 34, Best score = 0.5485\n[03:02:42.623547] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[03:02:44.045407] Epoch: [36]  [  0/969]  eta: 0:22:56  lr: 0.000218  loss: 0.6409 (0.6409)  time: 1.4206  data: 0.8492  max mem: 6820\n[03:02:55.401120] Epoch: [36]  [ 20/969]  eta: 0:09:37  lr: 0.000218  loss: 0.6438 (0.6502)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:03:06.809437] Epoch: [36]  [ 40/969]  eta: 0:09:07  lr: 0.000218  loss: 0.6596 (0.6554)  time: 0.5704  data: 0.0002  max mem: 6820\n[03:03:18.234622] Epoch: [36]  [ 60/969]  eta: 0:08:50  lr: 0.000218  loss: 0.6633 (0.6593)  time: 0.5712  data: 0.0002  max mem: 6820\n[03:03:29.659230] Epoch: [36]  [ 80/969]  eta: 0:08:36  lr: 0.000217  loss: 0.6692 (0.6589)  time: 0.5712  data: 0.0002  max mem: 6820\n[03:03:41.067684] Epoch: [36]  [100/969]  eta: 0:08:22  lr: 0.000217  loss: 0.6583 (0.6620)  time: 0.5704  data: 0.0002  max mem: 6820\n[03:03:52.466481] Epoch: [36]  [120/969]  eta: 0:08:10  lr: 0.000217  loss: 0.6655 (0.6641)  time: 0.5699  data: 0.0002  max mem: 6820\n[03:04:03.820918] Epoch: [36]  [140/969]  eta: 0:07:57  lr: 0.000217  loss: 0.6652 (0.6631)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:04:15.212391] Epoch: [36]  [160/969]  eta: 0:07:45  lr: 0.000217  loss: 0.6596 (0.6646)  time: 0.5695  data: 0.0002  max mem: 6820\n[03:04:26.570763] Epoch: [36]  [180/969]  eta: 0:07:33  lr: 0.000217  loss: 0.6504 (0.6646)  time: 0.5679  data: 0.0002  max mem: 6820\n[03:04:37.920534] Epoch: [36]  [200/969]  eta: 0:07:21  lr: 0.000217  loss: 0.6636 (0.6652)  time: 0.5674  data: 0.0002  max mem: 6820\n[03:04:49.291605] Epoch: [36]  [220/969]  eta: 0:07:09  lr: 0.000217  loss: 0.6626 (0.6655)  time: 0.5685  data: 0.0002  max mem: 6820\n[03:05:00.641762] Epoch: [36]  [240/969]  eta: 0:06:57  lr: 0.000216  loss: 0.6738 (0.6660)  time: 0.5675  data: 0.0002  max mem: 6820\n[03:05:11.985876] Epoch: [36]  [260/969]  eta: 0:06:45  lr: 0.000216  loss: 0.6269 (0.6641)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:05:23.314726] Epoch: [36]  [280/969]  eta: 0:06:33  lr: 0.000216  loss: 0.6731 (0.6658)  time: 0.5664  data: 0.0002  max mem: 6820\n[03:05:34.643074] Epoch: [36]  [300/969]  eta: 0:06:22  lr: 0.000216  loss: 0.6674 (0.6662)  time: 0.5664  data: 0.0002  max mem: 6820\n[03:05:45.986027] Epoch: [36]  [320/969]  eta: 0:06:10  lr: 0.000216  loss: 0.6561 (0.6659)  time: 0.5671  data: 0.0002  max mem: 6820\n[03:05:57.321334] Epoch: [36]  [340/969]  eta: 0:05:59  lr: 0.000216  loss: 0.6550 (0.6665)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:06:08.664886] Epoch: [36]  [360/969]  eta: 0:05:47  lr: 0.000216  loss: 0.6902 (0.6672)  time: 0.5671  data: 0.0002  max mem: 6820\n[03:06:20.038297] Epoch: [36]  [380/969]  eta: 0:05:36  lr: 0.000215  loss: 0.6714 (0.6673)  time: 0.5686  data: 0.0002  max mem: 6820\n[03:06:31.421967] Epoch: [36]  [400/969]  eta: 0:05:24  lr: 0.000215  loss: 0.6620 (0.6676)  time: 0.5691  data: 0.0002  max mem: 6820\n[03:06:42.789566] Epoch: [36]  [420/969]  eta: 0:05:13  lr: 0.000215  loss: 0.6758 (0.6681)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:06:54.185947] Epoch: [36]  [440/969]  eta: 0:05:01  lr: 0.000215  loss: 0.6674 (0.6681)  time: 0.5698  data: 0.0002  max mem: 6820\n[03:07:05.590360] Epoch: [36]  [460/969]  eta: 0:04:50  lr: 0.000215  loss: 0.6505 (0.6681)  time: 0.5702  data: 0.0002  max mem: 6820\n[03:07:16.958671] Epoch: [36]  [480/969]  eta: 0:04:38  lr: 0.000215  loss: 0.6611 (0.6680)  time: 0.5684  data: 0.0002  max mem: 6820\n[03:07:28.345640] Epoch: [36]  [500/969]  eta: 0:04:27  lr: 0.000215  loss: 0.6575 (0.6678)  time: 0.5693  data: 0.0002  max mem: 6820\n[03:07:39.712743] Epoch: [36]  [520/969]  eta: 0:04:16  lr: 0.000214  loss: 0.6350 (0.6669)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:07:51.057298] Epoch: [36]  [540/969]  eta: 0:04:04  lr: 0.000214  loss: 0.6432 (0.6662)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:08:02.407645] Epoch: [36]  [560/969]  eta: 0:03:53  lr: 0.000214  loss: 0.6313 (0.6656)  time: 0.5675  data: 0.0002  max mem: 6820\n[03:08:13.771065] Epoch: [36]  [580/969]  eta: 0:03:41  lr: 0.000214  loss: 0.6720 (0.6656)  time: 0.5681  data: 0.0002  max mem: 6820\n[03:08:25.120694] Epoch: [36]  [600/969]  eta: 0:03:30  lr: 0.000214  loss: 0.6584 (0.6662)  time: 0.5674  data: 0.0002  max mem: 6820\n[03:08:36.449177] Epoch: [36]  [620/969]  eta: 0:03:18  lr: 0.000214  loss: 0.6838 (0.6666)  time: 0.5664  data: 0.0002  max mem: 6820\n[03:08:47.768354] Epoch: [36]  [640/969]  eta: 0:03:07  lr: 0.000214  loss: 0.6525 (0.6666)  time: 0.5659  data: 0.0002  max mem: 6820\n[03:08:59.079309] Epoch: [36]  [660/969]  eta: 0:02:55  lr: 0.000214  loss: 0.6618 (0.6667)  time: 0.5655  data: 0.0002  max mem: 6820\n[03:09:10.404241] Epoch: [36]  [680/969]  eta: 0:02:44  lr: 0.000213  loss: 0.6830 (0.6674)  time: 0.5662  data: 0.0002  max mem: 6820\n[03:09:21.732476] Epoch: [36]  [700/969]  eta: 0:02:33  lr: 0.000213  loss: 0.6556 (0.6674)  time: 0.5664  data: 0.0002  max mem: 6820\n[03:09:33.056821] Epoch: [36]  [720/969]  eta: 0:02:21  lr: 0.000213  loss: 0.6605 (0.6672)  time: 0.5662  data: 0.0002  max mem: 6820\n[03:09:44.385153] Epoch: [36]  [740/969]  eta: 0:02:10  lr: 0.000213  loss: 0.6595 (0.6670)  time: 0.5664  data: 0.0002  max mem: 6820\n[03:09:55.730840] Epoch: [36]  [760/969]  eta: 0:01:58  lr: 0.000213  loss: 0.6511 (0.6668)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:10:07.078126] Epoch: [36]  [780/969]  eta: 0:01:47  lr: 0.000213  loss: 0.6282 (0.6662)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:10:18.472881] Epoch: [36]  [800/969]  eta: 0:01:36  lr: 0.000213  loss: 0.6810 (0.6669)  time: 0.5697  data: 0.0002  max mem: 6820\n[03:10:29.863722] Epoch: [36]  [820/969]  eta: 0:01:24  lr: 0.000212  loss: 0.6455 (0.6668)  time: 0.5695  data: 0.0002  max mem: 6820\n[03:10:41.268188] Epoch: [36]  [840/969]  eta: 0:01:13  lr: 0.000212  loss: 0.6233 (0.6660)  time: 0.5702  data: 0.0002  max mem: 6820\n[03:10:52.669407] Epoch: [36]  [860/969]  eta: 0:01:02  lr: 0.000212  loss: 0.6631 (0.6659)  time: 0.5700  data: 0.0002  max mem: 6820\n[03:11:04.070688] Epoch: [36]  [880/969]  eta: 0:00:50  lr: 0.000212  loss: 0.6591 (0.6662)  time: 0.5700  data: 0.0002  max mem: 6820\n[03:11:15.442750] Epoch: [36]  [900/969]  eta: 0:00:39  lr: 0.000212  loss: 0.6624 (0.6661)  time: 0.5686  data: 0.0002  max mem: 6820\n[03:11:26.828633] Epoch: [36]  [920/969]  eta: 0:00:27  lr: 0.000212  loss: 0.6535 (0.6659)  time: 0.5692  data: 0.0002  max mem: 6820\n[03:11:38.185022] Epoch: [36]  [940/969]  eta: 0:00:16  lr: 0.000212  loss: 0.6376 (0.6656)  time: 0.5678  data: 0.0002  max mem: 6820\n[03:11:49.543994] Epoch: [36]  [960/969]  eta: 0:00:05  lr: 0.000212  loss: 0.6678 (0.6657)  time: 0.5679  data: 0.0002  max mem: 6820\n[03:11:54.098231] Epoch: [36]  [968/969]  eta: 0:00:00  lr: 0.000211  loss: 0.6606 (0.6658)  time: 0.5688  data: 0.0001  max mem: 6820\n[03:11:54.214872] Epoch: [36] Total time: 0:09:11 (0.5692 s / it)\n[03:11:54.215005] Averaged stats: lr: 0.000211  loss: 0.6606 (0.6658)\n[03:11:55.081088] val:  [  0/139]  eta: 0:01:58  loss: 0.6243 (0.6243)  time: 0.8495  data: 0.6792  max mem: 6820\n[03:11:56.568811] val:  [ 10/139]  eta: 0:00:27  loss: 0.5759 (0.5538)  time: 0.2124  data: 0.0622  max mem: 6820\n[03:11:58.077589] val:  [ 20/139]  eta: 0:00:21  loss: 0.4966 (0.5391)  time: 0.1498  data: 0.0003  max mem: 6820\n[03:11:59.578455] val:  [ 30/139]  eta: 0:00:18  loss: 0.4858 (0.5358)  time: 0.1504  data: 0.0002  max mem: 6820\n[03:12:01.084712] val:  [ 40/139]  eta: 0:00:16  loss: 0.5971 (0.5617)  time: 0.1503  data: 0.0002  max mem: 6820\n[03:12:02.597582] val:  [ 50/139]  eta: 0:00:14  loss: 0.6215 (0.5707)  time: 0.1509  data: 0.0002  max mem: 6820\n[03:12:04.109492] val:  [ 60/139]  eta: 0:00:12  loss: 0.6085 (0.5777)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:12:05.617408] val:  [ 70/139]  eta: 0:00:11  loss: 0.6183 (0.5865)  time: 0.1509  data: 0.0002  max mem: 6820\n[03:12:07.127278] val:  [ 80/139]  eta: 0:00:09  loss: 0.6206 (0.6022)  time: 0.1508  data: 0.0002  max mem: 6820\n[03:12:08.643984] val:  [ 90/139]  eta: 0:00:07  loss: 0.7297 (0.6204)  time: 0.1513  data: 0.0002  max mem: 6820\n[03:12:10.155091] val:  [100/139]  eta: 0:00:06  loss: 0.7596 (0.6347)  time: 0.1513  data: 0.0002  max mem: 6820\n[03:12:11.662518] val:  [110/139]  eta: 0:00:04  loss: 0.7596 (0.6432)  time: 0.1508  data: 0.0002  max mem: 6820\n[03:12:13.173231] val:  [120/139]  eta: 0:00:02  loss: 0.6601 (0.6414)  time: 0.1508  data: 0.0002  max mem: 6820\n[03:12:14.689542] val:  [130/139]  eta: 0:00:01  loss: 0.5978 (0.6326)  time: 0.1513  data: 0.0002  max mem: 6820\n[03:12:15.801599] val:  [138/139]  eta: 0:00:00  loss: 0.5238 (0.6247)  time: 0.1464  data: 0.0001  max mem: 6820\n[03:12:15.907349] val: Total time: 0:00:21 (0.1560 s / it)\n[03:12:15.964216] val loss: 0.6246559499836654\n[03:12:15.964280] Accuracy: 0.6474, F1 Score: 0.6461, ROC AUC: 0.7047, Hamming Loss: 0.3526,\n Jaccard Score: 0.4776, Precision: 0.6496, Recall: 0.6474,\n Average Precision: 0.7032, Kappa: 0.2948, Score: 0.5485\n[03:12:18.707322] Best epoch = 36, Best score = 0.5485\n[03:12:18.710186] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[03:12:20.284317] Epoch: [37]  [  0/969]  eta: 0:25:24  lr: 0.000211  loss: 0.6169 (0.6169)  time: 1.5728  data: 0.9796  max mem: 6820\n[03:12:31.543654] Epoch: [37]  [ 20/969]  eta: 0:09:39  lr: 0.000211  loss: 0.6643 (0.6559)  time: 0.5629  data: 0.0002  max mem: 6820\n[03:12:42.980877] Epoch: [37]  [ 40/969]  eta: 0:09:09  lr: 0.000211  loss: 0.6623 (0.6688)  time: 0.5718  data: 0.0002  max mem: 6820\n[03:12:54.506465] Epoch: [37]  [ 60/969]  eta: 0:08:53  lr: 0.000211  loss: 0.6734 (0.6680)  time: 0.5762  data: 0.0002  max mem: 6820\n[03:13:05.997586] Epoch: [37]  [ 80/969]  eta: 0:08:38  lr: 0.000211  loss: 0.6505 (0.6657)  time: 0.5745  data: 0.0002  max mem: 6820\n[03:13:17.420696] Epoch: [37]  [100/969]  eta: 0:08:25  lr: 0.000211  loss: 0.6709 (0.6658)  time: 0.5711  data: 0.0002  max mem: 6820\n[03:13:28.760153] Epoch: [37]  [120/969]  eta: 0:08:11  lr: 0.000211  loss: 0.6631 (0.6674)  time: 0.5669  data: 0.0002  max mem: 6820\n[03:13:40.041536] Epoch: [37]  [140/969]  eta: 0:07:58  lr: 0.000211  loss: 0.6736 (0.6680)  time: 0.5640  data: 0.0002  max mem: 6820\n[03:13:51.343994] Epoch: [37]  [160/969]  eta: 0:07:45  lr: 0.000210  loss: 0.6523 (0.6674)  time: 0.5651  data: 0.0002  max mem: 6820\n[03:14:02.671619] Epoch: [37]  [180/969]  eta: 0:07:33  lr: 0.000210  loss: 0.6593 (0.6685)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:14:14.028751] Epoch: [37]  [200/969]  eta: 0:07:21  lr: 0.000210  loss: 0.6769 (0.6706)  time: 0.5678  data: 0.0002  max mem: 6820\n[03:14:25.399131] Epoch: [37]  [220/969]  eta: 0:07:09  lr: 0.000210  loss: 0.6777 (0.6710)  time: 0.5685  data: 0.0002  max mem: 6820\n[03:14:36.763603] Epoch: [37]  [240/969]  eta: 0:06:57  lr: 0.000210  loss: 0.6704 (0.6717)  time: 0.5682  data: 0.0002  max mem: 6820\n[03:14:48.160291] Epoch: [37]  [260/969]  eta: 0:06:45  lr: 0.000210  loss: 0.6536 (0.6706)  time: 0.5698  data: 0.0002  max mem: 6820\n[03:14:59.548639] Epoch: [37]  [280/969]  eta: 0:06:34  lr: 0.000210  loss: 0.6893 (0.6718)  time: 0.5694  data: 0.0002  max mem: 6820\n[03:15:10.930804] Epoch: [37]  [300/969]  eta: 0:06:22  lr: 0.000209  loss: 0.6612 (0.6712)  time: 0.5691  data: 0.0002  max mem: 6820\n[03:15:22.297802] Epoch: [37]  [320/969]  eta: 0:06:11  lr: 0.000209  loss: 0.6407 (0.6692)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:15:33.690687] Epoch: [37]  [340/969]  eta: 0:05:59  lr: 0.000209  loss: 0.6641 (0.6687)  time: 0.5696  data: 0.0002  max mem: 6820\n[03:15:45.095159] Epoch: [37]  [360/969]  eta: 0:05:48  lr: 0.000209  loss: 0.6882 (0.6687)  time: 0.5702  data: 0.0002  max mem: 6820\n[03:15:56.507613] Epoch: [37]  [380/969]  eta: 0:05:36  lr: 0.000209  loss: 0.6567 (0.6687)  time: 0.5706  data: 0.0002  max mem: 6820\n[03:16:07.931174] Epoch: [37]  [400/969]  eta: 0:05:25  lr: 0.000209  loss: 0.6579 (0.6683)  time: 0.5711  data: 0.0002  max mem: 6820\n[03:16:19.325358] Epoch: [37]  [420/969]  eta: 0:05:13  lr: 0.000209  loss: 0.6664 (0.6680)  time: 0.5697  data: 0.0002  max mem: 6820\n[03:16:30.704541] Epoch: [37]  [440/969]  eta: 0:05:02  lr: 0.000208  loss: 0.6744 (0.6685)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:16:42.097170] Epoch: [37]  [460/969]  eta: 0:04:50  lr: 0.000208  loss: 0.6589 (0.6685)  time: 0.5696  data: 0.0002  max mem: 6820\n[03:16:53.452176] Epoch: [37]  [480/969]  eta: 0:04:39  lr: 0.000208  loss: 0.6504 (0.6686)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:17:04.786554] Epoch: [37]  [500/969]  eta: 0:04:27  lr: 0.000208  loss: 0.6375 (0.6677)  time: 0.5667  data: 0.0001  max mem: 6820\n[03:17:16.105100] Epoch: [37]  [520/969]  eta: 0:04:16  lr: 0.000208  loss: 0.6106 (0.6664)  time: 0.5659  data: 0.0002  max mem: 6820\n[03:17:27.437018] Epoch: [37]  [540/969]  eta: 0:04:04  lr: 0.000208  loss: 0.6524 (0.6664)  time: 0.5665  data: 0.0002  max mem: 6820\n[03:17:38.758006] Epoch: [37]  [560/969]  eta: 0:03:53  lr: 0.000208  loss: 0.6766 (0.6670)  time: 0.5660  data: 0.0002  max mem: 6820\n[03:17:50.084616] Epoch: [37]  [580/969]  eta: 0:03:41  lr: 0.000208  loss: 0.6628 (0.6672)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:18:01.427118] Epoch: [37]  [600/969]  eta: 0:03:30  lr: 0.000207  loss: 0.6784 (0.6679)  time: 0.5671  data: 0.0002  max mem: 6820\n[03:18:12.769030] Epoch: [37]  [620/969]  eta: 0:03:18  lr: 0.000207  loss: 0.6534 (0.6679)  time: 0.5670  data: 0.0002  max mem: 6820\n[03:18:24.151370] Epoch: [37]  [640/969]  eta: 0:03:07  lr: 0.000207  loss: 0.6408 (0.6674)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:18:35.501778] Epoch: [37]  [660/969]  eta: 0:02:56  lr: 0.000207  loss: 0.6358 (0.6672)  time: 0.5675  data: 0.0002  max mem: 6820\n[03:18:46.906389] Epoch: [37]  [680/969]  eta: 0:02:44  lr: 0.000207  loss: 0.6797 (0.6675)  time: 0.5702  data: 0.0002  max mem: 6820\n[03:18:58.283078] Epoch: [37]  [700/969]  eta: 0:02:33  lr: 0.000207  loss: 0.6584 (0.6673)  time: 0.5688  data: 0.0002  max mem: 6820\n[03:19:09.645470] Epoch: [37]  [720/969]  eta: 0:02:21  lr: 0.000207  loss: 0.6711 (0.6677)  time: 0.5681  data: 0.0002  max mem: 6820\n[03:19:20.999250] Epoch: [37]  [740/969]  eta: 0:02:10  lr: 0.000206  loss: 0.6487 (0.6674)  time: 0.5676  data: 0.0002  max mem: 6820\n[03:19:32.361955] Epoch: [37]  [760/969]  eta: 0:01:59  lr: 0.000206  loss: 0.6402 (0.6673)  time: 0.5681  data: 0.0002  max mem: 6820\n[03:19:43.723228] Epoch: [37]  [780/969]  eta: 0:01:47  lr: 0.000206  loss: 0.6398 (0.6674)  time: 0.5680  data: 0.0002  max mem: 6820\n[03:19:55.078791] Epoch: [37]  [800/969]  eta: 0:01:36  lr: 0.000206  loss: 0.6822 (0.6678)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:20:06.432315] Epoch: [37]  [820/969]  eta: 0:01:24  lr: 0.000206  loss: 0.6422 (0.6674)  time: 0.5676  data: 0.0002  max mem: 6820\n[03:20:17.771975] Epoch: [37]  [840/969]  eta: 0:01:13  lr: 0.000206  loss: 0.6328 (0.6668)  time: 0.5669  data: 0.0002  max mem: 6820\n[03:20:29.126313] Epoch: [37]  [860/969]  eta: 0:01:02  lr: 0.000206  loss: 0.6505 (0.6667)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:20:40.471103] Epoch: [37]  [880/969]  eta: 0:00:50  lr: 0.000205  loss: 0.6498 (0.6663)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:20:51.838022] Epoch: [37]  [900/969]  eta: 0:00:39  lr: 0.000205  loss: 0.6704 (0.6664)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:21:03.187220] Epoch: [37]  [920/969]  eta: 0:00:27  lr: 0.000205  loss: 0.6365 (0.6658)  time: 0.5674  data: 0.0001  max mem: 6820\n[03:21:14.544824] Epoch: [37]  [940/969]  eta: 0:00:16  lr: 0.000205  loss: 0.6681 (0.6662)  time: 0.5678  data: 0.0002  max mem: 6820\n[03:21:25.795147] Epoch: [37]  [960/969]  eta: 0:00:05  lr: 0.000205  loss: 0.6664 (0.6660)  time: 0.5625  data: 0.0002  max mem: 6820\n[03:21:30.327439] Epoch: [37]  [968/969]  eta: 0:00:00  lr: 0.000205  loss: 0.6664 (0.6661)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:21:30.454417] Epoch: [37] Total time: 0:09:11 (0.5694 s / it)\n[03:21:30.454520] Averaged stats: lr: 0.000205  loss: 0.6664 (0.6661)\n[03:21:31.269607] val:  [  0/139]  eta: 0:01:52  loss: 0.5773 (0.5773)  time: 0.8078  data: 0.6500  max mem: 6820\n[03:21:32.768804] val:  [ 10/139]  eta: 0:00:27  loss: 0.5764 (0.5552)  time: 0.2097  data: 0.0594  max mem: 6820\n[03:21:34.269303] val:  [ 20/139]  eta: 0:00:21  loss: 0.5341 (0.5427)  time: 0.1499  data: 0.0002  max mem: 6820\n[03:21:35.775376] val:  [ 30/139]  eta: 0:00:18  loss: 0.5157 (0.5409)  time: 0.1503  data: 0.0002  max mem: 6820\n[03:21:37.286601] val:  [ 40/139]  eta: 0:00:16  loss: 0.6081 (0.5655)  time: 0.1508  data: 0.0002  max mem: 6820\n[03:21:38.799495] val:  [ 50/139]  eta: 0:00:14  loss: 0.6081 (0.5740)  time: 0.1511  data: 0.0002  max mem: 6820\n[03:21:40.315007] val:  [ 60/139]  eta: 0:00:12  loss: 0.6059 (0.5815)  time: 0.1513  data: 0.0002  max mem: 6820\n[03:21:41.826804] val:  [ 70/139]  eta: 0:00:11  loss: 0.6293 (0.5899)  time: 0.1513  data: 0.0002  max mem: 6820\n[03:21:43.335131] val:  [ 80/139]  eta: 0:00:09  loss: 0.6490 (0.6040)  time: 0.1509  data: 0.0002  max mem: 6820\n[03:21:44.850850] val:  [ 90/139]  eta: 0:00:07  loss: 0.7266 (0.6202)  time: 0.1511  data: 0.0002  max mem: 6820\n[03:21:46.370995] val:  [100/139]  eta: 0:00:06  loss: 0.7445 (0.6325)  time: 0.1517  data: 0.0002  max mem: 6820\n[03:21:47.884421] val:  [110/139]  eta: 0:00:04  loss: 0.7488 (0.6415)  time: 0.1516  data: 0.0002  max mem: 6820\n[03:21:49.396756] val:  [120/139]  eta: 0:00:02  loss: 0.7029 (0.6405)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:21:50.910137] val:  [130/139]  eta: 0:00:01  loss: 0.5991 (0.6308)  time: 0.1512  data: 0.0001  max mem: 6820\n[03:21:52.023833] val:  [138/139]  eta: 0:00:00  loss: 0.4971 (0.6217)  time: 0.1464  data: 0.0001  max mem: 6820\n[03:21:52.127870] val: Total time: 0:00:21 (0.1559 s / it)\n[03:21:52.185017] val loss: 0.621678194553732\n[03:21:52.185064] Accuracy: 0.6492, F1 Score: 0.6462, ROC AUC: 0.7086, Hamming Loss: 0.3508,\n Jaccard Score: 0.4782, Precision: 0.6543, Recall: 0.6492,\n Average Precision: 0.7074, Kappa: 0.2984, Score: 0.5511\n[03:21:54.871996] Best epoch = 37, Best score = 0.5511\n[03:21:54.874783] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[03:21:56.352553] Epoch: [38]  [  0/969]  eta: 0:23:50  lr: 0.000205  loss: 0.6386 (0.6386)  time: 1.4764  data: 0.8570  max mem: 6820\n[03:22:07.615904] Epoch: [38]  [ 20/969]  eta: 0:09:35  lr: 0.000205  loss: 0.6638 (0.6674)  time: 0.5631  data: 0.0002  max mem: 6820\n[03:22:19.048788] Epoch: [38]  [ 40/969]  eta: 0:09:07  lr: 0.000205  loss: 0.6619 (0.6680)  time: 0.5716  data: 0.0002  max mem: 6820\n[03:22:30.537629] Epoch: [38]  [ 60/969]  eta: 0:08:51  lr: 0.000204  loss: 0.6830 (0.6755)  time: 0.5744  data: 0.0002  max mem: 6820\n[03:22:42.003486] Epoch: [38]  [ 80/969]  eta: 0:08:37  lr: 0.000204  loss: 0.6703 (0.6718)  time: 0.5732  data: 0.0001  max mem: 6820\n[03:22:53.414792] Epoch: [38]  [100/969]  eta: 0:08:23  lr: 0.000204  loss: 0.6593 (0.6726)  time: 0.5705  data: 0.0002  max mem: 6820\n[03:23:04.733478] Epoch: [38]  [120/969]  eta: 0:08:10  lr: 0.000204  loss: 0.6615 (0.6710)  time: 0.5659  data: 0.0002  max mem: 6820\n[03:23:16.051330] Epoch: [38]  [140/969]  eta: 0:07:57  lr: 0.000204  loss: 0.6674 (0.6718)  time: 0.5658  data: 0.0002  max mem: 6820\n[03:23:27.367871] Epoch: [38]  [160/969]  eta: 0:07:44  lr: 0.000204  loss: 0.6663 (0.6713)  time: 0.5658  data: 0.0002  max mem: 6820\n[03:23:38.711212] Epoch: [38]  [180/969]  eta: 0:07:32  lr: 0.000204  loss: 0.6651 (0.6712)  time: 0.5671  data: 0.0002  max mem: 6820\n[03:23:50.049494] Epoch: [38]  [200/969]  eta: 0:07:20  lr: 0.000204  loss: 0.6734 (0.6724)  time: 0.5669  data: 0.0002  max mem: 6820\n[03:24:01.423428] Epoch: [38]  [220/969]  eta: 0:07:08  lr: 0.000203  loss: 0.6599 (0.6713)  time: 0.5686  data: 0.0002  max mem: 6820\n[03:24:12.834920] Epoch: [38]  [240/969]  eta: 0:06:57  lr: 0.000203  loss: 0.6551 (0.6717)  time: 0.5705  data: 0.0002  max mem: 6820\n[03:24:24.230462] Epoch: [38]  [260/969]  eta: 0:06:45  lr: 0.000203  loss: 0.6561 (0.6714)  time: 0.5697  data: 0.0002  max mem: 6820\n[03:24:35.617794] Epoch: [38]  [280/969]  eta: 0:06:34  lr: 0.000203  loss: 0.6758 (0.6720)  time: 0.5693  data: 0.0002  max mem: 6820\n[03:24:47.021959] Epoch: [38]  [300/969]  eta: 0:06:22  lr: 0.000203  loss: 0.6906 (0.6727)  time: 0.5702  data: 0.0002  max mem: 6820\n[03:24:58.422805] Epoch: [38]  [320/969]  eta: 0:06:11  lr: 0.000203  loss: 0.6727 (0.6729)  time: 0.5700  data: 0.0002  max mem: 6820\n[03:25:09.835421] Epoch: [38]  [340/969]  eta: 0:05:59  lr: 0.000203  loss: 0.6599 (0.6725)  time: 0.5706  data: 0.0002  max mem: 6820\n[03:25:21.235570] Epoch: [38]  [360/969]  eta: 0:05:48  lr: 0.000202  loss: 0.6811 (0.6725)  time: 0.5700  data: 0.0002  max mem: 6820\n[03:25:32.637858] Epoch: [38]  [380/969]  eta: 0:05:36  lr: 0.000202  loss: 0.6802 (0.6723)  time: 0.5701  data: 0.0002  max mem: 6820\n[03:25:44.026876] Epoch: [38]  [400/969]  eta: 0:05:25  lr: 0.000202  loss: 0.6580 (0.6718)  time: 0.5694  data: 0.0002  max mem: 6820\n[03:25:55.386787] Epoch: [38]  [420/969]  eta: 0:05:13  lr: 0.000202  loss: 0.6428 (0.6707)  time: 0.5679  data: 0.0002  max mem: 6820\n[03:26:06.766444] Epoch: [38]  [440/969]  eta: 0:05:02  lr: 0.000202  loss: 0.6577 (0.6707)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:26:18.150229] Epoch: [38]  [460/969]  eta: 0:04:50  lr: 0.000202  loss: 0.6482 (0.6704)  time: 0.5691  data: 0.0002  max mem: 6820\n[03:26:29.514449] Epoch: [38]  [480/969]  eta: 0:04:39  lr: 0.000202  loss: 0.6612 (0.6700)  time: 0.5682  data: 0.0002  max mem: 6820\n[03:26:40.880807] Epoch: [38]  [500/969]  eta: 0:04:27  lr: 0.000201  loss: 0.6818 (0.6707)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:26:52.246802] Epoch: [38]  [520/969]  eta: 0:04:16  lr: 0.000201  loss: 0.6311 (0.6699)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:27:03.582310] Epoch: [38]  [540/969]  eta: 0:04:04  lr: 0.000201  loss: 0.6477 (0.6699)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:27:14.955380] Epoch: [38]  [560/969]  eta: 0:03:53  lr: 0.000201  loss: 0.6216 (0.6694)  time: 0.5686  data: 0.0002  max mem: 6820\n[03:27:26.296144] Epoch: [38]  [580/969]  eta: 0:03:41  lr: 0.000201  loss: 0.6555 (0.6688)  time: 0.5670  data: 0.0002  max mem: 6820\n[03:27:37.628514] Epoch: [38]  [600/969]  eta: 0:03:30  lr: 0.000201  loss: 0.6747 (0.6691)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:27:48.956086] Epoch: [38]  [620/969]  eta: 0:03:18  lr: 0.000201  loss: 0.6594 (0.6689)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:28:00.283634] Epoch: [38]  [640/969]  eta: 0:03:07  lr: 0.000200  loss: 0.6708 (0.6688)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:28:11.619744] Epoch: [38]  [660/969]  eta: 0:02:56  lr: 0.000200  loss: 0.6373 (0.6681)  time: 0.5668  data: 0.0002  max mem: 6820\n[03:28:22.966233] Epoch: [38]  [680/969]  eta: 0:02:44  lr: 0.000200  loss: 0.7008 (0.6691)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:28:34.301646] Epoch: [38]  [700/969]  eta: 0:02:33  lr: 0.000200  loss: 0.6544 (0.6687)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:28:45.661662] Epoch: [38]  [720/969]  eta: 0:02:21  lr: 0.000200  loss: 0.6631 (0.6688)  time: 0.5679  data: 0.0002  max mem: 6820\n[03:28:57.041144] Epoch: [38]  [740/969]  eta: 0:02:10  lr: 0.000200  loss: 0.6573 (0.6686)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:29:08.426437] Epoch: [38]  [760/969]  eta: 0:01:59  lr: 0.000200  loss: 0.6616 (0.6686)  time: 0.5692  data: 0.0002  max mem: 6820\n[03:29:19.806444] Epoch: [38]  [780/969]  eta: 0:01:47  lr: 0.000199  loss: 0.6791 (0.6687)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:29:31.194097] Epoch: [38]  [800/969]  eta: 0:01:36  lr: 0.000199  loss: 0.6567 (0.6690)  time: 0.5693  data: 0.0002  max mem: 6820\n[03:29:42.605085] Epoch: [38]  [820/969]  eta: 0:01:24  lr: 0.000199  loss: 0.6447 (0.6690)  time: 0.5705  data: 0.0002  max mem: 6820\n[03:29:54.003891] Epoch: [38]  [840/969]  eta: 0:01:13  lr: 0.000199  loss: 0.6429 (0.6684)  time: 0.5699  data: 0.0002  max mem: 6820\n[03:30:05.397849] Epoch: [38]  [860/969]  eta: 0:01:02  lr: 0.000199  loss: 0.6570 (0.6683)  time: 0.5697  data: 0.0002  max mem: 6820\n[03:30:16.778590] Epoch: [38]  [880/969]  eta: 0:00:50  lr: 0.000199  loss: 0.6770 (0.6687)  time: 0.5690  data: 0.0002  max mem: 6820\n[03:30:28.139182] Epoch: [38]  [900/969]  eta: 0:00:39  lr: 0.000199  loss: 0.6610 (0.6687)  time: 0.5680  data: 0.0002  max mem: 6820\n[03:30:39.486795] Epoch: [38]  [920/969]  eta: 0:00:27  lr: 0.000199  loss: 0.6354 (0.6682)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:30:50.834968] Epoch: [38]  [940/969]  eta: 0:00:16  lr: 0.000198  loss: 0.6365 (0.6679)  time: 0.5674  data: 0.0002  max mem: 6820\n[03:31:02.182501] Epoch: [38]  [960/969]  eta: 0:00:05  lr: 0.000198  loss: 0.6635 (0.6678)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:31:06.730501] Epoch: [38]  [968/969]  eta: 0:00:00  lr: 0.000198  loss: 0.6436 (0.6679)  time: 0.5683  data: 0.0001  max mem: 6820\n[03:31:06.857704] Epoch: [38] Total time: 0:09:11 (0.5696 s / it)\n[03:31:06.857821] Averaged stats: lr: 0.000198  loss: 0.6436 (0.6679)\n[03:31:07.766222] val:  [  0/139]  eta: 0:02:05  loss: 0.7247 (0.7247)  time: 0.9039  data: 0.7662  max mem: 6820\n[03:31:09.268723] val:  [ 10/139]  eta: 0:00:28  loss: 0.6347 (0.6226)  time: 0.2187  data: 0.0701  max mem: 6820\n[03:31:10.761094] val:  [ 20/139]  eta: 0:00:22  loss: 0.5505 (0.5895)  time: 0.1497  data: 0.0003  max mem: 6820\n[03:31:12.268542] val:  [ 30/139]  eta: 0:00:19  loss: 0.5326 (0.5834)  time: 0.1499  data: 0.0002  max mem: 6820\n[03:31:13.773587] val:  [ 40/139]  eta: 0:00:16  loss: 0.6612 (0.6210)  time: 0.1506  data: 0.0002  max mem: 6820\n[03:31:15.284554] val:  [ 50/139]  eta: 0:00:14  loss: 0.6749 (0.6330)  time: 0.1507  data: 0.0002  max mem: 6820\n[03:31:16.800347] val:  [ 60/139]  eta: 0:00:12  loss: 0.6272 (0.6327)  time: 0.1513  data: 0.0002  max mem: 6820\n[03:31:18.308589] val:  [ 70/139]  eta: 0:00:11  loss: 0.6272 (0.6343)  time: 0.1511  data: 0.0002  max mem: 6820\n[03:31:19.819108] val:  [ 80/139]  eta: 0:00:09  loss: 0.6245 (0.6371)  time: 0.1508  data: 0.0002  max mem: 6820\n[03:31:21.331302] val:  [ 90/139]  eta: 0:00:07  loss: 0.7326 (0.6501)  time: 0.1511  data: 0.0002  max mem: 6820\n[03:31:22.842893] val:  [100/139]  eta: 0:00:06  loss: 0.7488 (0.6598)  time: 0.1511  data: 0.0002  max mem: 6820\n[03:31:24.356720] val:  [110/139]  eta: 0:00:04  loss: 0.7468 (0.6639)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:31:25.859876] val:  [120/139]  eta: 0:00:02  loss: 0.5894 (0.6544)  time: 0.1508  data: 0.0002  max mem: 6820\n[03:31:27.372489] val:  [130/139]  eta: 0:00:01  loss: 0.5162 (0.6353)  time: 0.1507  data: 0.0002  max mem: 6820\n[03:31:28.495255] val:  [138/139]  eta: 0:00:00  loss: 0.3735 (0.6218)  time: 0.1467  data: 0.0001  max mem: 6820\n[03:31:28.598949] val: Total time: 0:00:21 (0.1564 s / it)\n[03:31:28.655374] val loss: 0.6217713870590539\n[03:31:28.655426] Accuracy: 0.6510, F1 Score: 0.6491, ROC AUC: 0.7112, Hamming Loss: 0.3490,\n Jaccard Score: 0.4810, Precision: 0.6543, Recall: 0.6510,\n Average Precision: 0.7079, Kappa: 0.3020, Score: 0.5541\n[03:31:31.342297] Best epoch = 38, Best score = 0.5541\n[03:31:31.344989] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[03:31:32.793270] Epoch: [39]  [  0/969]  eta: 0:23:22  lr: 0.000198  loss: 0.5908 (0.5908)  time: 1.4469  data: 0.8134  max mem: 6820\n[03:31:44.076290] Epoch: [39]  [ 20/969]  eta: 0:09:35  lr: 0.000198  loss: 0.6469 (0.6534)  time: 0.5641  data: 0.0002  max mem: 6820\n[03:31:55.491847] Epoch: [39]  [ 40/969]  eta: 0:09:07  lr: 0.000198  loss: 0.6770 (0.6629)  time: 0.5707  data: 0.0002  max mem: 6820\n[03:32:06.968772] Epoch: [39]  [ 60/969]  eta: 0:08:50  lr: 0.000198  loss: 0.6498 (0.6642)  time: 0.5738  data: 0.0002  max mem: 6820\n[03:32:18.439093] Epoch: [39]  [ 80/969]  eta: 0:08:36  lr: 0.000198  loss: 0.6527 (0.6628)  time: 0.5735  data: 0.0002  max mem: 6820\n[03:32:29.802590] Epoch: [39]  [100/969]  eta: 0:08:22  lr: 0.000197  loss: 0.6804 (0.6671)  time: 0.5681  data: 0.0002  max mem: 6820\n[03:32:41.136226] Epoch: [39]  [120/969]  eta: 0:08:09  lr: 0.000197  loss: 0.6560 (0.6662)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:32:52.471156] Epoch: [39]  [140/969]  eta: 0:07:56  lr: 0.000197  loss: 0.6703 (0.6672)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:33:03.798575] Epoch: [39]  [160/969]  eta: 0:07:44  lr: 0.000197  loss: 0.6505 (0.6653)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:33:15.126221] Epoch: [39]  [180/969]  eta: 0:07:32  lr: 0.000197  loss: 0.6737 (0.6675)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:33:26.472948] Epoch: [39]  [200/969]  eta: 0:07:20  lr: 0.000197  loss: 0.6292 (0.6657)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:33:37.809351] Epoch: [39]  [220/969]  eta: 0:07:08  lr: 0.000197  loss: 0.6657 (0.6662)  time: 0.5668  data: 0.0002  max mem: 6820\n[03:33:49.188968] Epoch: [39]  [240/969]  eta: 0:06:56  lr: 0.000197  loss: 0.6637 (0.6662)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:34:00.555432] Epoch: [39]  [260/969]  eta: 0:06:45  lr: 0.000196  loss: 0.6592 (0.6659)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:34:11.955158] Epoch: [39]  [280/969]  eta: 0:06:33  lr: 0.000196  loss: 0.6479 (0.6660)  time: 0.5699  data: 0.0002  max mem: 6820\n[03:34:23.347795] Epoch: [39]  [300/969]  eta: 0:06:22  lr: 0.000196  loss: 0.6297 (0.6651)  time: 0.5696  data: 0.0002  max mem: 6820\n[03:34:34.741182] Epoch: [39]  [320/969]  eta: 0:06:10  lr: 0.000196  loss: 0.6692 (0.6653)  time: 0.5696  data: 0.0002  max mem: 6820\n[03:34:46.108830] Epoch: [39]  [340/969]  eta: 0:05:59  lr: 0.000196  loss: 0.6721 (0.6663)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:34:57.495488] Epoch: [39]  [360/969]  eta: 0:05:47  lr: 0.000196  loss: 0.6562 (0.6668)  time: 0.5693  data: 0.0002  max mem: 6820\n[03:35:08.906133] Epoch: [39]  [380/969]  eta: 0:05:36  lr: 0.000196  loss: 0.6692 (0.6666)  time: 0.5705  data: 0.0002  max mem: 6820\n[03:35:20.321658] Epoch: [39]  [400/969]  eta: 0:05:24  lr: 0.000195  loss: 0.6510 (0.6662)  time: 0.5707  data: 0.0002  max mem: 6820\n[03:35:31.723449] Epoch: [39]  [420/969]  eta: 0:05:13  lr: 0.000195  loss: 0.6596 (0.6661)  time: 0.5700  data: 0.0002  max mem: 6820\n[03:35:43.132598] Epoch: [39]  [440/969]  eta: 0:05:02  lr: 0.000195  loss: 0.6697 (0.6661)  time: 0.5704  data: 0.0002  max mem: 6820\n[03:35:54.522565] Epoch: [39]  [460/969]  eta: 0:04:50  lr: 0.000195  loss: 0.6415 (0.6659)  time: 0.5695  data: 0.0002  max mem: 6820\n[03:36:05.909523] Epoch: [39]  [480/969]  eta: 0:04:39  lr: 0.000195  loss: 0.6593 (0.6659)  time: 0.5693  data: 0.0002  max mem: 6820\n[03:36:17.293476] Epoch: [39]  [500/969]  eta: 0:04:27  lr: 0.000195  loss: 0.6595 (0.6658)  time: 0.5691  data: 0.0002  max mem: 6820\n[03:36:28.664044] Epoch: [39]  [520/969]  eta: 0:04:16  lr: 0.000195  loss: 0.6525 (0.6653)  time: 0.5685  data: 0.0002  max mem: 6820\n[03:36:40.010810] Epoch: [39]  [540/969]  eta: 0:04:04  lr: 0.000194  loss: 0.6604 (0.6652)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:36:51.363678] Epoch: [39]  [560/969]  eta: 0:03:53  lr: 0.000194  loss: 0.6753 (0.6662)  time: 0.5676  data: 0.0002  max mem: 6820\n[03:37:02.711165] Epoch: [39]  [580/969]  eta: 0:03:41  lr: 0.000194  loss: 0.6633 (0.6663)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:37:14.063794] Epoch: [39]  [600/969]  eta: 0:03:30  lr: 0.000194  loss: 0.6640 (0.6666)  time: 0.5676  data: 0.0002  max mem: 6820\n[03:37:25.414544] Epoch: [39]  [620/969]  eta: 0:03:18  lr: 0.000194  loss: 0.6434 (0.6664)  time: 0.5675  data: 0.0002  max mem: 6820\n[03:37:36.737924] Epoch: [39]  [640/969]  eta: 0:03:07  lr: 0.000194  loss: 0.6666 (0.6666)  time: 0.5661  data: 0.0002  max mem: 6820\n[03:37:48.071772] Epoch: [39]  [660/969]  eta: 0:02:56  lr: 0.000194  loss: 0.6911 (0.6671)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:37:59.398731] Epoch: [39]  [680/969]  eta: 0:02:44  lr: 0.000193  loss: 0.6788 (0.6676)  time: 0.5663  data: 0.0002  max mem: 6820\n[03:38:10.729392] Epoch: [39]  [700/969]  eta: 0:02:33  lr: 0.000193  loss: 0.6542 (0.6673)  time: 0.5665  data: 0.0002  max mem: 6820\n[03:38:22.075273] Epoch: [39]  [720/969]  eta: 0:02:21  lr: 0.000193  loss: 0.6912 (0.6678)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:38:33.454860] Epoch: [39]  [740/969]  eta: 0:02:10  lr: 0.000193  loss: 0.6811 (0.6681)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:38:44.833723] Epoch: [39]  [760/969]  eta: 0:01:59  lr: 0.000193  loss: 0.6429 (0.6677)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:38:56.212863] Epoch: [39]  [780/969]  eta: 0:01:47  lr: 0.000193  loss: 0.6307 (0.6673)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:39:07.626481] Epoch: [39]  [800/969]  eta: 0:01:36  lr: 0.000193  loss: 0.6503 (0.6674)  time: 0.5706  data: 0.0002  max mem: 6820\n[03:39:18.994369] Epoch: [39]  [820/969]  eta: 0:01:24  lr: 0.000192  loss: 0.6287 (0.6669)  time: 0.5683  data: 0.0002  max mem: 6820\n[03:39:30.408689] Epoch: [39]  [840/969]  eta: 0:01:13  lr: 0.000192  loss: 0.6527 (0.6668)  time: 0.5707  data: 0.0002  max mem: 6820\n[03:39:41.779466] Epoch: [39]  [860/969]  eta: 0:01:02  lr: 0.000192  loss: 0.6680 (0.6669)  time: 0.5685  data: 0.0002  max mem: 6820\n[03:39:53.121631] Epoch: [39]  [880/969]  eta: 0:00:50  lr: 0.000192  loss: 0.6410 (0.6667)  time: 0.5671  data: 0.0002  max mem: 6820\n[03:40:04.476595] Epoch: [39]  [900/969]  eta: 0:00:39  lr: 0.000192  loss: 0.6564 (0.6666)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:40:15.831011] Epoch: [39]  [920/969]  eta: 0:00:27  lr: 0.000192  loss: 0.6189 (0.6660)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:40:27.182723] Epoch: [39]  [940/969]  eta: 0:00:16  lr: 0.000192  loss: 0.6367 (0.6659)  time: 0.5675  data: 0.0002  max mem: 6820\n[03:40:38.529508] Epoch: [39]  [960/969]  eta: 0:00:05  lr: 0.000191  loss: 0.6526 (0.6657)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:40:43.060882] Epoch: [39]  [968/969]  eta: 0:00:00  lr: 0.000191  loss: 0.6500 (0.6658)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:40:43.181778] Epoch: [39] Total time: 0:09:11 (0.5695 s / it)\n[03:40:43.181896] Averaged stats: lr: 0.000191  loss: 0.6500 (0.6658)\n[03:40:44.290744] val:  [  0/139]  eta: 0:02:33  loss: 0.6362 (0.6362)  time: 1.1040  data: 0.9594  max mem: 6820\n[03:40:45.792994] val:  [ 10/139]  eta: 0:00:30  loss: 0.5775 (0.5439)  time: 0.2367  data: 0.0874  max mem: 6820\n[03:40:47.296672] val:  [ 20/139]  eta: 0:00:23  loss: 0.5146 (0.5349)  time: 0.1501  data: 0.0002  max mem: 6820\n[03:40:48.807023] val:  [ 30/139]  eta: 0:00:19  loss: 0.4987 (0.5301)  time: 0.1506  data: 0.0002  max mem: 6820\n[03:40:50.311676] val:  [ 40/139]  eta: 0:00:17  loss: 0.5900 (0.5635)  time: 0.1507  data: 0.0002  max mem: 6820\n[03:40:51.826017] val:  [ 50/139]  eta: 0:00:15  loss: 0.6195 (0.5764)  time: 0.1509  data: 0.0002  max mem: 6820\n[03:40:53.340752] val:  [ 60/139]  eta: 0:00:13  loss: 0.6108 (0.5824)  time: 0.1514  data: 0.0002  max mem: 6820\n[03:40:54.858289] val:  [ 70/139]  eta: 0:00:11  loss: 0.6206 (0.5926)  time: 0.1515  data: 0.0002  max mem: 6820\n[03:40:56.372147] val:  [ 80/139]  eta: 0:00:09  loss: 0.6306 (0.6082)  time: 0.1515  data: 0.0002  max mem: 6820\n[03:40:57.884453] val:  [ 90/139]  eta: 0:00:07  loss: 0.7484 (0.6254)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:40:59.402095] val:  [100/139]  eta: 0:00:06  loss: 0.7691 (0.6391)  time: 0.1514  data: 0.0002  max mem: 6820\n[03:41:00.915606] val:  [110/139]  eta: 0:00:04  loss: 0.7478 (0.6463)  time: 0.1515  data: 0.0002  max mem: 6820\n[03:41:02.431571] val:  [120/139]  eta: 0:00:03  loss: 0.6794 (0.6438)  time: 0.1514  data: 0.0002  max mem: 6820\n[03:41:03.959834] val:  [130/139]  eta: 0:00:01  loss: 0.5722 (0.6292)  time: 0.1521  data: 0.0001  max mem: 6820\n[03:41:05.081939] val:  [138/139]  eta: 0:00:00  loss: 0.4219 (0.6194)  time: 0.1474  data: 0.0001  max mem: 6820\n[03:41:05.188293] val: Total time: 0:00:22 (0.1583 s / it)\n[03:41:05.245219] val loss: 0.6193666389520219\n[03:41:05.245275] Accuracy: 0.6460, F1 Score: 0.6449, ROC AUC: 0.7075, Hamming Loss: 0.3540,\n Jaccard Score: 0.4762, Precision: 0.6479, Recall: 0.6460,\n Average Precision: 0.7070, Kappa: 0.2920, Score: 0.5482\n[03:41:05.247148] Best epoch = 38, Best score = 0.5541\n[03:41:05.249923] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[03:41:06.551007] Epoch: [40]  [  0/969]  eta: 0:20:59  lr: 0.000191  loss: 0.5810 (0.5810)  time: 1.2995  data: 0.7261  max mem: 6820\n[03:41:17.953684] Epoch: [40]  [ 20/969]  eta: 0:09:33  lr: 0.000191  loss: 0.6402 (0.6526)  time: 0.5701  data: 0.0002  max mem: 6820\n[03:41:29.367024] Epoch: [40]  [ 40/969]  eta: 0:09:06  lr: 0.000191  loss: 0.6697 (0.6609)  time: 0.5706  data: 0.0002  max mem: 6820\n[03:41:40.821557] Epoch: [40]  [ 60/969]  eta: 0:08:50  lr: 0.000191  loss: 0.6239 (0.6521)  time: 0.5727  data: 0.0002  max mem: 6820\n[03:41:52.265649] Epoch: [40]  [ 80/969]  eta: 0:08:35  lr: 0.000191  loss: 0.6479 (0.6588)  time: 0.5722  data: 0.0002  max mem: 6820\n[03:42:03.570763] Epoch: [40]  [100/969]  eta: 0:08:21  lr: 0.000191  loss: 0.6607 (0.6602)  time: 0.5652  data: 0.0002  max mem: 6820\n[03:42:14.959875] Epoch: [40]  [120/969]  eta: 0:08:09  lr: 0.000191  loss: 0.6857 (0.6647)  time: 0.5694  data: 0.0002  max mem: 6820\n[03:42:26.308179] Epoch: [40]  [140/969]  eta: 0:07:56  lr: 0.000190  loss: 0.6407 (0.6642)  time: 0.5674  data: 0.0002  max mem: 6820\n[03:42:37.642281] Epoch: [40]  [160/969]  eta: 0:07:44  lr: 0.000190  loss: 0.6577 (0.6642)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:42:48.981699] Epoch: [40]  [180/969]  eta: 0:07:32  lr: 0.000190  loss: 0.6500 (0.6632)  time: 0.5669  data: 0.0002  max mem: 6820\n[03:43:00.325939] Epoch: [40]  [200/969]  eta: 0:07:20  lr: 0.000190  loss: 0.6735 (0.6646)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:43:11.685965] Epoch: [40]  [220/969]  eta: 0:07:08  lr: 0.000190  loss: 0.6605 (0.6644)  time: 0.5679  data: 0.0002  max mem: 6820\n[03:43:23.041587] Epoch: [40]  [240/969]  eta: 0:06:56  lr: 0.000190  loss: 0.6740 (0.6655)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:43:34.411358] Epoch: [40]  [260/969]  eta: 0:06:45  lr: 0.000190  loss: 0.6502 (0.6652)  time: 0.5684  data: 0.0001  max mem: 6820\n[03:43:45.787107] Epoch: [40]  [280/969]  eta: 0:06:33  lr: 0.000189  loss: 0.6752 (0.6666)  time: 0.5687  data: 0.0002  max mem: 6820\n[03:43:57.177592] Epoch: [40]  [300/969]  eta: 0:06:22  lr: 0.000189  loss: 0.6762 (0.6673)  time: 0.5695  data: 0.0002  max mem: 6820\n[03:44:08.567422] Epoch: [40]  [320/969]  eta: 0:06:10  lr: 0.000189  loss: 0.6545 (0.6670)  time: 0.5694  data: 0.0002  max mem: 6820\n[03:44:19.955184] Epoch: [40]  [340/969]  eta: 0:05:59  lr: 0.000189  loss: 0.6639 (0.6674)  time: 0.5693  data: 0.0002  max mem: 6820\n[03:44:31.337611] Epoch: [40]  [360/969]  eta: 0:05:47  lr: 0.000189  loss: 0.6941 (0.6685)  time: 0.5691  data: 0.0002  max mem: 6820\n[03:44:42.718519] Epoch: [40]  [380/969]  eta: 0:05:36  lr: 0.000189  loss: 0.6516 (0.6687)  time: 0.5690  data: 0.0002  max mem: 6820\n[03:44:54.073090] Epoch: [40]  [400/969]  eta: 0:05:24  lr: 0.000189  loss: 0.6530 (0.6673)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:45:05.420284] Epoch: [40]  [420/969]  eta: 0:05:13  lr: 0.000188  loss: 0.6569 (0.6667)  time: 0.5673  data: 0.0002  max mem: 6820\n[03:45:16.782107] Epoch: [40]  [440/969]  eta: 0:05:01  lr: 0.000188  loss: 0.6587 (0.6671)  time: 0.5680  data: 0.0002  max mem: 6820\n[03:45:28.135287] Epoch: [40]  [460/969]  eta: 0:04:50  lr: 0.000188  loss: 0.6329 (0.6665)  time: 0.5676  data: 0.0002  max mem: 6820\n[03:45:39.488884] Epoch: [40]  [480/969]  eta: 0:04:38  lr: 0.000188  loss: 0.6790 (0.6671)  time: 0.5676  data: 0.0002  max mem: 6820\n[03:45:50.846512] Epoch: [40]  [500/969]  eta: 0:04:27  lr: 0.000188  loss: 0.6636 (0.6671)  time: 0.5678  data: 0.0002  max mem: 6820\n[03:46:02.179678] Epoch: [40]  [520/969]  eta: 0:04:15  lr: 0.000188  loss: 0.6249 (0.6659)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:46:13.536484] Epoch: [40]  [540/969]  eta: 0:04:04  lr: 0.000188  loss: 0.6145 (0.6647)  time: 0.5678  data: 0.0002  max mem: 6820\n[03:46:24.931378] Epoch: [40]  [560/969]  eta: 0:03:53  lr: 0.000187  loss: 0.6717 (0.6649)  time: 0.5697  data: 0.0002  max mem: 6820\n[03:46:36.320940] Epoch: [40]  [580/969]  eta: 0:03:41  lr: 0.000187  loss: 0.6673 (0.6655)  time: 0.5694  data: 0.0002  max mem: 6820\n[03:46:47.732622] Epoch: [40]  [600/969]  eta: 0:03:30  lr: 0.000187  loss: 0.6476 (0.6651)  time: 0.5705  data: 0.0002  max mem: 6820\n[03:46:59.143567] Epoch: [40]  [620/969]  eta: 0:03:18  lr: 0.000187  loss: 0.6577 (0.6648)  time: 0.5705  data: 0.0002  max mem: 6820\n[03:47:10.544492] Epoch: [40]  [640/969]  eta: 0:03:07  lr: 0.000187  loss: 0.6500 (0.6647)  time: 0.5700  data: 0.0002  max mem: 6820\n[03:47:21.961509] Epoch: [40]  [660/969]  eta: 0:02:56  lr: 0.000187  loss: 0.6869 (0.6653)  time: 0.5708  data: 0.0002  max mem: 6820\n[03:47:33.391062] Epoch: [40]  [680/969]  eta: 0:02:44  lr: 0.000187  loss: 0.6541 (0.6655)  time: 0.5714  data: 0.0002  max mem: 6820\n[03:47:44.799484] Epoch: [40]  [700/969]  eta: 0:02:33  lr: 0.000186  loss: 0.6640 (0.6652)  time: 0.5704  data: 0.0002  max mem: 6820\n[03:47:56.189661] Epoch: [40]  [720/969]  eta: 0:02:21  lr: 0.000186  loss: 0.6710 (0.6654)  time: 0.5694  data: 0.0002  max mem: 6820\n[03:48:07.577275] Epoch: [40]  [740/969]  eta: 0:02:10  lr: 0.000186  loss: 0.6232 (0.6644)  time: 0.5693  data: 0.0002  max mem: 6820\n[03:48:18.947997] Epoch: [40]  [760/969]  eta: 0:01:59  lr: 0.000186  loss: 0.6531 (0.6646)  time: 0.5685  data: 0.0002  max mem: 6820\n[03:48:30.309554] Epoch: [40]  [780/969]  eta: 0:01:47  lr: 0.000186  loss: 0.6538 (0.6644)  time: 0.5680  data: 0.0002  max mem: 6820\n[03:48:41.683088] Epoch: [40]  [800/969]  eta: 0:01:36  lr: 0.000186  loss: 0.6947 (0.6651)  time: 0.5686  data: 0.0002  max mem: 6820\n[03:48:53.047627] Epoch: [40]  [820/969]  eta: 0:01:24  lr: 0.000186  loss: 0.6428 (0.6649)  time: 0.5682  data: 0.0002  max mem: 6820\n[03:49:04.392289] Epoch: [40]  [840/969]  eta: 0:01:13  lr: 0.000185  loss: 0.6192 (0.6645)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:49:15.746777] Epoch: [40]  [860/969]  eta: 0:01:02  lr: 0.000185  loss: 0.6644 (0.6645)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:49:27.081314] Epoch: [40]  [880/969]  eta: 0:00:50  lr: 0.000185  loss: 0.6335 (0.6644)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:49:38.420105] Epoch: [40]  [900/969]  eta: 0:00:39  lr: 0.000185  loss: 0.6611 (0.6643)  time: 0.5669  data: 0.0002  max mem: 6820\n[03:49:49.750108] Epoch: [40]  [920/969]  eta: 0:00:27  lr: 0.000185  loss: 0.6438 (0.6642)  time: 0.5665  data: 0.0002  max mem: 6820\n[03:50:01.112356] Epoch: [40]  [940/969]  eta: 0:00:16  lr: 0.000185  loss: 0.6476 (0.6640)  time: 0.5681  data: 0.0002  max mem: 6820\n[03:50:12.453609] Epoch: [40]  [960/969]  eta: 0:00:05  lr: 0.000185  loss: 0.6614 (0.6642)  time: 0.5670  data: 0.0002  max mem: 6820\n[03:50:16.996830] Epoch: [40]  [968/969]  eta: 0:00:00  lr: 0.000185  loss: 0.6533 (0.6643)  time: 0.5669  data: 0.0001  max mem: 6820\n[03:50:17.120254] Epoch: [40] Total time: 0:09:11 (0.5695 s / it)\n[03:50:17.120363] Averaged stats: lr: 0.000185  loss: 0.6533 (0.6643)\n[03:50:18.007171] val:  [  0/139]  eta: 0:02:02  loss: 0.6106 (0.6106)  time: 0.8823  data: 0.7545  max mem: 6820\n[03:50:19.502066] val:  [ 10/139]  eta: 0:00:27  loss: 0.5703 (0.5541)  time: 0.2160  data: 0.0688  max mem: 6820\n[03:50:20.999537] val:  [ 20/139]  eta: 0:00:21  loss: 0.5639 (0.5433)  time: 0.1495  data: 0.0002  max mem: 6820\n[03:50:22.503430] val:  [ 30/139]  eta: 0:00:18  loss: 0.5078 (0.5451)  time: 0.1500  data: 0.0002  max mem: 6820\n[03:50:24.006590] val:  [ 40/139]  eta: 0:00:16  loss: 0.6083 (0.5766)  time: 0.1503  data: 0.0002  max mem: 6820\n[03:50:25.515886] val:  [ 50/139]  eta: 0:00:14  loss: 0.6409 (0.5906)  time: 0.1505  data: 0.0002  max mem: 6820\n[03:50:27.028619] val:  [ 60/139]  eta: 0:00:12  loss: 0.6409 (0.5975)  time: 0.1510  data: 0.0002  max mem: 6820\n[03:50:28.537821] val:  [ 70/139]  eta: 0:00:11  loss: 0.6318 (0.6053)  time: 0.1510  data: 0.0002  max mem: 6820\n[03:50:30.046877] val:  [ 80/139]  eta: 0:00:09  loss: 0.6389 (0.6174)  time: 0.1508  data: 0.0002  max mem: 6820\n[03:50:31.555760] val:  [ 90/139]  eta: 0:00:07  loss: 0.7224 (0.6306)  time: 0.1508  data: 0.0002  max mem: 6820\n[03:50:33.069057] val:  [100/139]  eta: 0:00:06  loss: 0.7287 (0.6405)  time: 0.1510  data: 0.0002  max mem: 6820\n[03:50:34.581250] val:  [110/139]  eta: 0:00:04  loss: 0.7287 (0.6453)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:50:36.094805] val:  [120/139]  eta: 0:00:02  loss: 0.6041 (0.6407)  time: 0.1512  data: 0.0002  max mem: 6820\n[03:50:37.610271] val:  [130/139]  eta: 0:00:01  loss: 0.5680 (0.6283)  time: 0.1514  data: 0.0001  max mem: 6820\n[03:50:38.719790] val:  [138/139]  eta: 0:00:00  loss: 0.4501 (0.6176)  time: 0.1462  data: 0.0001  max mem: 6820\n[03:50:38.825278] val: Total time: 0:00:21 (0.1561 s / it)\n[03:50:38.880753] val loss: 0.6175525034074303\n[03:50:38.880808] Accuracy: 0.6546, F1 Score: 0.6535, ROC AUC: 0.7141, Hamming Loss: 0.3454,\n Jaccard Score: 0.4856, Precision: 0.6566, Recall: 0.6546,\n Average Precision: 0.7107, Kappa: 0.3092, Score: 0.5589\n[03:50:41.644387] Best epoch = 40, Best score = 0.5589\n[03:50:41.649231] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[03:50:43.262550] Epoch: [41]  [  0/969]  eta: 0:26:01  lr: 0.000185  loss: 0.6005 (0.6005)  time: 1.6114  data: 0.9848  max mem: 6820\n[03:50:54.542845] Epoch: [41]  [ 20/969]  eta: 0:09:42  lr: 0.000184  loss: 0.6550 (0.6654)  time: 0.5640  data: 0.0002  max mem: 6820\n[03:51:06.018738] Epoch: [41]  [ 40/969]  eta: 0:09:12  lr: 0.000184  loss: 0.6535 (0.6597)  time: 0.5737  data: 0.0002  max mem: 6820\n[03:51:17.562255] Epoch: [41]  [ 60/969]  eta: 0:08:55  lr: 0.000184  loss: 0.6504 (0.6587)  time: 0.5771  data: 0.0002  max mem: 6820\n[03:51:29.045747] Epoch: [41]  [ 80/969]  eta: 0:08:40  lr: 0.000184  loss: 0.6556 (0.6608)  time: 0.5741  data: 0.0002  max mem: 6820\n[03:51:40.432264] Epoch: [41]  [100/969]  eta: 0:08:25  lr: 0.000184  loss: 0.6723 (0.6664)  time: 0.5693  data: 0.0002  max mem: 6820\n[03:51:51.765334] Epoch: [41]  [120/969]  eta: 0:08:11  lr: 0.000184  loss: 0.6643 (0.6674)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:52:03.071620] Epoch: [41]  [140/969]  eta: 0:07:58  lr: 0.000184  loss: 0.6768 (0.6682)  time: 0.5653  data: 0.0002  max mem: 6820\n[03:52:14.403378] Epoch: [41]  [160/969]  eta: 0:07:46  lr: 0.000183  loss: 0.6613 (0.6696)  time: 0.5665  data: 0.0002  max mem: 6820\n[03:52:25.746649] Epoch: [41]  [180/969]  eta: 0:07:33  lr: 0.000183  loss: 0.6731 (0.6707)  time: 0.5671  data: 0.0002  max mem: 6820\n[03:52:37.098356] Epoch: [41]  [200/969]  eta: 0:07:21  lr: 0.000183  loss: 0.6579 (0.6709)  time: 0.5675  data: 0.0001  max mem: 6820\n[03:52:48.474950] Epoch: [41]  [220/969]  eta: 0:07:09  lr: 0.000183  loss: 0.6784 (0.6720)  time: 0.5688  data: 0.0002  max mem: 6820\n[03:52:59.890766] Epoch: [41]  [240/969]  eta: 0:06:58  lr: 0.000183  loss: 0.6579 (0.6718)  time: 0.5707  data: 0.0002  max mem: 6820\n[03:53:11.296618] Epoch: [41]  [260/969]  eta: 0:06:46  lr: 0.000183  loss: 0.6748 (0.6719)  time: 0.5702  data: 0.0002  max mem: 6820\n[03:53:22.694141] Epoch: [41]  [280/969]  eta: 0:06:34  lr: 0.000183  loss: 0.6893 (0.6726)  time: 0.5698  data: 0.0002  max mem: 6820\n[03:53:34.123863] Epoch: [41]  [300/969]  eta: 0:06:23  lr: 0.000182  loss: 0.6636 (0.6724)  time: 0.5714  data: 0.0002  max mem: 6820\n[03:53:45.527934] Epoch: [41]  [320/969]  eta: 0:06:11  lr: 0.000182  loss: 0.6512 (0.6711)  time: 0.5702  data: 0.0002  max mem: 6820\n[03:53:56.944744] Epoch: [41]  [340/969]  eta: 0:06:00  lr: 0.000182  loss: 0.6479 (0.6709)  time: 0.5708  data: 0.0002  max mem: 6820\n[03:54:08.324162] Epoch: [41]  [360/969]  eta: 0:05:48  lr: 0.000182  loss: 0.6704 (0.6713)  time: 0.5689  data: 0.0002  max mem: 6820\n[03:54:19.706304] Epoch: [41]  [380/969]  eta: 0:05:37  lr: 0.000182  loss: 0.6417 (0.6701)  time: 0.5691  data: 0.0002  max mem: 6820\n[03:54:31.075460] Epoch: [41]  [400/969]  eta: 0:05:25  lr: 0.000182  loss: 0.6353 (0.6693)  time: 0.5684  data: 0.0002  max mem: 6820\n[03:54:42.439140] Epoch: [41]  [420/969]  eta: 0:05:13  lr: 0.000182  loss: 0.6307 (0.6689)  time: 0.5681  data: 0.0002  max mem: 6820\n[03:54:53.793672] Epoch: [41]  [440/969]  eta: 0:05:02  lr: 0.000181  loss: 0.6734 (0.6687)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:55:05.138773] Epoch: [41]  [460/969]  eta: 0:04:50  lr: 0.000181  loss: 0.6717 (0.6687)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:55:16.484027] Epoch: [41]  [480/969]  eta: 0:04:39  lr: 0.000181  loss: 0.6598 (0.6690)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:55:27.829681] Epoch: [41]  [500/969]  eta: 0:04:27  lr: 0.000181  loss: 0.6331 (0.6682)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:55:39.153212] Epoch: [41]  [520/969]  eta: 0:04:16  lr: 0.000181  loss: 0.6480 (0.6677)  time: 0.5661  data: 0.0002  max mem: 6820\n[03:55:50.485387] Epoch: [41]  [540/969]  eta: 0:04:04  lr: 0.000181  loss: 0.6654 (0.6672)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:56:01.813791] Epoch: [41]  [560/969]  eta: 0:03:53  lr: 0.000181  loss: 0.6276 (0.6669)  time: 0.5664  data: 0.0002  max mem: 6820\n[03:56:13.154839] Epoch: [41]  [580/969]  eta: 0:03:41  lr: 0.000180  loss: 0.6416 (0.6669)  time: 0.5670  data: 0.0002  max mem: 6820\n[03:56:24.498557] Epoch: [41]  [600/969]  eta: 0:03:30  lr: 0.000180  loss: 0.6864 (0.6678)  time: 0.5671  data: 0.0002  max mem: 6820\n[03:56:35.875312] Epoch: [41]  [620/969]  eta: 0:03:19  lr: 0.000180  loss: 0.6568 (0.6676)  time: 0.5688  data: 0.0002  max mem: 6820\n[03:56:47.270608] Epoch: [41]  [640/969]  eta: 0:03:07  lr: 0.000180  loss: 0.6602 (0.6676)  time: 0.5697  data: 0.0002  max mem: 6820\n[03:56:58.659819] Epoch: [41]  [660/969]  eta: 0:02:56  lr: 0.000180  loss: 0.6737 (0.6676)  time: 0.5694  data: 0.0002  max mem: 6820\n[03:57:10.091453] Epoch: [41]  [680/969]  eta: 0:02:44  lr: 0.000180  loss: 0.6819 (0.6681)  time: 0.5715  data: 0.0002  max mem: 6820\n[03:57:21.471457] Epoch: [41]  [700/969]  eta: 0:02:33  lr: 0.000180  loss: 0.6613 (0.6677)  time: 0.5690  data: 0.0002  max mem: 6820\n[03:57:32.861250] Epoch: [41]  [720/969]  eta: 0:02:22  lr: 0.000179  loss: 0.6589 (0.6677)  time: 0.5694  data: 0.0002  max mem: 6820\n[03:57:44.260342] Epoch: [41]  [740/969]  eta: 0:02:10  lr: 0.000179  loss: 0.6630 (0.6675)  time: 0.5699  data: 0.0002  max mem: 6820\n[03:57:55.645043] Epoch: [41]  [760/969]  eta: 0:01:59  lr: 0.000179  loss: 0.6643 (0.6674)  time: 0.5692  data: 0.0002  max mem: 6820\n[03:58:07.042055] Epoch: [41]  [780/969]  eta: 0:01:47  lr: 0.000179  loss: 0.6795 (0.6676)  time: 0.5698  data: 0.0002  max mem: 6820\n[03:58:18.401855] Epoch: [41]  [800/969]  eta: 0:01:36  lr: 0.000179  loss: 0.6788 (0.6679)  time: 0.5679  data: 0.0002  max mem: 6820\n[03:58:29.750450] Epoch: [41]  [820/969]  eta: 0:01:24  lr: 0.000179  loss: 0.6642 (0.6680)  time: 0.5674  data: 0.0002  max mem: 6820\n[03:58:41.086165] Epoch: [41]  [840/969]  eta: 0:01:13  lr: 0.000179  loss: 0.6482 (0.6676)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:58:52.430858] Epoch: [41]  [860/969]  eta: 0:01:02  lr: 0.000178  loss: 0.6615 (0.6676)  time: 0.5672  data: 0.0002  max mem: 6820\n[03:59:03.765772] Epoch: [41]  [880/969]  eta: 0:00:50  lr: 0.000178  loss: 0.6674 (0.6677)  time: 0.5667  data: 0.0002  max mem: 6820\n[03:59:15.106640] Epoch: [41]  [900/969]  eta: 0:00:39  lr: 0.000178  loss: 0.6745 (0.6679)  time: 0.5670  data: 0.0002  max mem: 6820\n[03:59:26.440278] Epoch: [41]  [920/969]  eta: 0:00:27  lr: 0.000178  loss: 0.6481 (0.6677)  time: 0.5666  data: 0.0002  max mem: 6820\n[03:59:37.796165] Epoch: [41]  [940/969]  eta: 0:00:16  lr: 0.000178  loss: 0.6422 (0.6671)  time: 0.5677  data: 0.0002  max mem: 6820\n[03:59:49.196399] Epoch: [41]  [960/969]  eta: 0:00:05  lr: 0.000178  loss: 0.6672 (0.6672)  time: 0.5700  data: 0.0002  max mem: 6820\n[03:59:53.750042] Epoch: [41]  [968/969]  eta: 0:00:00  lr: 0.000178  loss: 0.6575 (0.6672)  time: 0.5703  data: 0.0002  max mem: 6820\n[03:59:53.872375] Epoch: [41] Total time: 0:09:12 (0.5699 s / it)\n[03:59:53.872477] Averaged stats: lr: 0.000178  loss: 0.6575 (0.6672)\n[03:59:54.848576] val:  [  0/139]  eta: 0:02:15  loss: 0.7365 (0.7365)  time: 0.9718  data: 0.8130  max mem: 6820\n[03:59:56.362653] val:  [ 10/139]  eta: 0:00:29  loss: 0.7081 (0.6528)  time: 0.2259  data: 0.0741  max mem: 6820\n[03:59:57.868987] val:  [ 20/139]  eta: 0:00:22  loss: 0.5825 (0.6264)  time: 0.1509  data: 0.0002  max mem: 6820\n[03:59:59.381987] val:  [ 30/139]  eta: 0:00:19  loss: 0.5739 (0.6199)  time: 0.1509  data: 0.0002  max mem: 6820\n[04:00:00.901905] val:  [ 40/139]  eta: 0:00:16  loss: 0.7083 (0.6555)  time: 0.1516  data: 0.0002  max mem: 6820\n[04:00:02.420609] val:  [ 50/139]  eta: 0:00:14  loss: 0.7479 (0.6715)  time: 0.1519  data: 0.0002  max mem: 6820\n[04:00:03.938806] val:  [ 60/139]  eta: 0:00:13  loss: 0.7040 (0.6741)  time: 0.1518  data: 0.0002  max mem: 6820\n[04:00:05.452215] val:  [ 70/139]  eta: 0:00:11  loss: 0.6650 (0.6723)  time: 0.1515  data: 0.0002  max mem: 6820\n[04:00:06.964936] val:  [ 80/139]  eta: 0:00:09  loss: 0.6386 (0.6671)  time: 0.1512  data: 0.0002  max mem: 6820\n[04:00:08.483929] val:  [ 90/139]  eta: 0:00:07  loss: 0.6779 (0.6704)  time: 0.1515  data: 0.0002  max mem: 6820\n[04:00:10.005424] val:  [100/139]  eta: 0:00:06  loss: 0.6910 (0.6716)  time: 0.1519  data: 0.0002  max mem: 6820\n[04:00:11.524681] val:  [110/139]  eta: 0:00:04  loss: 0.6626 (0.6684)  time: 0.1520  data: 0.0002  max mem: 6820\n[04:00:13.046475] val:  [120/139]  eta: 0:00:03  loss: 0.5812 (0.6569)  time: 0.1520  data: 0.0002  max mem: 6820\n[04:00:14.567915] val:  [130/139]  eta: 0:00:01  loss: 0.4950 (0.6365)  time: 0.1521  data: 0.0001  max mem: 6820\n[04:00:15.686319] val:  [138/139]  eta: 0:00:00  loss: 0.3739 (0.6220)  time: 0.1471  data: 0.0001  max mem: 6820\n[04:00:15.789032] val: Total time: 0:00:21 (0.1577 s / it)\n[04:00:15.845920] val loss: 0.621987596690226\n[04:00:15.846431] Accuracy: 0.6420, F1 Score: 0.6414, ROC AUC: 0.7065, Hamming Loss: 0.3580,\n Jaccard Score: 0.4722, Precision: 0.6429, Recall: 0.6420,\n Average Precision: 0.7060, Kappa: 0.2839, Score: 0.5439\n[04:00:15.847994] Best epoch = 40, Best score = 0.5589\n[04:00:15.850381] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[04:00:17.276026] Epoch: [42]  [  0/969]  eta: 0:23:00  lr: 0.000178  loss: 0.6566 (0.6566)  time: 1.4244  data: 0.8068  max mem: 6820\n[04:00:28.659808] Epoch: [42]  [ 20/969]  eta: 0:09:38  lr: 0.000178  loss: 0.6793 (0.6734)  time: 0.5691  data: 0.0002  max mem: 6820\n[04:00:40.093632] Epoch: [42]  [ 40/969]  eta: 0:09:09  lr: 0.000177  loss: 0.6877 (0.6804)  time: 0.5716  data: 0.0002  max mem: 6820\n[04:00:51.529392] Epoch: [42]  [ 60/969]  eta: 0:08:51  lr: 0.000177  loss: 0.6588 (0.6767)  time: 0.5717  data: 0.0002  max mem: 6820\n[04:01:02.966387] Epoch: [42]  [ 80/969]  eta: 0:08:37  lr: 0.000177  loss: 0.6626 (0.6732)  time: 0.5718  data: 0.0001  max mem: 6820\n[04:01:14.401141] Epoch: [42]  [100/969]  eta: 0:08:23  lr: 0.000177  loss: 0.6764 (0.6746)  time: 0.5717  data: 0.0002  max mem: 6820\n[04:01:25.820532] Epoch: [42]  [120/969]  eta: 0:08:10  lr: 0.000177  loss: 0.6935 (0.6772)  time: 0.5709  data: 0.0002  max mem: 6820\n[04:01:37.193530] Epoch: [42]  [140/969]  eta: 0:07:58  lr: 0.000177  loss: 0.6697 (0.6769)  time: 0.5686  data: 0.0002  max mem: 6820\n[04:01:48.558128] Epoch: [42]  [160/969]  eta: 0:07:45  lr: 0.000177  loss: 0.6485 (0.6738)  time: 0.5682  data: 0.0002  max mem: 6820\n[04:01:59.842096] Epoch: [42]  [180/969]  eta: 0:07:33  lr: 0.000176  loss: 0.6489 (0.6723)  time: 0.5642  data: 0.0002  max mem: 6820\n[04:02:11.212726] Epoch: [42]  [200/969]  eta: 0:07:21  lr: 0.000176  loss: 0.6932 (0.6735)  time: 0.5685  data: 0.0002  max mem: 6820\n[04:02:22.566658] Epoch: [42]  [220/969]  eta: 0:07:09  lr: 0.000176  loss: 0.6628 (0.6737)  time: 0.5677  data: 0.0002  max mem: 6820\n[04:02:33.908551] Epoch: [42]  [240/969]  eta: 0:06:57  lr: 0.000176  loss: 0.6800 (0.6750)  time: 0.5670  data: 0.0002  max mem: 6820\n[04:02:45.235048] Epoch: [42]  [260/969]  eta: 0:06:45  lr: 0.000176  loss: 0.6697 (0.6748)  time: 0.5663  data: 0.0001  max mem: 6820\n[04:02:56.591721] Epoch: [42]  [280/969]  eta: 0:06:34  lr: 0.000176  loss: 0.6695 (0.6750)  time: 0.5678  data: 0.0002  max mem: 6820\n[04:03:07.943821] Epoch: [42]  [300/969]  eta: 0:06:22  lr: 0.000176  loss: 0.6531 (0.6746)  time: 0.5676  data: 0.0002  max mem: 6820\n[04:03:19.291140] Epoch: [42]  [320/969]  eta: 0:06:10  lr: 0.000175  loss: 0.6663 (0.6739)  time: 0.5673  data: 0.0002  max mem: 6820\n[04:03:30.644493] Epoch: [42]  [340/969]  eta: 0:05:59  lr: 0.000175  loss: 0.6625 (0.6725)  time: 0.5676  data: 0.0002  max mem: 6820\n[04:03:41.985438] Epoch: [42]  [360/969]  eta: 0:05:47  lr: 0.000175  loss: 0.6894 (0.6732)  time: 0.5670  data: 0.0002  max mem: 6820\n[04:03:53.314938] Epoch: [42]  [380/969]  eta: 0:05:36  lr: 0.000175  loss: 0.6559 (0.6731)  time: 0.5664  data: 0.0002  max mem: 6820\n[04:04:04.657963] Epoch: [42]  [400/969]  eta: 0:05:24  lr: 0.000175  loss: 0.6433 (0.6721)  time: 0.5671  data: 0.0002  max mem: 6820\n[04:04:15.986639] Epoch: [42]  [420/969]  eta: 0:05:13  lr: 0.000175  loss: 0.6546 (0.6716)  time: 0.5664  data: 0.0002  max mem: 6820\n[04:04:27.335087] Epoch: [42]  [440/969]  eta: 0:05:01  lr: 0.000175  loss: 0.6680 (0.6714)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:04:38.682142] Epoch: [42]  [460/969]  eta: 0:04:50  lr: 0.000174  loss: 0.6625 (0.6703)  time: 0.5673  data: 0.0002  max mem: 6820\n[04:04:50.023448] Epoch: [42]  [480/969]  eta: 0:04:38  lr: 0.000174  loss: 0.6519 (0.6704)  time: 0.5670  data: 0.0001  max mem: 6820\n[04:05:01.376763] Epoch: [42]  [500/969]  eta: 0:04:27  lr: 0.000174  loss: 0.6597 (0.6701)  time: 0.5676  data: 0.0002  max mem: 6820\n[04:05:12.709877] Epoch: [42]  [520/969]  eta: 0:04:15  lr: 0.000174  loss: 0.6637 (0.6697)  time: 0.5666  data: 0.0002  max mem: 6820\n[04:05:24.049622] Epoch: [42]  [540/969]  eta: 0:04:04  lr: 0.000174  loss: 0.6512 (0.6695)  time: 0.5669  data: 0.0002  max mem: 6820\n[04:05:35.380077] Epoch: [42]  [560/969]  eta: 0:03:52  lr: 0.000174  loss: 0.6434 (0.6690)  time: 0.5665  data: 0.0001  max mem: 6820\n[04:05:46.742634] Epoch: [42]  [580/969]  eta: 0:03:41  lr: 0.000174  loss: 0.6456 (0.6684)  time: 0.5681  data: 0.0002  max mem: 6820\n[04:05:58.092403] Epoch: [42]  [600/969]  eta: 0:03:30  lr: 0.000173  loss: 0.6661 (0.6686)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:06:09.427653] Epoch: [42]  [620/969]  eta: 0:03:18  lr: 0.000173  loss: 0.6595 (0.6680)  time: 0.5667  data: 0.0002  max mem: 6820\n[04:06:20.779065] Epoch: [42]  [640/969]  eta: 0:03:07  lr: 0.000173  loss: 0.6609 (0.6680)  time: 0.5675  data: 0.0002  max mem: 6820\n[04:06:32.125634] Epoch: [42]  [660/969]  eta: 0:02:55  lr: 0.000173  loss: 0.6527 (0.6681)  time: 0.5673  data: 0.0002  max mem: 6820\n[04:06:43.456166] Epoch: [42]  [680/969]  eta: 0:02:44  lr: 0.000173  loss: 0.6751 (0.6684)  time: 0.5665  data: 0.0002  max mem: 6820\n[04:06:54.798961] Epoch: [42]  [700/969]  eta: 0:02:33  lr: 0.000173  loss: 0.6448 (0.6679)  time: 0.5671  data: 0.0002  max mem: 6820\n[04:07:06.144790] Epoch: [42]  [720/969]  eta: 0:02:21  lr: 0.000172  loss: 0.6708 (0.6681)  time: 0.5672  data: 0.0002  max mem: 6820\n[04:07:17.483217] Epoch: [42]  [740/969]  eta: 0:02:10  lr: 0.000172  loss: 0.6630 (0.6680)  time: 0.5669  data: 0.0002  max mem: 6820\n[04:07:28.841655] Epoch: [42]  [760/969]  eta: 0:01:58  lr: 0.000172  loss: 0.6707 (0.6679)  time: 0.5679  data: 0.0002  max mem: 6820\n[04:07:40.206844] Epoch: [42]  [780/969]  eta: 0:01:47  lr: 0.000172  loss: 0.6738 (0.6680)  time: 0.5682  data: 0.0002  max mem: 6820\n[04:07:51.602666] Epoch: [42]  [800/969]  eta: 0:01:36  lr: 0.000172  loss: 0.6954 (0.6689)  time: 0.5697  data: 0.0002  max mem: 6820\n[04:08:02.992657] Epoch: [42]  [820/969]  eta: 0:01:24  lr: 0.000172  loss: 0.6564 (0.6686)  time: 0.5695  data: 0.0002  max mem: 6820\n[04:08:14.376498] Epoch: [42]  [840/969]  eta: 0:01:13  lr: 0.000172  loss: 0.6493 (0.6684)  time: 0.5691  data: 0.0002  max mem: 6820\n[04:08:25.804278] Epoch: [42]  [860/969]  eta: 0:01:02  lr: 0.000171  loss: 0.6646 (0.6684)  time: 0.5713  data: 0.0002  max mem: 6820\n[04:08:37.202507] Epoch: [42]  [880/969]  eta: 0:00:50  lr: 0.000171  loss: 0.6476 (0.6681)  time: 0.5699  data: 0.0002  max mem: 6820\n[04:08:48.591062] Epoch: [42]  [900/969]  eta: 0:00:39  lr: 0.000171  loss: 0.6857 (0.6687)  time: 0.5694  data: 0.0002  max mem: 6820\n[04:08:59.976066] Epoch: [42]  [920/969]  eta: 0:00:27  lr: 0.000171  loss: 0.6413 (0.6682)  time: 0.5692  data: 0.0002  max mem: 6820\n[04:09:11.372526] Epoch: [42]  [940/969]  eta: 0:00:16  lr: 0.000171  loss: 0.6544 (0.6679)  time: 0.5698  data: 0.0002  max mem: 6820\n[04:09:22.758030] Epoch: [42]  [960/969]  eta: 0:00:05  lr: 0.000171  loss: 0.6508 (0.6676)  time: 0.5692  data: 0.0002  max mem: 6820\n[04:09:27.318962] Epoch: [42]  [968/969]  eta: 0:00:00  lr: 0.000171  loss: 0.6646 (0.6678)  time: 0.5693  data: 0.0001  max mem: 6820\n[04:09:27.462063] Epoch: [42] Total time: 0:09:11 (0.5693 s / it)\n[04:09:27.462173] Averaged stats: lr: 0.000171  loss: 0.6646 (0.6678)\n[04:09:28.332582] val:  [  0/139]  eta: 0:02:00  loss: 0.6912 (0.6912)  time: 0.8642  data: 0.7306  max mem: 6820\n[04:09:29.837479] val:  [ 10/139]  eta: 0:00:27  loss: 0.6622 (0.6152)  time: 0.2153  data: 0.0666  max mem: 6820\n[04:09:31.341474] val:  [ 20/139]  eta: 0:00:21  loss: 0.5425 (0.5869)  time: 0.1504  data: 0.0002  max mem: 6820\n[04:09:32.847767] val:  [ 30/139]  eta: 0:00:18  loss: 0.5175 (0.5768)  time: 0.1504  data: 0.0002  max mem: 6820\n[04:09:34.364357] val:  [ 40/139]  eta: 0:00:16  loss: 0.6597 (0.6062)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:09:35.877028] val:  [ 50/139]  eta: 0:00:14  loss: 0.6672 (0.6172)  time: 0.1514  data: 0.0002  max mem: 6820\n[04:09:37.391472] val:  [ 60/139]  eta: 0:00:12  loss: 0.6303 (0.6189)  time: 0.1513  data: 0.0002  max mem: 6820\n[04:09:38.895834] val:  [ 70/139]  eta: 0:00:11  loss: 0.6303 (0.6209)  time: 0.1509  data: 0.0002  max mem: 6820\n[04:09:40.408967] val:  [ 80/139]  eta: 0:00:09  loss: 0.6381 (0.6265)  time: 0.1508  data: 0.0002  max mem: 6820\n[04:09:41.922557] val:  [ 90/139]  eta: 0:00:07  loss: 0.7256 (0.6403)  time: 0.1513  data: 0.0002  max mem: 6820\n[04:09:43.446495] val:  [100/139]  eta: 0:00:06  loss: 0.7324 (0.6508)  time: 0.1518  data: 0.0002  max mem: 6820\n[04:09:44.959563] val:  [110/139]  eta: 0:00:04  loss: 0.7302 (0.6547)  time: 0.1518  data: 0.0002  max mem: 6820\n[04:09:46.473526] val:  [120/139]  eta: 0:00:02  loss: 0.6136 (0.6476)  time: 0.1513  data: 0.0002  max mem: 6820\n[04:09:47.990522] val:  [130/139]  eta: 0:00:01  loss: 0.5317 (0.6305)  time: 0.1515  data: 0.0001  max mem: 6820\n[04:09:49.109478] val:  [138/139]  eta: 0:00:00  loss: 0.3826 (0.6191)  time: 0.1469  data: 0.0001  max mem: 6820\n[04:09:49.220912] val: Total time: 0:00:21 (0.1565 s / it)\n[04:09:49.279575] val loss: 0.6191249819968244\n[04:09:49.279947] Accuracy: 0.6433, F1 Score: 0.6433, ROC AUC: 0.7086, Hamming Loss: 0.3567,\n Jaccard Score: 0.4741, Precision: 0.6434, Recall: 0.6433,\n Average Precision: 0.7081, Kappa: 0.2866, Score: 0.5462\n[04:09:49.281436] Best epoch = 40, Best score = 0.5589\n[04:09:49.283963] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[04:09:50.800593] Epoch: [43]  [  0/969]  eta: 0:24:28  lr: 0.000171  loss: 0.5991 (0.5991)  time: 1.5153  data: 0.9246  max mem: 6820\n[04:10:02.179744] Epoch: [43]  [ 20/969]  eta: 0:09:42  lr: 0.000171  loss: 0.6253 (0.6209)  time: 0.5689  data: 0.0002  max mem: 6820\n[04:10:13.580124] Epoch: [43]  [ 40/969]  eta: 0:09:10  lr: 0.000170  loss: 0.6866 (0.6501)  time: 0.5700  data: 0.0002  max mem: 6820\n[04:10:25.001853] Epoch: [43]  [ 60/969]  eta: 0:08:52  lr: 0.000170  loss: 0.6535 (0.6556)  time: 0.5710  data: 0.0002  max mem: 6820\n[04:10:36.438789] Epoch: [43]  [ 80/969]  eta: 0:08:37  lr: 0.000170  loss: 0.6452 (0.6537)  time: 0.5718  data: 0.0002  max mem: 6820\n[04:10:47.862867] Epoch: [43]  [100/969]  eta: 0:08:23  lr: 0.000170  loss: 0.6715 (0.6583)  time: 0.5712  data: 0.0002  max mem: 6820\n[04:10:59.291365] Epoch: [43]  [120/969]  eta: 0:08:11  lr: 0.000170  loss: 0.6743 (0.6604)  time: 0.5714  data: 0.0002  max mem: 6820\n[04:11:10.709412] Epoch: [43]  [140/969]  eta: 0:07:58  lr: 0.000170  loss: 0.6712 (0.6612)  time: 0.5709  data: 0.0001  max mem: 6820\n[04:11:22.095993] Epoch: [43]  [160/969]  eta: 0:07:46  lr: 0.000170  loss: 0.6581 (0.6620)  time: 0.5693  data: 0.0002  max mem: 6820\n[04:11:33.477180] Epoch: [43]  [180/969]  eta: 0:07:34  lr: 0.000169  loss: 0.6532 (0.6633)  time: 0.5690  data: 0.0002  max mem: 6820\n[04:11:44.864203] Epoch: [43]  [200/969]  eta: 0:07:22  lr: 0.000169  loss: 0.6703 (0.6629)  time: 0.5693  data: 0.0002  max mem: 6820\n[04:11:56.239452] Epoch: [43]  [220/969]  eta: 0:07:10  lr: 0.000169  loss: 0.6349 (0.6615)  time: 0.5687  data: 0.0002  max mem: 6820\n[04:12:07.626305] Epoch: [43]  [240/969]  eta: 0:06:58  lr: 0.000169  loss: 0.6843 (0.6634)  time: 0.5693  data: 0.0002  max mem: 6820\n[04:12:18.989195] Epoch: [43]  [260/969]  eta: 0:06:46  lr: 0.000169  loss: 0.6667 (0.6634)  time: 0.5681  data: 0.0002  max mem: 6820\n[04:12:30.364785] Epoch: [43]  [280/969]  eta: 0:06:34  lr: 0.000169  loss: 0.6528 (0.6632)  time: 0.5687  data: 0.0002  max mem: 6820\n[04:12:41.733922] Epoch: [43]  [300/969]  eta: 0:06:23  lr: 0.000169  loss: 0.6834 (0.6639)  time: 0.5684  data: 0.0002  max mem: 6820\n[04:12:53.093074] Epoch: [43]  [320/969]  eta: 0:06:11  lr: 0.000168  loss: 0.6678 (0.6630)  time: 0.5679  data: 0.0002  max mem: 6820\n[04:13:04.464072] Epoch: [43]  [340/969]  eta: 0:05:59  lr: 0.000168  loss: 0.6868 (0.6647)  time: 0.5685  data: 0.0002  max mem: 6820\n[04:13:15.825573] Epoch: [43]  [360/969]  eta: 0:05:48  lr: 0.000168  loss: 0.6843 (0.6656)  time: 0.5680  data: 0.0002  max mem: 6820\n[04:13:27.185762] Epoch: [43]  [380/969]  eta: 0:05:36  lr: 0.000168  loss: 0.6552 (0.6648)  time: 0.5680  data: 0.0002  max mem: 6820\n[04:13:38.553032] Epoch: [43]  [400/969]  eta: 0:05:25  lr: 0.000168  loss: 0.6445 (0.6648)  time: 0.5683  data: 0.0002  max mem: 6820\n[04:13:49.886069] Epoch: [43]  [420/969]  eta: 0:05:13  lr: 0.000168  loss: 0.6510 (0.6640)  time: 0.5666  data: 0.0002  max mem: 6820\n[04:14:01.237483] Epoch: [43]  [440/969]  eta: 0:05:02  lr: 0.000168  loss: 0.6520 (0.6633)  time: 0.5675  data: 0.0002  max mem: 6820\n[04:14:12.590639] Epoch: [43]  [460/969]  eta: 0:04:50  lr: 0.000167  loss: 0.6545 (0.6636)  time: 0.5676  data: 0.0002  max mem: 6820\n[04:14:23.945626] Epoch: [43]  [480/969]  eta: 0:04:39  lr: 0.000167  loss: 0.6602 (0.6637)  time: 0.5677  data: 0.0002  max mem: 6820\n[04:14:35.286480] Epoch: [43]  [500/969]  eta: 0:04:27  lr: 0.000167  loss: 0.6542 (0.6642)  time: 0.5670  data: 0.0002  max mem: 6820\n[04:14:46.631451] Epoch: [43]  [520/969]  eta: 0:04:16  lr: 0.000167  loss: 0.6408 (0.6637)  time: 0.5672  data: 0.0002  max mem: 6820\n[04:14:57.986372] Epoch: [43]  [540/969]  eta: 0:04:04  lr: 0.000167  loss: 0.6527 (0.6635)  time: 0.5677  data: 0.0002  max mem: 6820\n[04:15:09.367332] Epoch: [43]  [560/969]  eta: 0:03:53  lr: 0.000167  loss: 0.6249 (0.6629)  time: 0.5690  data: 0.0002  max mem: 6820\n[04:15:20.745992] Epoch: [43]  [580/969]  eta: 0:03:41  lr: 0.000167  loss: 0.6739 (0.6635)  time: 0.5689  data: 0.0002  max mem: 6820\n[04:15:32.135676] Epoch: [43]  [600/969]  eta: 0:03:30  lr: 0.000166  loss: 0.6461 (0.6638)  time: 0.5694  data: 0.0002  max mem: 6820\n[04:15:43.533934] Epoch: [43]  [620/969]  eta: 0:03:19  lr: 0.000166  loss: 0.6649 (0.6642)  time: 0.5699  data: 0.0002  max mem: 6820\n[04:15:54.947235] Epoch: [43]  [640/969]  eta: 0:03:07  lr: 0.000166  loss: 0.6593 (0.6641)  time: 0.5706  data: 0.0002  max mem: 6820\n[04:16:06.350701] Epoch: [43]  [660/969]  eta: 0:02:56  lr: 0.000166  loss: 0.6682 (0.6644)  time: 0.5701  data: 0.0002  max mem: 6820\n[04:16:17.775085] Epoch: [43]  [680/969]  eta: 0:02:44  lr: 0.000166  loss: 0.6792 (0.6650)  time: 0.5712  data: 0.0002  max mem: 6820\n[04:16:29.176117] Epoch: [43]  [700/969]  eta: 0:02:33  lr: 0.000166  loss: 0.6588 (0.6649)  time: 0.5700  data: 0.0002  max mem: 6820\n[04:16:40.584417] Epoch: [43]  [720/969]  eta: 0:02:22  lr: 0.000166  loss: 0.6916 (0.6656)  time: 0.5704  data: 0.0002  max mem: 6820\n[04:16:51.941390] Epoch: [43]  [740/969]  eta: 0:02:10  lr: 0.000165  loss: 0.6534 (0.6654)  time: 0.5678  data: 0.0002  max mem: 6820\n[04:17:03.338609] Epoch: [43]  [760/969]  eta: 0:01:59  lr: 0.000165  loss: 0.6588 (0.6652)  time: 0.5698  data: 0.0002  max mem: 6820\n[04:17:14.731135] Epoch: [43]  [780/969]  eta: 0:01:47  lr: 0.000165  loss: 0.6716 (0.6655)  time: 0.5696  data: 0.0002  max mem: 6820\n[04:17:26.103863] Epoch: [43]  [800/969]  eta: 0:01:36  lr: 0.000165  loss: 0.6716 (0.6659)  time: 0.5686  data: 0.0002  max mem: 6820\n[04:17:37.473431] Epoch: [43]  [820/969]  eta: 0:01:24  lr: 0.000165  loss: 0.6479 (0.6653)  time: 0.5684  data: 0.0002  max mem: 6820\n[04:17:48.835378] Epoch: [43]  [840/969]  eta: 0:01:13  lr: 0.000165  loss: 0.6499 (0.6651)  time: 0.5680  data: 0.0001  max mem: 6820\n[04:18:00.203860] Epoch: [43]  [860/969]  eta: 0:01:02  lr: 0.000165  loss: 0.6425 (0.6648)  time: 0.5684  data: 0.0002  max mem: 6820\n[04:18:11.572749] Epoch: [43]  [880/969]  eta: 0:00:50  lr: 0.000164  loss: 0.6474 (0.6647)  time: 0.5684  data: 0.0002  max mem: 6820\n[04:18:22.939458] Epoch: [43]  [900/969]  eta: 0:00:39  lr: 0.000164  loss: 0.6451 (0.6645)  time: 0.5683  data: 0.0002  max mem: 6820\n[04:18:34.280509] Epoch: [43]  [920/969]  eta: 0:00:27  lr: 0.000164  loss: 0.6412 (0.6642)  time: 0.5670  data: 0.0001  max mem: 6820\n[04:18:45.634886] Epoch: [43]  [940/969]  eta: 0:00:16  lr: 0.000164  loss: 0.6256 (0.6638)  time: 0.5677  data: 0.0002  max mem: 6820\n[04:18:56.966937] Epoch: [43]  [960/969]  eta: 0:00:05  lr: 0.000164  loss: 0.6846 (0.6645)  time: 0.5666  data: 0.0002  max mem: 6820\n[04:19:01.512043] Epoch: [43]  [968/969]  eta: 0:00:00  lr: 0.000164  loss: 0.6838 (0.6646)  time: 0.5670  data: 0.0001  max mem: 6820\n[04:19:01.632759] Epoch: [43] Total time: 0:09:12 (0.5700 s / it)\n[04:19:01.632858] Averaged stats: lr: 0.000164  loss: 0.6838 (0.6646)\n[04:19:02.396153] val:  [  0/139]  eta: 0:01:45  loss: 0.5951 (0.5951)  time: 0.7584  data: 0.6047  max mem: 6820\n[04:19:03.891285] val:  [ 10/139]  eta: 0:00:26  loss: 0.5774 (0.5552)  time: 0.2048  data: 0.0552  max mem: 6820\n[04:19:05.394449] val:  [ 20/139]  eta: 0:00:21  loss: 0.5322 (0.5408)  time: 0.1498  data: 0.0002  max mem: 6820\n[04:19:06.897497] val:  [ 30/139]  eta: 0:00:18  loss: 0.5080 (0.5372)  time: 0.1502  data: 0.0002  max mem: 6820\n[04:19:08.404694] val:  [ 40/139]  eta: 0:00:16  loss: 0.5893 (0.5623)  time: 0.1504  data: 0.0002  max mem: 6820\n[04:19:09.916747] val:  [ 50/139]  eta: 0:00:14  loss: 0.6011 (0.5727)  time: 0.1509  data: 0.0002  max mem: 6820\n[04:19:11.429463] val:  [ 60/139]  eta: 0:00:12  loss: 0.5991 (0.5776)  time: 0.1512  data: 0.0002  max mem: 6820\n[04:19:12.941062] val:  [ 70/139]  eta: 0:00:10  loss: 0.6103 (0.5854)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:19:14.446475] val:  [ 80/139]  eta: 0:00:09  loss: 0.6358 (0.6018)  time: 0.1508  data: 0.0002  max mem: 6820\n[04:19:15.957737] val:  [ 90/139]  eta: 0:00:07  loss: 0.7296 (0.6194)  time: 0.1508  data: 0.0002  max mem: 6820\n[04:19:17.472685] val:  [100/139]  eta: 0:00:06  loss: 0.7699 (0.6340)  time: 0.1512  data: 0.0002  max mem: 6820\n[04:19:18.985754] val:  [110/139]  eta: 0:00:04  loss: 0.7552 (0.6421)  time: 0.1513  data: 0.0002  max mem: 6820\n[04:19:20.492370] val:  [120/139]  eta: 0:00:02  loss: 0.6478 (0.6396)  time: 0.1509  data: 0.0002  max mem: 6820\n[04:19:22.008791] val:  [130/139]  eta: 0:00:01  loss: 0.5795 (0.6292)  time: 0.1511  data: 0.0001  max mem: 6820\n[04:19:23.123820] val:  [138/139]  eta: 0:00:00  loss: 0.5082 (0.6217)  time: 0.1464  data: 0.0001  max mem: 6820\n[04:19:23.233883] val: Total time: 0:00:21 (0.1554 s / it)\n[04:19:23.291965] val loss: 0.621652534539751\n[04:19:23.292056] Accuracy: 0.6487, F1 Score: 0.6466, ROC AUC: 0.7145, Hamming Loss: 0.3513,\n Jaccard Score: 0.4784, Precision: 0.6524, Recall: 0.6487,\n Average Precision: 0.7125, Kappa: 0.2975, Score: 0.5529\n[04:19:23.293992] Best epoch = 40, Best score = 0.5589\n[04:19:23.296635] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[04:19:24.729502] Epoch: [44]  [  0/969]  eta: 0:23:07  lr: 0.000164  loss: 0.5486 (0.5486)  time: 1.4316  data: 0.8406  max mem: 6820\n[04:19:36.075611] Epoch: [44]  [ 20/969]  eta: 0:09:37  lr: 0.000164  loss: 0.6429 (0.6476)  time: 0.5673  data: 0.0002  max mem: 6820\n[04:19:47.437277] Epoch: [44]  [ 40/969]  eta: 0:09:06  lr: 0.000163  loss: 0.6652 (0.6587)  time: 0.5680  data: 0.0002  max mem: 6820\n[04:19:58.821467] Epoch: [44]  [ 60/969]  eta: 0:08:49  lr: 0.000163  loss: 0.6551 (0.6582)  time: 0.5692  data: 0.0002  max mem: 6820\n[04:20:10.208388] Epoch: [44]  [ 80/969]  eta: 0:08:34  lr: 0.000163  loss: 0.6485 (0.6581)  time: 0.5693  data: 0.0002  max mem: 6820\n[04:20:21.621498] Epoch: [44]  [100/969]  eta: 0:08:21  lr: 0.000163  loss: 0.6604 (0.6622)  time: 0.5706  data: 0.0002  max mem: 6820\n[04:20:33.023682] Epoch: [44]  [120/969]  eta: 0:08:09  lr: 0.000163  loss: 0.6592 (0.6625)  time: 0.5701  data: 0.0002  max mem: 6820\n[04:20:44.439369] Epoch: [44]  [140/969]  eta: 0:07:57  lr: 0.000163  loss: 0.6640 (0.6630)  time: 0.5707  data: 0.0002  max mem: 6820\n[04:20:55.862071] Epoch: [44]  [160/969]  eta: 0:07:45  lr: 0.000163  loss: 0.6666 (0.6627)  time: 0.5711  data: 0.0002  max mem: 6820\n[04:21:07.265960] Epoch: [44]  [180/969]  eta: 0:07:33  lr: 0.000162  loss: 0.6587 (0.6635)  time: 0.5701  data: 0.0002  max mem: 6820\n[04:21:18.680680] Epoch: [44]  [200/969]  eta: 0:07:21  lr: 0.000162  loss: 0.6850 (0.6657)  time: 0.5707  data: 0.0002  max mem: 6820\n[04:21:30.070363] Epoch: [44]  [220/969]  eta: 0:07:09  lr: 0.000162  loss: 0.6648 (0.6667)  time: 0.5694  data: 0.0002  max mem: 6820\n[04:21:41.459497] Epoch: [44]  [240/969]  eta: 0:06:57  lr: 0.000162  loss: 0.6662 (0.6673)  time: 0.5694  data: 0.0002  max mem: 6820\n[04:21:52.721624] Epoch: [44]  [260/969]  eta: 0:06:45  lr: 0.000162  loss: 0.6445 (0.6660)  time: 0.5631  data: 0.0002  max mem: 6820\n[04:22:04.079034] Epoch: [44]  [280/969]  eta: 0:06:34  lr: 0.000162  loss: 0.6587 (0.6666)  time: 0.5678  data: 0.0002  max mem: 6820\n[04:22:15.433202] Epoch: [44]  [300/969]  eta: 0:06:22  lr: 0.000162  loss: 0.6651 (0.6674)  time: 0.5677  data: 0.0002  max mem: 6820\n[04:22:26.798246] Epoch: [44]  [320/969]  eta: 0:06:10  lr: 0.000161  loss: 0.6415 (0.6661)  time: 0.5682  data: 0.0002  max mem: 6820\n[04:22:38.165056] Epoch: [44]  [340/969]  eta: 0:05:59  lr: 0.000161  loss: 0.6639 (0.6661)  time: 0.5683  data: 0.0002  max mem: 6820\n[04:22:49.514052] Epoch: [44]  [360/969]  eta: 0:05:47  lr: 0.000161  loss: 0.6765 (0.6667)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:23:00.877350] Epoch: [44]  [380/969]  eta: 0:05:36  lr: 0.000161  loss: 0.6508 (0.6663)  time: 0.5681  data: 0.0002  max mem: 6820\n[04:23:12.222861] Epoch: [44]  [400/969]  eta: 0:05:24  lr: 0.000161  loss: 0.6435 (0.6655)  time: 0.5672  data: 0.0002  max mem: 6820\n[04:23:23.556362] Epoch: [44]  [420/969]  eta: 0:05:13  lr: 0.000161  loss: 0.6529 (0.6645)  time: 0.5666  data: 0.0002  max mem: 6820\n[04:23:34.911529] Epoch: [44]  [440/969]  eta: 0:05:01  lr: 0.000161  loss: 0.6445 (0.6638)  time: 0.5677  data: 0.0002  max mem: 6820\n[04:23:46.233580] Epoch: [44]  [460/969]  eta: 0:04:50  lr: 0.000160  loss: 0.6026 (0.6623)  time: 0.5661  data: 0.0003  max mem: 6820\n[04:23:57.615216] Epoch: [44]  [480/969]  eta: 0:04:38  lr: 0.000160  loss: 0.6758 (0.6629)  time: 0.5690  data: 0.0002  max mem: 6820\n[04:24:08.949451] Epoch: [44]  [500/969]  eta: 0:04:27  lr: 0.000160  loss: 0.6658 (0.6631)  time: 0.5667  data: 0.0002  max mem: 6820\n[04:24:20.287989] Epoch: [44]  [520/969]  eta: 0:04:15  lr: 0.000160  loss: 0.6416 (0.6628)  time: 0.5669  data: 0.0002  max mem: 6820\n[04:24:31.649597] Epoch: [44]  [540/969]  eta: 0:04:04  lr: 0.000160  loss: 0.6483 (0.6626)  time: 0.5680  data: 0.0002  max mem: 6820\n[04:24:43.011353] Epoch: [44]  [560/969]  eta: 0:03:53  lr: 0.000160  loss: 0.6577 (0.6622)  time: 0.5680  data: 0.0002  max mem: 6820\n[04:24:54.359948] Epoch: [44]  [580/969]  eta: 0:03:41  lr: 0.000160  loss: 0.6587 (0.6627)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:25:05.719889] Epoch: [44]  [600/969]  eta: 0:03:30  lr: 0.000159  loss: 0.6496 (0.6629)  time: 0.5679  data: 0.0002  max mem: 6820\n[04:25:17.112100] Epoch: [44]  [620/969]  eta: 0:03:18  lr: 0.000159  loss: 0.6294 (0.6624)  time: 0.5696  data: 0.0002  max mem: 6820\n[04:25:28.509650] Epoch: [44]  [640/969]  eta: 0:03:07  lr: 0.000159  loss: 0.6832 (0.6628)  time: 0.5698  data: 0.0002  max mem: 6820\n[04:25:39.914681] Epoch: [44]  [660/969]  eta: 0:02:56  lr: 0.000159  loss: 0.6638 (0.6631)  time: 0.5702  data: 0.0001  max mem: 6820\n[04:25:51.318461] Epoch: [44]  [680/969]  eta: 0:02:44  lr: 0.000159  loss: 0.6709 (0.6634)  time: 0.5701  data: 0.0002  max mem: 6820\n[04:26:02.736011] Epoch: [44]  [700/969]  eta: 0:02:33  lr: 0.000159  loss: 0.6426 (0.6632)  time: 0.5708  data: 0.0002  max mem: 6820\n[04:26:14.152751] Epoch: [44]  [720/969]  eta: 0:02:21  lr: 0.000159  loss: 0.6599 (0.6634)  time: 0.5708  data: 0.0002  max mem: 6820\n[04:26:25.530151] Epoch: [44]  [740/969]  eta: 0:02:10  lr: 0.000158  loss: 0.6485 (0.6632)  time: 0.5688  data: 0.0002  max mem: 6820\n[04:26:36.910775] Epoch: [44]  [760/969]  eta: 0:01:59  lr: 0.000158  loss: 0.6318 (0.6627)  time: 0.5690  data: 0.0002  max mem: 6820\n[04:26:48.293909] Epoch: [44]  [780/969]  eta: 0:01:47  lr: 0.000158  loss: 0.6574 (0.6628)  time: 0.5691  data: 0.0002  max mem: 6820\n[04:26:59.693885] Epoch: [44]  [800/969]  eta: 0:01:36  lr: 0.000158  loss: 0.6785 (0.6633)  time: 0.5700  data: 0.0002  max mem: 6820\n[04:27:11.053596] Epoch: [44]  [820/969]  eta: 0:01:24  lr: 0.000158  loss: 0.6657 (0.6636)  time: 0.5679  data: 0.0002  max mem: 6820\n[04:27:22.429246] Epoch: [44]  [840/969]  eta: 0:01:13  lr: 0.000158  loss: 0.6388 (0.6635)  time: 0.5687  data: 0.0002  max mem: 6820\n[04:27:33.785264] Epoch: [44]  [860/969]  eta: 0:01:02  lr: 0.000158  loss: 0.6608 (0.6634)  time: 0.5677  data: 0.0002  max mem: 6820\n[04:27:45.126440] Epoch: [44]  [880/969]  eta: 0:00:50  lr: 0.000157  loss: 0.6447 (0.6632)  time: 0.5670  data: 0.0002  max mem: 6820\n[04:27:56.482349] Epoch: [44]  [900/969]  eta: 0:00:39  lr: 0.000157  loss: 0.6532 (0.6634)  time: 0.5677  data: 0.0002  max mem: 6820\n[04:28:07.815225] Epoch: [44]  [920/969]  eta: 0:00:27  lr: 0.000157  loss: 0.6238 (0.6630)  time: 0.5666  data: 0.0002  max mem: 6820\n[04:28:19.173152] Epoch: [44]  [940/969]  eta: 0:00:16  lr: 0.000157  loss: 0.6356 (0.6628)  time: 0.5678  data: 0.0001  max mem: 6820\n[04:28:30.541088] Epoch: [44]  [960/969]  eta: 0:00:05  lr: 0.000157  loss: 0.6562 (0.6627)  time: 0.5684  data: 0.0002  max mem: 6820\n[04:28:35.083732] Epoch: [44]  [968/969]  eta: 0:00:00  lr: 0.000157  loss: 0.6467 (0.6627)  time: 0.5683  data: 0.0002  max mem: 6820\n[04:28:35.205064] Epoch: [44] Total time: 0:09:11 (0.5696 s / it)\n[04:28:35.205179] Averaged stats: lr: 0.000157  loss: 0.6467 (0.6627)\n[04:28:36.126798] val:  [  0/139]  eta: 0:02:07  loss: 0.7678 (0.7678)  time: 0.9170  data: 0.7788  max mem: 6820\n[04:28:37.632481] val:  [ 10/139]  eta: 0:00:28  loss: 0.6581 (0.6336)  time: 0.2202  data: 0.0710  max mem: 6820\n[04:28:39.133040] val:  [ 20/139]  eta: 0:00:22  loss: 0.5698 (0.5982)  time: 0.1502  data: 0.0002  max mem: 6820\n[04:28:40.644761] val:  [ 30/139]  eta: 0:00:19  loss: 0.5350 (0.5870)  time: 0.1505  data: 0.0002  max mem: 6820\n[04:28:42.150737] val:  [ 40/139]  eta: 0:00:16  loss: 0.6251 (0.6164)  time: 0.1508  data: 0.0002  max mem: 6820\n[04:28:43.663689] val:  [ 50/139]  eta: 0:00:14  loss: 0.6735 (0.6282)  time: 0.1509  data: 0.0002  max mem: 6820\n[04:28:45.181588] val:  [ 60/139]  eta: 0:00:12  loss: 0.6133 (0.6237)  time: 0.1515  data: 0.0002  max mem: 6820\n[04:28:46.693275] val:  [ 70/139]  eta: 0:00:11  loss: 0.6004 (0.6217)  time: 0.1514  data: 0.0002  max mem: 6820\n[04:28:48.207662] val:  [ 80/139]  eta: 0:00:09  loss: 0.6004 (0.6288)  time: 0.1512  data: 0.0002  max mem: 6820\n[04:28:49.723678] val:  [ 90/139]  eta: 0:00:07  loss: 0.7613 (0.6455)  time: 0.1514  data: 0.0002  max mem: 6820\n[04:28:51.238961] val:  [100/139]  eta: 0:00:06  loss: 0.7757 (0.6580)  time: 0.1515  data: 0.0002  max mem: 6820\n[04:28:52.755396] val:  [110/139]  eta: 0:00:04  loss: 0.7527 (0.6627)  time: 0.1515  data: 0.0002  max mem: 6820\n[04:28:54.264446] val:  [120/139]  eta: 0:00:02  loss: 0.5966 (0.6523)  time: 0.1512  data: 0.0002  max mem: 6820\n[04:28:55.783478] val:  [130/139]  eta: 0:00:01  loss: 0.4959 (0.6337)  time: 0.1513  data: 0.0001  max mem: 6820\n[04:28:56.893030] val:  [138/139]  eta: 0:00:00  loss: 0.3878 (0.6210)  time: 0.1464  data: 0.0001  max mem: 6820\n[04:28:57.002565] val: Total time: 0:00:21 (0.1568 s / it)\n[04:28:57.062554] val loss: 0.6209586301295877\n[04:28:57.062611] Accuracy: 0.6505, F1 Score: 0.6480, ROC AUC: 0.7100, Hamming Loss: 0.3495,\n Jaccard Score: 0.4800, Precision: 0.6551, Recall: 0.6505,\n Average Precision: 0.7075, Kappa: 0.3011, Score: 0.5530\n[04:28:57.064539] Best epoch = 40, Best score = 0.5589\n[04:28:57.067434] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[04:28:58.551879] Epoch: [45]  [  0/969]  eta: 0:23:57  lr: 0.000157  loss: 0.4785 (0.4785)  time: 1.4832  data: 0.9020  max mem: 6820\n[04:29:09.907173] Epoch: [45]  [ 20/969]  eta: 0:09:40  lr: 0.000157  loss: 0.6533 (0.6522)  time: 0.5677  data: 0.0001  max mem: 6820\n[04:29:21.292871] Epoch: [45]  [ 40/969]  eta: 0:09:08  lr: 0.000156  loss: 0.6535 (0.6622)  time: 0.5692  data: 0.0002  max mem: 6820\n[04:29:32.717385] Epoch: [45]  [ 60/969]  eta: 0:08:51  lr: 0.000156  loss: 0.6546 (0.6620)  time: 0.5712  data: 0.0002  max mem: 6820\n[04:29:44.171532] Epoch: [45]  [ 80/969]  eta: 0:08:36  lr: 0.000156  loss: 0.6297 (0.6593)  time: 0.5727  data: 0.0002  max mem: 6820\n[04:29:55.620328] Epoch: [45]  [100/969]  eta: 0:08:23  lr: 0.000156  loss: 0.6404 (0.6568)  time: 0.5724  data: 0.0002  max mem: 6820\n[04:30:07.051712] Epoch: [45]  [120/969]  eta: 0:08:10  lr: 0.000156  loss: 0.6629 (0.6608)  time: 0.5715  data: 0.0002  max mem: 6820\n[04:30:18.475725] Epoch: [45]  [140/969]  eta: 0:07:58  lr: 0.000156  loss: 0.6640 (0.6612)  time: 0.5712  data: 0.0002  max mem: 6820\n[04:30:29.850680] Epoch: [45]  [160/969]  eta: 0:07:46  lr: 0.000156  loss: 0.6763 (0.6631)  time: 0.5687  data: 0.0002  max mem: 6820\n[04:30:41.229036] Epoch: [45]  [180/969]  eta: 0:07:34  lr: 0.000155  loss: 0.6515 (0.6629)  time: 0.5689  data: 0.0002  max mem: 6820\n[04:30:52.577279] Epoch: [45]  [200/969]  eta: 0:07:21  lr: 0.000155  loss: 0.6620 (0.6634)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:31:03.924673] Epoch: [45]  [220/969]  eta: 0:07:09  lr: 0.000155  loss: 0.6704 (0.6646)  time: 0.5673  data: 0.0002  max mem: 6820\n[04:31:15.259655] Epoch: [45]  [240/969]  eta: 0:06:57  lr: 0.000155  loss: 0.6718 (0.6650)  time: 0.5667  data: 0.0002  max mem: 6820\n[04:31:26.605944] Epoch: [45]  [260/969]  eta: 0:06:46  lr: 0.000155  loss: 0.6631 (0.6651)  time: 0.5673  data: 0.0002  max mem: 6820\n[04:31:37.941346] Epoch: [45]  [280/969]  eta: 0:06:34  lr: 0.000155  loss: 0.6294 (0.6629)  time: 0.5667  data: 0.0002  max mem: 6820\n[04:31:49.295484] Epoch: [45]  [300/969]  eta: 0:06:22  lr: 0.000155  loss: 0.6521 (0.6630)  time: 0.5677  data: 0.0001  max mem: 6820\n[04:32:00.687156] Epoch: [45]  [320/969]  eta: 0:06:11  lr: 0.000154  loss: 0.6480 (0.6640)  time: 0.5695  data: 0.0002  max mem: 6820\n[04:32:12.049344] Epoch: [45]  [340/969]  eta: 0:05:59  lr: 0.000154  loss: 0.6441 (0.6635)  time: 0.5681  data: 0.0002  max mem: 6820\n[04:32:23.450477] Epoch: [45]  [360/969]  eta: 0:05:48  lr: 0.000154  loss: 0.6603 (0.6642)  time: 0.5700  data: 0.0002  max mem: 6820\n[04:32:34.855535] Epoch: [45]  [380/969]  eta: 0:05:36  lr: 0.000154  loss: 0.6521 (0.6638)  time: 0.5702  data: 0.0002  max mem: 6820\n[04:32:46.246200] Epoch: [45]  [400/969]  eta: 0:05:25  lr: 0.000154  loss: 0.6344 (0.6630)  time: 0.5695  data: 0.0002  max mem: 6820\n[04:32:57.664701] Epoch: [45]  [420/969]  eta: 0:05:13  lr: 0.000154  loss: 0.6582 (0.6633)  time: 0.5709  data: 0.0002  max mem: 6820\n[04:33:09.072858] Epoch: [45]  [440/969]  eta: 0:05:02  lr: 0.000154  loss: 0.6638 (0.6636)  time: 0.5704  data: 0.0002  max mem: 6820\n[04:33:20.495492] Epoch: [45]  [460/969]  eta: 0:04:50  lr: 0.000153  loss: 0.6459 (0.6628)  time: 0.5711  data: 0.0002  max mem: 6820\n[04:33:31.940467] Epoch: [45]  [480/969]  eta: 0:04:39  lr: 0.000153  loss: 0.6458 (0.6630)  time: 0.5722  data: 0.0002  max mem: 6820\n[04:33:43.360177] Epoch: [45]  [500/969]  eta: 0:04:27  lr: 0.000153  loss: 0.6495 (0.6634)  time: 0.5709  data: 0.0002  max mem: 6820\n[04:33:54.778358] Epoch: [45]  [520/969]  eta: 0:04:16  lr: 0.000153  loss: 0.6534 (0.6629)  time: 0.5709  data: 0.0002  max mem: 6820\n[04:34:06.160553] Epoch: [45]  [540/969]  eta: 0:04:05  lr: 0.000153  loss: 0.6408 (0.6621)  time: 0.5691  data: 0.0002  max mem: 6820\n[04:34:17.543743] Epoch: [45]  [560/969]  eta: 0:03:53  lr: 0.000153  loss: 0.6646 (0.6622)  time: 0.5691  data: 0.0002  max mem: 6820\n[04:34:28.908533] Epoch: [45]  [580/969]  eta: 0:03:42  lr: 0.000153  loss: 0.6795 (0.6629)  time: 0.5682  data: 0.0002  max mem: 6820\n[04:34:40.278694] Epoch: [45]  [600/969]  eta: 0:03:30  lr: 0.000152  loss: 0.6606 (0.6633)  time: 0.5685  data: 0.0002  max mem: 6820\n[04:34:51.646494] Epoch: [45]  [620/969]  eta: 0:03:19  lr: 0.000152  loss: 0.6453 (0.6628)  time: 0.5683  data: 0.0001  max mem: 6820\n[04:35:03.019015] Epoch: [45]  [640/969]  eta: 0:03:07  lr: 0.000152  loss: 0.6444 (0.6626)  time: 0.5686  data: 0.0002  max mem: 6820\n[04:35:14.371677] Epoch: [45]  [660/969]  eta: 0:02:56  lr: 0.000152  loss: 0.6498 (0.6625)  time: 0.5676  data: 0.0002  max mem: 6820\n[04:35:25.730470] Epoch: [45]  [680/969]  eta: 0:02:44  lr: 0.000152  loss: 0.6667 (0.6629)  time: 0.5679  data: 0.0002  max mem: 6820\n[04:35:37.068386] Epoch: [45]  [700/969]  eta: 0:02:33  lr: 0.000152  loss: 0.6663 (0.6626)  time: 0.5668  data: 0.0002  max mem: 6820\n[04:35:48.415384] Epoch: [45]  [720/969]  eta: 0:02:22  lr: 0.000152  loss: 0.6439 (0.6620)  time: 0.5673  data: 0.0002  max mem: 6820\n[04:35:59.765194] Epoch: [45]  [740/969]  eta: 0:02:10  lr: 0.000151  loss: 0.6395 (0.6620)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:36:11.113549] Epoch: [45]  [760/969]  eta: 0:01:59  lr: 0.000151  loss: 0.6537 (0.6622)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:36:22.441560] Epoch: [45]  [780/969]  eta: 0:01:47  lr: 0.000151  loss: 0.6663 (0.6627)  time: 0.5663  data: 0.0002  max mem: 6820\n[04:36:33.787099] Epoch: [45]  [800/969]  eta: 0:01:36  lr: 0.000151  loss: 0.6431 (0.6627)  time: 0.5672  data: 0.0002  max mem: 6820\n[04:36:45.129098] Epoch: [45]  [820/969]  eta: 0:01:24  lr: 0.000151  loss: 0.6475 (0.6622)  time: 0.5671  data: 0.0002  max mem: 6820\n[04:36:56.502694] Epoch: [45]  [840/969]  eta: 0:01:13  lr: 0.000151  loss: 0.6369 (0.6622)  time: 0.5686  data: 0.0001  max mem: 6820\n[04:37:07.869183] Epoch: [45]  [860/969]  eta: 0:01:02  lr: 0.000151  loss: 0.6299 (0.6619)  time: 0.5683  data: 0.0002  max mem: 6820\n[04:37:19.257944] Epoch: [45]  [880/969]  eta: 0:00:50  lr: 0.000150  loss: 0.6506 (0.6619)  time: 0.5694  data: 0.0002  max mem: 6820\n[04:37:30.649467] Epoch: [45]  [900/969]  eta: 0:00:39  lr: 0.000150  loss: 0.6517 (0.6620)  time: 0.5695  data: 0.0002  max mem: 6820\n[04:37:42.059888] Epoch: [45]  [920/969]  eta: 0:00:27  lr: 0.000150  loss: 0.6230 (0.6617)  time: 0.5705  data: 0.0002  max mem: 6820\n[04:37:53.454485] Epoch: [45]  [940/969]  eta: 0:00:16  lr: 0.000150  loss: 0.6745 (0.6618)  time: 0.5697  data: 0.0002  max mem: 6820\n[04:38:04.878144] Epoch: [45]  [960/969]  eta: 0:00:05  lr: 0.000150  loss: 0.6548 (0.6618)  time: 0.5711  data: 0.0002  max mem: 6820\n[04:38:09.447626] Epoch: [45]  [968/969]  eta: 0:00:00  lr: 0.000150  loss: 0.6631 (0.6621)  time: 0.5707  data: 0.0002  max mem: 6820\n[04:38:09.571140] Epoch: [45] Total time: 0:09:12 (0.5702 s / it)\n[04:38:09.571254] Averaged stats: lr: 0.000150  loss: 0.6631 (0.6621)\n[04:38:10.490716] val:  [  0/139]  eta: 0:02:07  loss: 0.6500 (0.6500)  time: 0.9138  data: 0.7783  max mem: 6820\n[04:38:11.991138] val:  [ 10/139]  eta: 0:00:28  loss: 0.6500 (0.6055)  time: 0.2194  data: 0.0710  max mem: 6820\n[04:38:13.502053] val:  [ 20/139]  eta: 0:00:22  loss: 0.5650 (0.5802)  time: 0.1505  data: 0.0002  max mem: 6820\n[04:38:15.014280] val:  [ 30/139]  eta: 0:00:19  loss: 0.5520 (0.5692)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:38:16.525590] val:  [ 40/139]  eta: 0:00:16  loss: 0.6080 (0.5965)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:38:18.035095] val:  [ 50/139]  eta: 0:00:14  loss: 0.6598 (0.6059)  time: 0.1510  data: 0.0002  max mem: 6820\n[04:38:19.553762] val:  [ 60/139]  eta: 0:00:12  loss: 0.6029 (0.6059)  time: 0.1513  data: 0.0002  max mem: 6820\n[04:38:21.075081] val:  [ 70/139]  eta: 0:00:11  loss: 0.5969 (0.6074)  time: 0.1519  data: 0.0002  max mem: 6820\n[04:38:22.596864] val:  [ 80/139]  eta: 0:00:09  loss: 0.6267 (0.6153)  time: 0.1521  data: 0.0002  max mem: 6820\n[04:38:24.113626] val:  [ 90/139]  eta: 0:00:07  loss: 0.7420 (0.6316)  time: 0.1519  data: 0.0002  max mem: 6820\n[04:38:25.625695] val:  [100/139]  eta: 0:00:06  loss: 0.7556 (0.6438)  time: 0.1514  data: 0.0002  max mem: 6820\n[04:38:27.137633] val:  [110/139]  eta: 0:00:04  loss: 0.7425 (0.6499)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:38:28.649615] val:  [120/139]  eta: 0:00:02  loss: 0.6304 (0.6437)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:38:30.172182] val:  [130/139]  eta: 0:00:01  loss: 0.5397 (0.6276)  time: 0.1516  data: 0.0002  max mem: 6820\n[04:38:31.294480] val:  [138/139]  eta: 0:00:00  loss: 0.4011 (0.6162)  time: 0.1473  data: 0.0002  max mem: 6820\n[04:38:31.404088] val: Total time: 0:00:21 (0.1570 s / it)\n[04:38:31.460467] val loss: 0.6161641868755972\n[04:38:31.460518] Accuracy: 0.6573, F1 Score: 0.6538, ROC AUC: 0.7130, Hamming Loss: 0.3427,\n Jaccard Score: 0.4867, Precision: 0.6639, Recall: 0.6573,\n Average Precision: 0.7128, Kappa: 0.3146, Score: 0.5605\n[04:38:34.217298] Best epoch = 45, Best score = 0.5605\n[04:38:34.220292] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[04:38:35.657042] Epoch: [46]  [  0/969]  eta: 0:23:10  lr: 0.000150  loss: 0.5849 (0.5849)  time: 1.4354  data: 0.8383  max mem: 6820\n[04:38:46.972717] Epoch: [46]  [ 20/969]  eta: 0:09:36  lr: 0.000150  loss: 0.6503 (0.6548)  time: 0.5657  data: 0.0002  max mem: 6820\n[04:38:58.458273] Epoch: [46]  [ 40/969]  eta: 0:09:09  lr: 0.000149  loss: 0.6567 (0.6536)  time: 0.5742  data: 0.0003  max mem: 6820\n[04:39:09.947576] Epoch: [46]  [ 60/969]  eta: 0:08:52  lr: 0.000149  loss: 0.6520 (0.6556)  time: 0.5744  data: 0.0002  max mem: 6820\n[04:39:21.413827] Epoch: [46]  [ 80/969]  eta: 0:08:37  lr: 0.000149  loss: 0.6671 (0.6605)  time: 0.5733  data: 0.0002  max mem: 6820\n[04:39:32.805907] Epoch: [46]  [100/969]  eta: 0:08:24  lr: 0.000149  loss: 0.6667 (0.6613)  time: 0.5696  data: 0.0002  max mem: 6820\n[04:39:44.169856] Epoch: [46]  [120/969]  eta: 0:08:10  lr: 0.000149  loss: 0.6805 (0.6632)  time: 0.5681  data: 0.0002  max mem: 6820\n[04:39:55.491019] Epoch: [46]  [140/969]  eta: 0:07:57  lr: 0.000149  loss: 0.6697 (0.6642)  time: 0.5660  data: 0.0002  max mem: 6820\n[04:40:06.823018] Epoch: [46]  [160/969]  eta: 0:07:45  lr: 0.000149  loss: 0.6734 (0.6652)  time: 0.5666  data: 0.0001  max mem: 6820\n[04:40:18.174689] Epoch: [46]  [180/969]  eta: 0:07:33  lr: 0.000148  loss: 0.6539 (0.6644)  time: 0.5675  data: 0.0002  max mem: 6820\n[04:40:29.555013] Epoch: [46]  [200/969]  eta: 0:07:21  lr: 0.000148  loss: 0.6413 (0.6636)  time: 0.5690  data: 0.0002  max mem: 6820\n[04:40:40.965302] Epoch: [46]  [220/969]  eta: 0:07:09  lr: 0.000148  loss: 0.6510 (0.6640)  time: 0.5705  data: 0.0002  max mem: 6820\n[04:40:52.428924] Epoch: [46]  [240/969]  eta: 0:06:58  lr: 0.000148  loss: 0.6624 (0.6654)  time: 0.5731  data: 0.0002  max mem: 6820\n[04:41:03.856710] Epoch: [46]  [260/969]  eta: 0:06:46  lr: 0.000148  loss: 0.6369 (0.6652)  time: 0.5713  data: 0.0002  max mem: 6820\n[04:41:15.290134] Epoch: [46]  [280/969]  eta: 0:06:34  lr: 0.000148  loss: 0.6648 (0.6660)  time: 0.5716  data: 0.0002  max mem: 6820\n[04:41:26.724110] Epoch: [46]  [300/969]  eta: 0:06:23  lr: 0.000148  loss: 0.6663 (0.6664)  time: 0.5717  data: 0.0002  max mem: 6820\n[04:41:38.166341] Epoch: [46]  [320/969]  eta: 0:06:11  lr: 0.000147  loss: 0.6477 (0.6657)  time: 0.5721  data: 0.0002  max mem: 6820\n[04:41:49.453632] Epoch: [46]  [340/969]  eta: 0:06:00  lr: 0.000147  loss: 0.6698 (0.6660)  time: 0.5643  data: 0.0002  max mem: 6820\n[04:42:00.826944] Epoch: [46]  [360/969]  eta: 0:05:48  lr: 0.000147  loss: 0.6742 (0.6661)  time: 0.5686  data: 0.0002  max mem: 6820\n[04:42:12.175824] Epoch: [46]  [380/969]  eta: 0:05:36  lr: 0.000147  loss: 0.6547 (0.6660)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:42:23.512973] Epoch: [46]  [400/969]  eta: 0:05:25  lr: 0.000147  loss: 0.6582 (0.6653)  time: 0.5668  data: 0.0002  max mem: 6820\n[04:42:34.883094] Epoch: [46]  [420/969]  eta: 0:05:13  lr: 0.000147  loss: 0.6754 (0.6651)  time: 0.5685  data: 0.0002  max mem: 6820\n[04:42:46.205562] Epoch: [46]  [440/969]  eta: 0:05:02  lr: 0.000147  loss: 0.6707 (0.6648)  time: 0.5661  data: 0.0002  max mem: 6820\n[04:42:57.536875] Epoch: [46]  [460/969]  eta: 0:04:50  lr: 0.000146  loss: 0.6203 (0.6640)  time: 0.5665  data: 0.0002  max mem: 6820\n[04:43:08.866294] Epoch: [46]  [480/969]  eta: 0:04:39  lr: 0.000146  loss: 0.6642 (0.6637)  time: 0.5664  data: 0.0002  max mem: 6820\n[04:43:20.214375] Epoch: [46]  [500/969]  eta: 0:04:27  lr: 0.000146  loss: 0.6483 (0.6634)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:43:31.565755] Epoch: [46]  [520/969]  eta: 0:04:16  lr: 0.000146  loss: 0.6160 (0.6622)  time: 0.5675  data: 0.0002  max mem: 6820\n[04:43:42.939787] Epoch: [46]  [540/969]  eta: 0:04:04  lr: 0.000146  loss: 0.6474 (0.6619)  time: 0.5687  data: 0.0002  max mem: 6820\n[04:43:54.333260] Epoch: [46]  [560/969]  eta: 0:03:53  lr: 0.000146  loss: 0.6643 (0.6627)  time: 0.5696  data: 0.0002  max mem: 6820\n[04:44:05.736477] Epoch: [46]  [580/969]  eta: 0:03:41  lr: 0.000146  loss: 0.6253 (0.6618)  time: 0.5701  data: 0.0002  max mem: 6820\n[04:44:17.151647] Epoch: [46]  [600/969]  eta: 0:03:30  lr: 0.000145  loss: 0.6746 (0.6623)  time: 0.5707  data: 0.0002  max mem: 6820\n[04:44:28.538723] Epoch: [46]  [620/969]  eta: 0:03:19  lr: 0.000145  loss: 0.6487 (0.6618)  time: 0.5693  data: 0.0002  max mem: 6820\n[04:44:39.954285] Epoch: [46]  [640/969]  eta: 0:03:07  lr: 0.000145  loss: 0.6854 (0.6625)  time: 0.5707  data: 0.0002  max mem: 6820\n[04:44:51.367639] Epoch: [46]  [660/969]  eta: 0:02:56  lr: 0.000145  loss: 0.6559 (0.6629)  time: 0.5706  data: 0.0002  max mem: 6820\n[04:45:02.791702] Epoch: [46]  [680/969]  eta: 0:02:44  lr: 0.000145  loss: 0.6745 (0.6627)  time: 0.5712  data: 0.0002  max mem: 6820\n[04:45:14.228022] Epoch: [46]  [700/969]  eta: 0:02:33  lr: 0.000145  loss: 0.6556 (0.6625)  time: 0.5718  data: 0.0002  max mem: 6820\n[04:45:25.645659] Epoch: [46]  [720/969]  eta: 0:02:22  lr: 0.000145  loss: 0.6740 (0.6629)  time: 0.5708  data: 0.0002  max mem: 6820\n[04:45:37.039666] Epoch: [46]  [740/969]  eta: 0:02:10  lr: 0.000144  loss: 0.6432 (0.6626)  time: 0.5697  data: 0.0003  max mem: 6820\n[04:45:48.413586] Epoch: [46]  [760/969]  eta: 0:01:59  lr: 0.000144  loss: 0.6500 (0.6622)  time: 0.5686  data: 0.0002  max mem: 6820\n[04:45:59.787795] Epoch: [46]  [780/969]  eta: 0:01:47  lr: 0.000144  loss: 0.6548 (0.6623)  time: 0.5687  data: 0.0002  max mem: 6820\n[04:46:11.187830] Epoch: [46]  [800/969]  eta: 0:01:36  lr: 0.000144  loss: 0.6614 (0.6627)  time: 0.5700  data: 0.0002  max mem: 6820\n[04:46:22.560683] Epoch: [46]  [820/969]  eta: 0:01:24  lr: 0.000144  loss: 0.6506 (0.6626)  time: 0.5686  data: 0.0002  max mem: 6820\n[04:46:33.928056] Epoch: [46]  [840/969]  eta: 0:01:13  lr: 0.000144  loss: 0.6420 (0.6623)  time: 0.5683  data: 0.0002  max mem: 6820\n[04:46:45.288204] Epoch: [46]  [860/969]  eta: 0:01:02  lr: 0.000144  loss: 0.6636 (0.6627)  time: 0.5680  data: 0.0002  max mem: 6820\n[04:46:56.650133] Epoch: [46]  [880/969]  eta: 0:00:50  lr: 0.000143  loss: 0.6499 (0.6625)  time: 0.5681  data: 0.0002  max mem: 6820\n[04:47:08.027661] Epoch: [46]  [900/969]  eta: 0:00:39  lr: 0.000143  loss: 0.6427 (0.6621)  time: 0.5688  data: 0.0002  max mem: 6820\n[04:47:19.392782] Epoch: [46]  [920/969]  eta: 0:00:27  lr: 0.000143  loss: 0.6690 (0.6620)  time: 0.5682  data: 0.0002  max mem: 6820\n[04:47:30.739948] Epoch: [46]  [940/969]  eta: 0:00:16  lr: 0.000143  loss: 0.6490 (0.6618)  time: 0.5673  data: 0.0002  max mem: 6820\n[04:47:42.084024] Epoch: [46]  [960/969]  eta: 0:00:05  lr: 0.000143  loss: 0.6806 (0.6621)  time: 0.5672  data: 0.0002  max mem: 6820\n[04:47:46.621806] Epoch: [46]  [968/969]  eta: 0:00:00  lr: 0.000143  loss: 0.6832 (0.6623)  time: 0.5672  data: 0.0002  max mem: 6820\n[04:47:46.742565] Epoch: [46] Total time: 0:09:12 (0.5702 s / it)\n[04:47:46.742668] Averaged stats: lr: 0.000143  loss: 0.6832 (0.6623)\n[04:47:47.841025] val:  [  0/139]  eta: 0:02:31  loss: 0.6057 (0.6057)  time: 1.0900  data: 0.9347  max mem: 6820\n[04:47:49.327464] val:  [ 10/139]  eta: 0:00:30  loss: 0.6057 (0.5631)  time: 0.2342  data: 0.0851  max mem: 6820\n[04:47:50.832063] val:  [ 20/139]  eta: 0:00:23  loss: 0.5375 (0.5420)  time: 0.1495  data: 0.0002  max mem: 6820\n[04:47:52.340647] val:  [ 30/139]  eta: 0:00:19  loss: 0.5074 (0.5357)  time: 0.1506  data: 0.0002  max mem: 6820\n[04:47:53.851163] val:  [ 40/139]  eta: 0:00:17  loss: 0.5951 (0.5647)  time: 0.1509  data: 0.0002  max mem: 6820\n[04:47:55.355318] val:  [ 50/139]  eta: 0:00:15  loss: 0.6329 (0.5743)  time: 0.1507  data: 0.0002  max mem: 6820\n[04:47:56.873795] val:  [ 60/139]  eta: 0:00:13  loss: 0.5730 (0.5772)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:47:58.382074] val:  [ 70/139]  eta: 0:00:11  loss: 0.5869 (0.5821)  time: 0.1513  data: 0.0002  max mem: 6820\n[04:47:59.891616] val:  [ 80/139]  eta: 0:00:09  loss: 0.6034 (0.5958)  time: 0.1508  data: 0.0002  max mem: 6820\n[04:48:01.408842] val:  [ 90/139]  eta: 0:00:07  loss: 0.7657 (0.6172)  time: 0.1513  data: 0.0002  max mem: 6820\n[04:48:02.927371] val:  [100/139]  eta: 0:00:06  loss: 0.7758 (0.6336)  time: 0.1517  data: 0.0002  max mem: 6820\n[04:48:04.447890] val:  [110/139]  eta: 0:00:04  loss: 0.7776 (0.6430)  time: 0.1519  data: 0.0002  max mem: 6820\n[04:48:05.968688] val:  [120/139]  eta: 0:00:03  loss: 0.6268 (0.6387)  time: 0.1520  data: 0.0002  max mem: 6820\n[04:48:07.485443] val:  [130/139]  eta: 0:00:01  loss: 0.5700 (0.6256)  time: 0.1518  data: 0.0001  max mem: 6820\n[04:48:08.600769] val:  [138/139]  eta: 0:00:00  loss: 0.4439 (0.6162)  time: 0.1467  data: 0.0001  max mem: 6820\n[04:48:08.708332] val: Total time: 0:00:21 (0.1580 s / it)\n[04:48:08.769582] val loss: 0.616197174401592\n[04:48:08.769649] Accuracy: 0.6537, F1 Score: 0.6485, ROC AUC: 0.7152, Hamming Loss: 0.3463,\n Jaccard Score: 0.4814, Precision: 0.6633, Recall: 0.6537,\n Average Precision: 0.7140, Kappa: 0.3074, Score: 0.5570\n[04:48:08.771648] Best epoch = 45, Best score = 0.5605\n[04:48:08.774450] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[04:48:10.126661] Epoch: [47]  [  0/969]  eta: 0:21:48  lr: 0.000143  loss: 0.5993 (0.5993)  time: 1.3509  data: 0.7531  max mem: 6820\n[04:48:21.519856] Epoch: [47]  [ 20/969]  eta: 0:09:35  lr: 0.000143  loss: 0.6415 (0.6457)  time: 0.5696  data: 0.0002  max mem: 6820\n[04:48:32.925441] Epoch: [47]  [ 40/969]  eta: 0:09:07  lr: 0.000143  loss: 0.6474 (0.6502)  time: 0.5702  data: 0.0002  max mem: 6820\n[04:48:44.370245] Epoch: [47]  [ 60/969]  eta: 0:08:50  lr: 0.000142  loss: 0.6644 (0.6560)  time: 0.5722  data: 0.0002  max mem: 6820\n[04:48:55.785074] Epoch: [47]  [ 80/969]  eta: 0:08:35  lr: 0.000142  loss: 0.6684 (0.6592)  time: 0.5707  data: 0.0002  max mem: 6820\n[04:49:07.190674] Epoch: [47]  [100/969]  eta: 0:08:22  lr: 0.000142  loss: 0.6677 (0.6611)  time: 0.5702  data: 0.0002  max mem: 6820\n[04:49:18.579670] Epoch: [47]  [120/969]  eta: 0:08:09  lr: 0.000142  loss: 0.6426 (0.6612)  time: 0.5694  data: 0.0002  max mem: 6820\n[04:49:29.948777] Epoch: [47]  [140/969]  eta: 0:07:57  lr: 0.000142  loss: 0.6362 (0.6619)  time: 0.5684  data: 0.0002  max mem: 6820\n[04:49:41.322587] Epoch: [47]  [160/969]  eta: 0:07:44  lr: 0.000142  loss: 0.6592 (0.6625)  time: 0.5686  data: 0.0002  max mem: 6820\n[04:49:52.697847] Epoch: [47]  [180/969]  eta: 0:07:32  lr: 0.000141  loss: 0.6649 (0.6637)  time: 0.5687  data: 0.0002  max mem: 6820\n[04:50:04.066268] Epoch: [47]  [200/969]  eta: 0:07:21  lr: 0.000141  loss: 0.6781 (0.6653)  time: 0.5684  data: 0.0002  max mem: 6820\n[04:50:15.415515] Epoch: [47]  [220/969]  eta: 0:07:09  lr: 0.000141  loss: 0.6601 (0.6649)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:50:26.783954] Epoch: [47]  [240/969]  eta: 0:06:57  lr: 0.000141  loss: 0.6283 (0.6630)  time: 0.5683  data: 0.0002  max mem: 6820\n[04:50:38.119078] Epoch: [47]  [260/969]  eta: 0:06:45  lr: 0.000141  loss: 0.6576 (0.6623)  time: 0.5667  data: 0.0002  max mem: 6820\n[04:50:49.507712] Epoch: [47]  [280/969]  eta: 0:06:34  lr: 0.000141  loss: 0.6759 (0.6631)  time: 0.5694  data: 0.0002  max mem: 6820\n[04:51:00.911066] Epoch: [47]  [300/969]  eta: 0:06:22  lr: 0.000141  loss: 0.6636 (0.6631)  time: 0.5701  data: 0.0002  max mem: 6820\n[04:51:12.305232] Epoch: [47]  [320/969]  eta: 0:06:11  lr: 0.000140  loss: 0.6238 (0.6608)  time: 0.5697  data: 0.0002  max mem: 6820\n[04:51:23.712746] Epoch: [47]  [340/969]  eta: 0:05:59  lr: 0.000140  loss: 0.6346 (0.6601)  time: 0.5703  data: 0.0002  max mem: 6820\n[04:51:35.118264] Epoch: [47]  [360/969]  eta: 0:05:48  lr: 0.000140  loss: 0.6656 (0.6605)  time: 0.5702  data: 0.0002  max mem: 6820\n[04:51:46.533895] Epoch: [47]  [380/969]  eta: 0:05:36  lr: 0.000140  loss: 0.6883 (0.6618)  time: 0.5707  data: 0.0002  max mem: 6820\n[04:51:57.951699] Epoch: [47]  [400/969]  eta: 0:05:25  lr: 0.000140  loss: 0.6576 (0.6621)  time: 0.5708  data: 0.0002  max mem: 6820\n[04:52:09.363500] Epoch: [47]  [420/969]  eta: 0:05:13  lr: 0.000140  loss: 0.6612 (0.6617)  time: 0.5705  data: 0.0002  max mem: 6820\n[04:52:20.747239] Epoch: [47]  [440/969]  eta: 0:05:02  lr: 0.000140  loss: 0.6640 (0.6610)  time: 0.5691  data: 0.0002  max mem: 6820\n[04:52:32.147093] Epoch: [47]  [460/969]  eta: 0:04:50  lr: 0.000139  loss: 0.6570 (0.6612)  time: 0.5699  data: 0.0002  max mem: 6820\n[04:52:43.521939] Epoch: [47]  [480/969]  eta: 0:04:39  lr: 0.000139  loss: 0.6636 (0.6610)  time: 0.5687  data: 0.0002  max mem: 6820\n[04:52:54.918111] Epoch: [47]  [500/969]  eta: 0:04:27  lr: 0.000139  loss: 0.6512 (0.6609)  time: 0.5698  data: 0.0002  max mem: 6820\n[04:53:06.302890] Epoch: [47]  [520/969]  eta: 0:04:16  lr: 0.000139  loss: 0.6386 (0.6604)  time: 0.5692  data: 0.0002  max mem: 6820\n[04:53:17.689741] Epoch: [47]  [540/969]  eta: 0:04:04  lr: 0.000139  loss: 0.6302 (0.6600)  time: 0.5693  data: 0.0002  max mem: 6820\n[04:53:29.062938] Epoch: [47]  [560/969]  eta: 0:03:53  lr: 0.000139  loss: 0.6690 (0.6601)  time: 0.5686  data: 0.0002  max mem: 6820\n[04:53:40.444603] Epoch: [47]  [580/969]  eta: 0:03:42  lr: 0.000139  loss: 0.6647 (0.6603)  time: 0.5690  data: 0.0002  max mem: 6820\n[04:53:51.837486] Epoch: [47]  [600/969]  eta: 0:03:30  lr: 0.000138  loss: 0.6566 (0.6607)  time: 0.5696  data: 0.0002  max mem: 6820\n[04:54:03.263536] Epoch: [47]  [620/969]  eta: 0:03:19  lr: 0.000138  loss: 0.6684 (0.6608)  time: 0.5713  data: 0.0002  max mem: 6820\n[04:54:14.688161] Epoch: [47]  [640/969]  eta: 0:03:07  lr: 0.000138  loss: 0.6428 (0.6604)  time: 0.5712  data: 0.0002  max mem: 6820\n[04:54:26.124115] Epoch: [47]  [660/969]  eta: 0:02:56  lr: 0.000138  loss: 0.6263 (0.6598)  time: 0.5718  data: 0.0002  max mem: 6820\n[04:54:37.540214] Epoch: [47]  [680/969]  eta: 0:02:44  lr: 0.000138  loss: 0.6843 (0.6605)  time: 0.5707  data: 0.0002  max mem: 6820\n[04:54:48.962518] Epoch: [47]  [700/969]  eta: 0:02:33  lr: 0.000138  loss: 0.6314 (0.6602)  time: 0.5711  data: 0.0001  max mem: 6820\n[04:55:00.398913] Epoch: [47]  [720/969]  eta: 0:02:22  lr: 0.000138  loss: 0.6552 (0.6607)  time: 0.5718  data: 0.0002  max mem: 6820\n[04:55:11.820961] Epoch: [47]  [740/969]  eta: 0:02:10  lr: 0.000137  loss: 0.6664 (0.6608)  time: 0.5711  data: 0.0001  max mem: 6820\n[04:55:23.230247] Epoch: [47]  [760/969]  eta: 0:01:59  lr: 0.000137  loss: 0.6563 (0.6609)  time: 0.5704  data: 0.0002  max mem: 6820\n[04:55:34.614328] Epoch: [47]  [780/969]  eta: 0:01:47  lr: 0.000137  loss: 0.6600 (0.6610)  time: 0.5692  data: 0.0002  max mem: 6820\n[04:55:45.979847] Epoch: [47]  [800/969]  eta: 0:01:36  lr: 0.000137  loss: 0.6673 (0.6614)  time: 0.5682  data: 0.0002  max mem: 6820\n[04:55:57.336357] Epoch: [47]  [820/969]  eta: 0:01:25  lr: 0.000137  loss: 0.6552 (0.6613)  time: 0.5678  data: 0.0002  max mem: 6820\n[04:56:08.685783] Epoch: [47]  [840/969]  eta: 0:01:13  lr: 0.000137  loss: 0.6440 (0.6612)  time: 0.5674  data: 0.0002  max mem: 6820\n[04:56:20.057257] Epoch: [47]  [860/969]  eta: 0:01:02  lr: 0.000137  loss: 0.6496 (0.6610)  time: 0.5685  data: 0.0002  max mem: 6820\n[04:56:31.423304] Epoch: [47]  [880/969]  eta: 0:00:50  lr: 0.000136  loss: 0.6589 (0.6609)  time: 0.5683  data: 0.0002  max mem: 6820\n[04:56:42.754457] Epoch: [47]  [900/969]  eta: 0:00:39  lr: 0.000136  loss: 0.6748 (0.6613)  time: 0.5665  data: 0.0002  max mem: 6820\n[04:56:54.094996] Epoch: [47]  [920/969]  eta: 0:00:27  lr: 0.000136  loss: 0.6501 (0.6610)  time: 0.5670  data: 0.0002  max mem: 6820\n[04:57:05.431283] Epoch: [47]  [940/969]  eta: 0:00:16  lr: 0.000136  loss: 0.6185 (0.6607)  time: 0.5668  data: 0.0002  max mem: 6820\n[04:57:16.782420] Epoch: [47]  [960/969]  eta: 0:00:05  lr: 0.000136  loss: 0.6454 (0.6606)  time: 0.5675  data: 0.0002  max mem: 6820\n[04:57:21.327363] Epoch: [47]  [968/969]  eta: 0:00:00  lr: 0.000136  loss: 0.6319 (0.6605)  time: 0.5671  data: 0.0002  max mem: 6820\n[04:57:21.454620] Epoch: [47] Total time: 0:09:12 (0.5704 s / it)\n[04:57:21.454738] Averaged stats: lr: 0.000136  loss: 0.6319 (0.6605)\n[04:57:22.234594] val:  [  0/139]  eta: 0:01:47  loss: 0.6043 (0.6043)  time: 0.7750  data: 0.6344  max mem: 6820\n[04:57:23.731243] val:  [ 10/139]  eta: 0:00:26  loss: 0.6043 (0.5579)  time: 0.2064  data: 0.0582  max mem: 6820\n[04:57:25.230466] val:  [ 20/139]  eta: 0:00:21  loss: 0.5050 (0.5310)  time: 0.1497  data: 0.0004  max mem: 6820\n[04:57:26.737024] val:  [ 30/139]  eta: 0:00:18  loss: 0.5050 (0.5248)  time: 0.1502  data: 0.0002  max mem: 6820\n[04:57:28.251698] val:  [ 40/139]  eta: 0:00:16  loss: 0.5916 (0.5614)  time: 0.1510  data: 0.0002  max mem: 6820\n[04:57:29.760434] val:  [ 50/139]  eta: 0:00:14  loss: 0.6207 (0.5780)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:57:31.274093] val:  [ 60/139]  eta: 0:00:12  loss: 0.5918 (0.5813)  time: 0.1510  data: 0.0002  max mem: 6820\n[04:57:32.788065] val:  [ 70/139]  eta: 0:00:11  loss: 0.5948 (0.5880)  time: 0.1513  data: 0.0002  max mem: 6820\n[04:57:34.298403] val:  [ 80/139]  eta: 0:00:09  loss: 0.6348 (0.6041)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:57:35.810927] val:  [ 90/139]  eta: 0:00:07  loss: 0.7782 (0.6251)  time: 0.1511  data: 0.0002  max mem: 6820\n[04:57:37.335477] val:  [100/139]  eta: 0:00:06  loss: 0.7789 (0.6400)  time: 0.1518  data: 0.0002  max mem: 6820\n[04:57:38.845434] val:  [110/139]  eta: 0:00:04  loss: 0.7621 (0.6474)  time: 0.1516  data: 0.0002  max mem: 6820\n[04:57:40.360809] val:  [120/139]  eta: 0:00:02  loss: 0.6402 (0.6413)  time: 0.1512  data: 0.0002  max mem: 6820\n[04:57:41.876951] val:  [130/139]  eta: 0:00:01  loss: 0.5457 (0.6251)  time: 0.1515  data: 0.0001  max mem: 6820\n[04:57:42.991258] val:  [138/139]  eta: 0:00:00  loss: 0.3986 (0.6137)  time: 0.1466  data: 0.0001  max mem: 6820\n[04:57:43.100485] val: Total time: 0:00:21 (0.1557 s / it)\n[04:57:43.156554] val loss: 0.6136769973974434\n[04:57:43.156616] Accuracy: 0.6519, F1 Score: 0.6487, ROC AUC: 0.7189, Hamming Loss: 0.3481,\n Jaccard Score: 0.4809, Precision: 0.6577, Recall: 0.6519,\n Average Precision: 0.7167, Kappa: 0.3038, Score: 0.5571\n[04:57:43.158354] Best epoch = 45, Best score = 0.5605\n[04:57:43.160794] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[04:57:44.456320] Epoch: [48]  [  0/969]  eta: 0:20:54  lr: 0.000136  loss: 0.5844 (0.5844)  time: 1.2942  data: 0.7278  max mem: 6820\n[04:57:55.813253] Epoch: [48]  [ 20/969]  eta: 0:09:31  lr: 0.000136  loss: 0.6357 (0.6641)  time: 0.5678  data: 0.0002  max mem: 6820\n[04:58:07.202890] Epoch: [48]  [ 40/969]  eta: 0:09:04  lr: 0.000136  loss: 0.6454 (0.6631)  time: 0.5694  data: 0.0002  max mem: 6820\n[04:58:18.640963] Epoch: [48]  [ 60/969]  eta: 0:08:48  lr: 0.000135  loss: 0.6565 (0.6656)  time: 0.5719  data: 0.0002  max mem: 6820\n[04:58:30.117082] Epoch: [48]  [ 80/969]  eta: 0:08:35  lr: 0.000135  loss: 0.6527 (0.6643)  time: 0.5738  data: 0.0002  max mem: 6820\n[04:58:41.575007] Epoch: [48]  [100/969]  eta: 0:08:22  lr: 0.000135  loss: 0.6837 (0.6706)  time: 0.5728  data: 0.0002  max mem: 6820\n[04:58:53.017723] Epoch: [48]  [120/969]  eta: 0:08:10  lr: 0.000135  loss: 0.6697 (0.6693)  time: 0.5721  data: 0.0002  max mem: 6820\n[04:59:04.440231] Epoch: [48]  [140/969]  eta: 0:07:57  lr: 0.000135  loss: 0.6317 (0.6660)  time: 0.5711  data: 0.0002  max mem: 6820\n[04:59:15.838219] Epoch: [48]  [160/969]  eta: 0:07:45  lr: 0.000135  loss: 0.6562 (0.6661)  time: 0.5698  data: 0.0002  max mem: 6820\n[04:59:27.251296] Epoch: [48]  [180/969]  eta: 0:07:33  lr: 0.000135  loss: 0.6633 (0.6672)  time: 0.5706  data: 0.0002  max mem: 6820\n[04:59:38.657820] Epoch: [48]  [200/969]  eta: 0:07:21  lr: 0.000134  loss: 0.6693 (0.6671)  time: 0.5703  data: 0.0002  max mem: 6820\n[04:59:50.048191] Epoch: [48]  [220/969]  eta: 0:07:09  lr: 0.000134  loss: 0.6683 (0.6683)  time: 0.5695  data: 0.0002  max mem: 6820\n[05:00:01.410512] Epoch: [48]  [240/969]  eta: 0:06:58  lr: 0.000134  loss: 0.6658 (0.6676)  time: 0.5681  data: 0.0002  max mem: 6820\n[05:00:12.754143] Epoch: [48]  [260/969]  eta: 0:06:46  lr: 0.000134  loss: 0.6490 (0.6671)  time: 0.5671  data: 0.0002  max mem: 6820\n[05:00:24.122298] Epoch: [48]  [280/969]  eta: 0:06:34  lr: 0.000134  loss: 0.6576 (0.6666)  time: 0.5684  data: 0.0002  max mem: 6820\n[05:00:35.472299] Epoch: [48]  [300/969]  eta: 0:06:22  lr: 0.000134  loss: 0.6582 (0.6668)  time: 0.5675  data: 0.0002  max mem: 6820\n[05:00:46.842693] Epoch: [48]  [320/969]  eta: 0:06:11  lr: 0.000134  loss: 0.6347 (0.6657)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:00:58.207061] Epoch: [48]  [340/969]  eta: 0:05:59  lr: 0.000133  loss: 0.6474 (0.6656)  time: 0.5682  data: 0.0002  max mem: 6820\n[05:01:09.573990] Epoch: [48]  [360/969]  eta: 0:05:48  lr: 0.000133  loss: 0.6554 (0.6658)  time: 0.5683  data: 0.0002  max mem: 6820\n[05:01:20.931655] Epoch: [48]  [380/969]  eta: 0:05:36  lr: 0.000133  loss: 0.6517 (0.6652)  time: 0.5678  data: 0.0002  max mem: 6820\n[05:01:32.185844] Epoch: [48]  [400/969]  eta: 0:05:24  lr: 0.000133  loss: 0.6245 (0.6638)  time: 0.5627  data: 0.0002  max mem: 6820\n[05:01:43.538239] Epoch: [48]  [420/969]  eta: 0:05:13  lr: 0.000133  loss: 0.6309 (0.6632)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:01:54.866485] Epoch: [48]  [440/969]  eta: 0:05:01  lr: 0.000133  loss: 0.6684 (0.6637)  time: 0.5664  data: 0.0002  max mem: 6820\n[05:02:06.213308] Epoch: [48]  [460/969]  eta: 0:04:50  lr: 0.000133  loss: 0.6300 (0.6633)  time: 0.5673  data: 0.0002  max mem: 6820\n[05:02:17.577705] Epoch: [48]  [480/969]  eta: 0:04:38  lr: 0.000132  loss: 0.6496 (0.6628)  time: 0.5682  data: 0.0002  max mem: 6820\n[05:02:28.935288] Epoch: [48]  [500/969]  eta: 0:04:27  lr: 0.000132  loss: 0.6452 (0.6627)  time: 0.5678  data: 0.0002  max mem: 6820\n[05:02:40.296491] Epoch: [48]  [520/969]  eta: 0:04:16  lr: 0.000132  loss: 0.6512 (0.6620)  time: 0.5680  data: 0.0002  max mem: 6820\n[05:02:51.641861] Epoch: [48]  [540/969]  eta: 0:04:04  lr: 0.000132  loss: 0.6659 (0.6618)  time: 0.5672  data: 0.0002  max mem: 6820\n[05:03:03.028147] Epoch: [48]  [560/969]  eta: 0:03:53  lr: 0.000132  loss: 0.6470 (0.6617)  time: 0.5693  data: 0.0002  max mem: 6820\n[05:03:14.420217] Epoch: [48]  [580/969]  eta: 0:03:41  lr: 0.000132  loss: 0.6781 (0.6622)  time: 0.5696  data: 0.0002  max mem: 6820\n[05:03:25.829537] Epoch: [48]  [600/969]  eta: 0:03:30  lr: 0.000132  loss: 0.6764 (0.6627)  time: 0.5704  data: 0.0002  max mem: 6820\n[05:03:37.204135] Epoch: [48]  [620/969]  eta: 0:03:18  lr: 0.000131  loss: 0.6656 (0.6628)  time: 0.5687  data: 0.0002  max mem: 6820\n[05:03:48.589085] Epoch: [48]  [640/969]  eta: 0:03:07  lr: 0.000131  loss: 0.6711 (0.6630)  time: 0.5692  data: 0.0002  max mem: 6820\n[05:03:59.999087] Epoch: [48]  [660/969]  eta: 0:02:56  lr: 0.000131  loss: 0.6545 (0.6629)  time: 0.5705  data: 0.0002  max mem: 6820\n[05:04:11.430860] Epoch: [48]  [680/969]  eta: 0:02:44  lr: 0.000131  loss: 0.6417 (0.6627)  time: 0.5715  data: 0.0002  max mem: 6820\n[05:04:22.822454] Epoch: [48]  [700/969]  eta: 0:02:33  lr: 0.000131  loss: 0.6355 (0.6620)  time: 0.5695  data: 0.0002  max mem: 6820\n[05:04:34.228253] Epoch: [48]  [720/969]  eta: 0:02:21  lr: 0.000131  loss: 0.6343 (0.6615)  time: 0.5702  data: 0.0002  max mem: 6820\n[05:04:45.623840] Epoch: [48]  [740/969]  eta: 0:02:10  lr: 0.000131  loss: 0.6737 (0.6613)  time: 0.5697  data: 0.0002  max mem: 6820\n[05:04:56.983792] Epoch: [48]  [760/969]  eta: 0:01:59  lr: 0.000130  loss: 0.6514 (0.6610)  time: 0.5679  data: 0.0002  max mem: 6820\n[05:05:08.364419] Epoch: [48]  [780/969]  eta: 0:01:47  lr: 0.000130  loss: 0.6823 (0.6611)  time: 0.5690  data: 0.0002  max mem: 6820\n[05:05:19.732401] Epoch: [48]  [800/969]  eta: 0:01:36  lr: 0.000130  loss: 0.6564 (0.6618)  time: 0.5683  data: 0.0002  max mem: 6820\n[05:05:31.084520] Epoch: [48]  [820/969]  eta: 0:01:24  lr: 0.000130  loss: 0.6579 (0.6616)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:05:42.459056] Epoch: [48]  [840/969]  eta: 0:01:13  lr: 0.000130  loss: 0.6464 (0.6614)  time: 0.5687  data: 0.0002  max mem: 6820\n[05:05:53.800851] Epoch: [48]  [860/969]  eta: 0:01:02  lr: 0.000130  loss: 0.6697 (0.6615)  time: 0.5670  data: 0.0002  max mem: 6820\n[05:06:05.173662] Epoch: [48]  [880/969]  eta: 0:00:50  lr: 0.000130  loss: 0.6514 (0.6614)  time: 0.5686  data: 0.0002  max mem: 6820\n[05:06:16.542397] Epoch: [48]  [900/969]  eta: 0:00:39  lr: 0.000129  loss: 0.6485 (0.6616)  time: 0.5684  data: 0.0002  max mem: 6820\n[05:06:27.879582] Epoch: [48]  [920/969]  eta: 0:00:27  lr: 0.000129  loss: 0.6440 (0.6612)  time: 0.5668  data: 0.0002  max mem: 6820\n[05:06:39.235786] Epoch: [48]  [940/969]  eta: 0:00:16  lr: 0.000129  loss: 0.6463 (0.6610)  time: 0.5678  data: 0.0002  max mem: 6820\n[05:06:50.588797] Epoch: [48]  [960/969]  eta: 0:00:05  lr: 0.000129  loss: 0.6594 (0.6611)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:06:55.122932] Epoch: [48]  [968/969]  eta: 0:00:00  lr: 0.000129  loss: 0.6744 (0.6613)  time: 0.5670  data: 0.0001  max mem: 6820\n[05:06:55.247559] Epoch: [48] Total time: 0:09:12 (0.5697 s / it)\n[05:06:55.247670] Averaged stats: lr: 0.000129  loss: 0.6744 (0.6613)\n[05:06:56.091644] val:  [  0/139]  eta: 0:01:56  loss: 0.6258 (0.6258)  time: 0.8364  data: 0.7029  max mem: 6820\n[05:06:57.582965] val:  [ 10/139]  eta: 0:00:27  loss: 0.6258 (0.5783)  time: 0.2115  data: 0.0641  max mem: 6820\n[05:06:59.085172] val:  [ 20/139]  eta: 0:00:21  loss: 0.5644 (0.5619)  time: 0.1496  data: 0.0002  max mem: 6820\n[05:07:00.590892] val:  [ 30/139]  eta: 0:00:18  loss: 0.5174 (0.5522)  time: 0.1503  data: 0.0002  max mem: 6820\n[05:07:02.097029] val:  [ 40/139]  eta: 0:00:16  loss: 0.5943 (0.5779)  time: 0.1505  data: 0.0002  max mem: 6820\n[05:07:03.608387] val:  [ 50/139]  eta: 0:00:14  loss: 0.6123 (0.5881)  time: 0.1508  data: 0.0002  max mem: 6820\n[05:07:05.119453] val:  [ 60/139]  eta: 0:00:12  loss: 0.5973 (0.5896)  time: 0.1510  data: 0.0002  max mem: 6820\n[05:07:06.632013] val:  [ 70/139]  eta: 0:00:11  loss: 0.5953 (0.5925)  time: 0.1511  data: 0.0002  max mem: 6820\n[05:07:08.139354] val:  [ 80/139]  eta: 0:00:09  loss: 0.6170 (0.6035)  time: 0.1509  data: 0.0002  max mem: 6820\n[05:07:09.659009] val:  [ 90/139]  eta: 0:00:07  loss: 0.7611 (0.6221)  time: 0.1513  data: 0.0002  max mem: 6820\n[05:07:11.178223] val:  [100/139]  eta: 0:00:06  loss: 0.7691 (0.6361)  time: 0.1519  data: 0.0002  max mem: 6820\n[05:07:12.686107] val:  [110/139]  eta: 0:00:04  loss: 0.7524 (0.6444)  time: 0.1513  data: 0.0002  max mem: 6820\n[05:07:14.200138] val:  [120/139]  eta: 0:00:02  loss: 0.6044 (0.6376)  time: 0.1510  data: 0.0002  max mem: 6820\n[05:07:15.717172] val:  [130/139]  eta: 0:00:01  loss: 0.5130 (0.6231)  time: 0.1515  data: 0.0001  max mem: 6820\n[05:07:16.832369] val:  [138/139]  eta: 0:00:00  loss: 0.4305 (0.6129)  time: 0.1467  data: 0.0001  max mem: 6820\n[05:07:16.935531] val: Total time: 0:00:21 (0.1560 s / it)\n[05:07:16.990550] val loss: 0.6129055297632011\n[05:07:16.990609] Accuracy: 0.6564, F1 Score: 0.6533, ROC AUC: 0.7213, Hamming Loss: 0.3436,\n Jaccard Score: 0.4860, Precision: 0.6623, Recall: 0.6564,\n Average Precision: 0.7184, Kappa: 0.3128, Score: 0.5625\n[05:07:19.623857] Best epoch = 48, Best score = 0.5625\n[05:07:19.626586] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[05:07:21.004962] Epoch: [49]  [  0/969]  eta: 0:22:14  lr: 0.000129  loss: 0.5622 (0.5622)  time: 1.3770  data: 0.7672  max mem: 6820\n[05:07:32.298799] Epoch: [49]  [ 20/969]  eta: 0:09:32  lr: 0.000129  loss: 0.6665 (0.6452)  time: 0.5646  data: 0.0002  max mem: 6820\n[05:07:43.739262] Epoch: [49]  [ 40/969]  eta: 0:09:06  lr: 0.000129  loss: 0.6314 (0.6489)  time: 0.5720  data: 0.0002  max mem: 6820\n[05:07:55.297711] Epoch: [49]  [ 60/969]  eta: 0:08:51  lr: 0.000129  loss: 0.6661 (0.6576)  time: 0.5779  data: 0.0002  max mem: 6820\n[05:08:06.806050] Epoch: [49]  [ 80/969]  eta: 0:08:37  lr: 0.000128  loss: 0.6349 (0.6557)  time: 0.5754  data: 0.0002  max mem: 6820\n[05:08:18.220162] Epoch: [49]  [100/969]  eta: 0:08:24  lr: 0.000128  loss: 0.6670 (0.6609)  time: 0.5707  data: 0.0002  max mem: 6820\n[05:08:29.554091] Epoch: [49]  [120/969]  eta: 0:08:10  lr: 0.000128  loss: 0.6766 (0.6632)  time: 0.5666  data: 0.0002  max mem: 6820\n[05:08:40.856779] Epoch: [49]  [140/969]  eta: 0:07:57  lr: 0.000128  loss: 0.6586 (0.6626)  time: 0.5651  data: 0.0001  max mem: 6820\n[05:08:52.182366] Epoch: [49]  [160/969]  eta: 0:07:45  lr: 0.000128  loss: 0.6652 (0.6636)  time: 0.5662  data: 0.0002  max mem: 6820\n[05:09:03.577625] Epoch: [49]  [180/969]  eta: 0:07:33  lr: 0.000128  loss: 0.6505 (0.6621)  time: 0.5697  data: 0.0002  max mem: 6820\n[05:09:14.975862] Epoch: [49]  [200/969]  eta: 0:07:21  lr: 0.000128  loss: 0.6653 (0.6619)  time: 0.5699  data: 0.0002  max mem: 6820\n[05:09:26.418567] Epoch: [49]  [220/969]  eta: 0:07:09  lr: 0.000127  loss: 0.6669 (0.6616)  time: 0.5721  data: 0.0002  max mem: 6820\n[05:09:37.888484] Epoch: [49]  [240/969]  eta: 0:06:58  lr: 0.000127  loss: 0.6415 (0.6607)  time: 0.5734  data: 0.0002  max mem: 6820\n[05:09:49.313483] Epoch: [49]  [260/969]  eta: 0:06:46  lr: 0.000127  loss: 0.6609 (0.6607)  time: 0.5712  data: 0.0002  max mem: 6820\n[05:10:00.723181] Epoch: [49]  [280/969]  eta: 0:06:34  lr: 0.000127  loss: 0.6687 (0.6606)  time: 0.5704  data: 0.0002  max mem: 6820\n[05:10:12.135995] Epoch: [49]  [300/969]  eta: 0:06:23  lr: 0.000127  loss: 0.6738 (0.6608)  time: 0.5706  data: 0.0002  max mem: 6820\n[05:10:23.534885] Epoch: [49]  [320/969]  eta: 0:06:11  lr: 0.000127  loss: 0.6380 (0.6600)  time: 0.5699  data: 0.0002  max mem: 6820\n[05:10:34.917465] Epoch: [49]  [340/969]  eta: 0:06:00  lr: 0.000127  loss: 0.6390 (0.6599)  time: 0.5691  data: 0.0002  max mem: 6820\n[05:10:46.283396] Epoch: [49]  [360/969]  eta: 0:05:48  lr: 0.000126  loss: 0.6645 (0.6604)  time: 0.5682  data: 0.0002  max mem: 6820\n[05:10:57.654380] Epoch: [49]  [380/969]  eta: 0:05:37  lr: 0.000126  loss: 0.6427 (0.6592)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:11:09.025178] Epoch: [49]  [400/969]  eta: 0:05:25  lr: 0.000126  loss: 0.6428 (0.6591)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:11:20.399313] Epoch: [49]  [420/969]  eta: 0:05:13  lr: 0.000126  loss: 0.6591 (0.6596)  time: 0.5687  data: 0.0002  max mem: 6820\n[05:11:31.759533] Epoch: [49]  [440/969]  eta: 0:05:02  lr: 0.000126  loss: 0.6756 (0.6601)  time: 0.5680  data: 0.0002  max mem: 6820\n[05:11:43.120093] Epoch: [49]  [460/969]  eta: 0:04:50  lr: 0.000126  loss: 0.6605 (0.6600)  time: 0.5680  data: 0.0002  max mem: 6820\n[05:11:54.488129] Epoch: [49]  [480/969]  eta: 0:04:39  lr: 0.000126  loss: 0.6633 (0.6605)  time: 0.5684  data: 0.0002  max mem: 6820\n[05:12:05.830806] Epoch: [49]  [500/969]  eta: 0:04:27  lr: 0.000125  loss: 0.6510 (0.6603)  time: 0.5671  data: 0.0002  max mem: 6820\n[05:12:17.183804] Epoch: [49]  [520/969]  eta: 0:04:16  lr: 0.000125  loss: 0.6364 (0.6595)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:12:28.535316] Epoch: [49]  [540/969]  eta: 0:04:04  lr: 0.000125  loss: 0.6264 (0.6582)  time: 0.5675  data: 0.0002  max mem: 6820\n[05:12:39.892031] Epoch: [49]  [560/969]  eta: 0:03:53  lr: 0.000125  loss: 0.6191 (0.6580)  time: 0.5678  data: 0.0002  max mem: 6820\n[05:12:51.234126] Epoch: [49]  [580/969]  eta: 0:03:42  lr: 0.000125  loss: 0.6636 (0.6580)  time: 0.5671  data: 0.0002  max mem: 6820\n[05:13:02.593843] Epoch: [49]  [600/969]  eta: 0:03:30  lr: 0.000125  loss: 0.6359 (0.6582)  time: 0.5679  data: 0.0002  max mem: 6820\n[05:13:13.949270] Epoch: [49]  [620/969]  eta: 0:03:19  lr: 0.000125  loss: 0.6364 (0.6581)  time: 0.5677  data: 0.0002  max mem: 6820\n[05:13:25.304878] Epoch: [49]  [640/969]  eta: 0:03:07  lr: 0.000124  loss: 0.6509 (0.6581)  time: 0.5677  data: 0.0002  max mem: 6820\n[05:13:36.663614] Epoch: [49]  [660/969]  eta: 0:02:56  lr: 0.000124  loss: 0.6261 (0.6575)  time: 0.5679  data: 0.0002  max mem: 6820\n[05:13:48.033904] Epoch: [49]  [680/969]  eta: 0:02:44  lr: 0.000124  loss: 0.6590 (0.6577)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:13:59.416687] Epoch: [49]  [700/969]  eta: 0:02:33  lr: 0.000124  loss: 0.6506 (0.6577)  time: 0.5691  data: 0.0002  max mem: 6820\n[05:14:10.815581] Epoch: [49]  [720/969]  eta: 0:02:21  lr: 0.000124  loss: 0.6555 (0.6580)  time: 0.5699  data: 0.0002  max mem: 6820\n[05:14:22.225220] Epoch: [49]  [740/969]  eta: 0:02:10  lr: 0.000124  loss: 0.6576 (0.6581)  time: 0.5704  data: 0.0002  max mem: 6820\n[05:14:33.629695] Epoch: [49]  [760/969]  eta: 0:01:59  lr: 0.000124  loss: 0.6691 (0.6584)  time: 0.5702  data: 0.0002  max mem: 6820\n[05:14:45.036789] Epoch: [49]  [780/969]  eta: 0:01:47  lr: 0.000123  loss: 0.6563 (0.6584)  time: 0.5703  data: 0.0002  max mem: 6820\n[05:14:56.408133] Epoch: [49]  [800/969]  eta: 0:01:36  lr: 0.000123  loss: 0.6643 (0.6588)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:15:07.773742] Epoch: [49]  [820/969]  eta: 0:01:24  lr: 0.000123  loss: 0.6490 (0.6587)  time: 0.5682  data: 0.0002  max mem: 6820\n[05:15:19.124472] Epoch: [49]  [840/969]  eta: 0:01:13  lr: 0.000123  loss: 0.6348 (0.6584)  time: 0.5675  data: 0.0002  max mem: 6820\n[05:15:30.497331] Epoch: [49]  [860/969]  eta: 0:01:02  lr: 0.000123  loss: 0.6652 (0.6586)  time: 0.5686  data: 0.0002  max mem: 6820\n[05:15:41.868522] Epoch: [49]  [880/969]  eta: 0:00:50  lr: 0.000123  loss: 0.6658 (0.6584)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:15:53.221113] Epoch: [49]  [900/969]  eta: 0:00:39  lr: 0.000123  loss: 0.6512 (0.6587)  time: 0.5676  data: 0.0001  max mem: 6820\n[05:16:04.568157] Epoch: [49]  [920/969]  eta: 0:00:27  lr: 0.000122  loss: 0.6275 (0.6582)  time: 0.5673  data: 0.0002  max mem: 6820\n[05:16:15.920190] Epoch: [49]  [940/969]  eta: 0:00:16  lr: 0.000122  loss: 0.6109 (0.6577)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:16:27.284907] Epoch: [49]  [960/969]  eta: 0:00:05  lr: 0.000122  loss: 0.6748 (0.6580)  time: 0.5682  data: 0.0002  max mem: 6820\n[05:16:31.825446] Epoch: [49]  [968/969]  eta: 0:00:00  lr: 0.000122  loss: 0.6569 (0.6580)  time: 0.5686  data: 0.0001  max mem: 6820\n[05:16:31.948115] Epoch: [49] Total time: 0:09:12 (0.5700 s / it)\n[05:16:31.948229] Averaged stats: lr: 0.000122  loss: 0.6569 (0.6580)\n[05:16:32.676730] val:  [  0/139]  eta: 0:01:40  loss: 0.6166 (0.6166)  time: 0.7236  data: 0.5908  max mem: 6820\n[05:16:34.176702] val:  [ 10/139]  eta: 0:00:26  loss: 0.6076 (0.5569)  time: 0.2020  data: 0.0540  max mem: 6820\n[05:16:35.678558] val:  [ 20/139]  eta: 0:00:21  loss: 0.5110 (0.5347)  time: 0.1500  data: 0.0002  max mem: 6820\n[05:16:37.191139] val:  [ 30/139]  eta: 0:00:18  loss: 0.5008 (0.5274)  time: 0.1506  data: 0.0002  max mem: 6820\n[05:16:38.703394] val:  [ 40/139]  eta: 0:00:16  loss: 0.5942 (0.5606)  time: 0.1512  data: 0.0002  max mem: 6820\n[05:16:40.213330] val:  [ 50/139]  eta: 0:00:14  loss: 0.6144 (0.5749)  time: 0.1510  data: 0.0002  max mem: 6820\n[05:16:41.725491] val:  [ 60/139]  eta: 0:00:12  loss: 0.6032 (0.5803)  time: 0.1510  data: 0.0002  max mem: 6820\n[05:16:43.241704] val:  [ 70/139]  eta: 0:00:10  loss: 0.6032 (0.5874)  time: 0.1513  data: 0.0002  max mem: 6820\n[05:16:44.747084] val:  [ 80/139]  eta: 0:00:09  loss: 0.6358 (0.6029)  time: 0.1510  data: 0.0002  max mem: 6820\n[05:16:46.257449] val:  [ 90/139]  eta: 0:00:07  loss: 0.7338 (0.6216)  time: 0.1507  data: 0.0002  max mem: 6820\n[05:16:47.771532] val:  [100/139]  eta: 0:00:06  loss: 0.7659 (0.6358)  time: 0.1512  data: 0.0002  max mem: 6820\n[05:16:49.285901] val:  [110/139]  eta: 0:00:04  loss: 0.7514 (0.6436)  time: 0.1513  data: 0.0002  max mem: 6820\n[05:16:50.798009] val:  [120/139]  eta: 0:00:02  loss: 0.6083 (0.6379)  time: 0.1512  data: 0.0002  max mem: 6820\n[05:16:52.309869] val:  [130/139]  eta: 0:00:01  loss: 0.5444 (0.6238)  time: 0.1511  data: 0.0001  max mem: 6820\n[05:16:53.425010] val:  [138/139]  eta: 0:00:00  loss: 0.4234 (0.6133)  time: 0.1464  data: 0.0001  max mem: 6820\n[05:16:53.529424] val: Total time: 0:00:21 (0.1552 s / it)\n[05:16:53.584909] val loss: 0.6133293453737986\n[05:16:53.584989] Accuracy: 0.6582, F1 Score: 0.6561, ROC AUC: 0.7197, Hamming Loss: 0.3418,\n Jaccard Score: 0.4888, Precision: 0.6621, Recall: 0.6582,\n Average Precision: 0.7164, Kappa: 0.3165, Score: 0.5641\n[05:16:56.340789] Best epoch = 49, Best score = 0.5641\n[05:16:56.347566] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[05:16:57.810216] Epoch: [50]  [  0/969]  eta: 0:23:35  lr: 0.000122  loss: 0.7267 (0.7267)  time: 1.4611  data: 0.8351  max mem: 6820\n[05:17:09.102093] Epoch: [50]  [ 20/969]  eta: 0:09:36  lr: 0.000122  loss: 0.6492 (0.6557)  time: 0.5645  data: 0.0002  max mem: 6820\n[05:17:20.591555] Epoch: [50]  [ 40/969]  eta: 0:09:09  lr: 0.000122  loss: 0.6662 (0.6563)  time: 0.5744  data: 0.0002  max mem: 6820\n[05:17:32.164189] Epoch: [50]  [ 60/969]  eta: 0:08:53  lr: 0.000122  loss: 0.6449 (0.6533)  time: 0.5786  data: 0.0002  max mem: 6820\n[05:17:43.680373] Epoch: [50]  [ 80/969]  eta: 0:08:39  lr: 0.000122  loss: 0.6441 (0.6554)  time: 0.5758  data: 0.0002  max mem: 6820\n[05:17:55.124701] Epoch: [50]  [100/969]  eta: 0:08:25  lr: 0.000121  loss: 0.6547 (0.6594)  time: 0.5722  data: 0.0002  max mem: 6820\n[05:18:06.485870] Epoch: [50]  [120/969]  eta: 0:08:12  lr: 0.000121  loss: 0.6794 (0.6635)  time: 0.5680  data: 0.0002  max mem: 6820\n[05:18:17.801774] Epoch: [50]  [140/969]  eta: 0:07:58  lr: 0.000121  loss: 0.6556 (0.6626)  time: 0.5657  data: 0.0001  max mem: 6820\n[05:18:29.143958] Epoch: [50]  [160/969]  eta: 0:07:46  lr: 0.000121  loss: 0.6306 (0.6594)  time: 0.5671  data: 0.0001  max mem: 6820\n[05:18:40.537726] Epoch: [50]  [180/969]  eta: 0:07:34  lr: 0.000121  loss: 0.6292 (0.6592)  time: 0.5696  data: 0.0002  max mem: 6820\n[05:18:51.955501] Epoch: [50]  [200/969]  eta: 0:07:22  lr: 0.000121  loss: 0.6682 (0.6605)  time: 0.5708  data: 0.0002  max mem: 6820\n[05:19:03.420444] Epoch: [50]  [220/969]  eta: 0:07:10  lr: 0.000121  loss: 0.6575 (0.6600)  time: 0.5732  data: 0.0002  max mem: 6820\n[05:19:14.867045] Epoch: [50]  [240/969]  eta: 0:06:58  lr: 0.000120  loss: 0.6400 (0.6586)  time: 0.5723  data: 0.0002  max mem: 6820\n[05:19:26.278928] Epoch: [50]  [260/969]  eta: 0:06:47  lr: 0.000120  loss: 0.6560 (0.6588)  time: 0.5705  data: 0.0002  max mem: 6820\n[05:19:37.708777] Epoch: [50]  [280/969]  eta: 0:06:35  lr: 0.000120  loss: 0.6570 (0.6593)  time: 0.5714  data: 0.0002  max mem: 6820\n[05:19:49.133212] Epoch: [50]  [300/969]  eta: 0:06:23  lr: 0.000120  loss: 0.6760 (0.6603)  time: 0.5712  data: 0.0002  max mem: 6820\n[05:20:00.543016] Epoch: [50]  [320/969]  eta: 0:06:12  lr: 0.000120  loss: 0.6275 (0.6583)  time: 0.5704  data: 0.0002  max mem: 6820\n[05:20:11.939425] Epoch: [50]  [340/969]  eta: 0:06:00  lr: 0.000120  loss: 0.6319 (0.6580)  time: 0.5698  data: 0.0002  max mem: 6820\n[05:20:23.309685] Epoch: [50]  [360/969]  eta: 0:05:49  lr: 0.000120  loss: 0.6568 (0.6584)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:20:34.717044] Epoch: [50]  [380/969]  eta: 0:05:37  lr: 0.000119  loss: 0.6804 (0.6589)  time: 0.5703  data: 0.0002  max mem: 6820\n[05:20:46.065177] Epoch: [50]  [400/969]  eta: 0:05:25  lr: 0.000119  loss: 0.6420 (0.6587)  time: 0.5674  data: 0.0002  max mem: 6820\n[05:20:57.441930] Epoch: [50]  [420/969]  eta: 0:05:14  lr: 0.000119  loss: 0.6723 (0.6593)  time: 0.5688  data: 0.0002  max mem: 6820\n[05:21:08.819731] Epoch: [50]  [440/969]  eta: 0:05:02  lr: 0.000119  loss: 0.6456 (0.6584)  time: 0.5688  data: 0.0002  max mem: 6820\n[05:21:20.180169] Epoch: [50]  [460/969]  eta: 0:04:51  lr: 0.000119  loss: 0.6327 (0.6579)  time: 0.5680  data: 0.0002  max mem: 6820\n[05:21:31.437403] Epoch: [50]  [480/969]  eta: 0:04:39  lr: 0.000119  loss: 0.6602 (0.6578)  time: 0.5628  data: 0.0001  max mem: 6820\n[05:21:42.768784] Epoch: [50]  [500/969]  eta: 0:04:28  lr: 0.000119  loss: 0.6491 (0.6577)  time: 0.5665  data: 0.0001  max mem: 6820\n[05:21:54.114674] Epoch: [50]  [520/969]  eta: 0:04:16  lr: 0.000118  loss: 0.6326 (0.6567)  time: 0.5672  data: 0.0002  max mem: 6820\n[05:22:05.483505] Epoch: [50]  [540/969]  eta: 0:04:05  lr: 0.000118  loss: 0.6453 (0.6562)  time: 0.5684  data: 0.0002  max mem: 6820\n[05:22:16.842955] Epoch: [50]  [560/969]  eta: 0:03:53  lr: 0.000118  loss: 0.6576 (0.6562)  time: 0.5679  data: 0.0002  max mem: 6820\n[05:22:28.229061] Epoch: [50]  [580/969]  eta: 0:03:42  lr: 0.000118  loss: 0.6197 (0.6557)  time: 0.5693  data: 0.0002  max mem: 6820\n[05:22:39.566808] Epoch: [50]  [600/969]  eta: 0:03:30  lr: 0.000118  loss: 0.6645 (0.6562)  time: 0.5668  data: 0.0002  max mem: 6820\n[05:22:50.955858] Epoch: [50]  [620/969]  eta: 0:03:19  lr: 0.000118  loss: 0.6433 (0.6561)  time: 0.5694  data: 0.0002  max mem: 6820\n[05:23:02.320726] Epoch: [50]  [640/969]  eta: 0:03:07  lr: 0.000118  loss: 0.6376 (0.6558)  time: 0.5682  data: 0.0002  max mem: 6820\n[05:23:13.724675] Epoch: [50]  [660/969]  eta: 0:02:56  lr: 0.000117  loss: 0.6806 (0.6563)  time: 0.5701  data: 0.0002  max mem: 6820\n[05:23:25.158481] Epoch: [50]  [680/969]  eta: 0:02:44  lr: 0.000117  loss: 0.6715 (0.6569)  time: 0.5716  data: 0.0002  max mem: 6820\n[05:23:36.568936] Epoch: [50]  [700/969]  eta: 0:02:33  lr: 0.000117  loss: 0.6429 (0.6567)  time: 0.5705  data: 0.0002  max mem: 6820\n[05:23:47.957584] Epoch: [50]  [720/969]  eta: 0:02:22  lr: 0.000117  loss: 0.6204 (0.6562)  time: 0.5694  data: 0.0002  max mem: 6820\n[05:23:59.330372] Epoch: [50]  [740/969]  eta: 0:02:10  lr: 0.000117  loss: 0.6488 (0.6560)  time: 0.5686  data: 0.0002  max mem: 6820\n[05:24:10.693956] Epoch: [50]  [760/969]  eta: 0:01:59  lr: 0.000117  loss: 0.6050 (0.6550)  time: 0.5681  data: 0.0002  max mem: 6820\n[05:24:22.094200] Epoch: [50]  [780/969]  eta: 0:01:47  lr: 0.000117  loss: 0.6685 (0.6553)  time: 0.5700  data: 0.0002  max mem: 6820\n[05:24:33.469126] Epoch: [50]  [800/969]  eta: 0:01:36  lr: 0.000116  loss: 0.6619 (0.6559)  time: 0.5687  data: 0.0002  max mem: 6820\n[05:24:44.836818] Epoch: [50]  [820/969]  eta: 0:01:25  lr: 0.000116  loss: 0.6242 (0.6557)  time: 0.5683  data: 0.0002  max mem: 6820\n[05:24:56.188957] Epoch: [50]  [840/969]  eta: 0:01:13  lr: 0.000116  loss: 0.6425 (0.6557)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:25:07.546199] Epoch: [50]  [860/969]  eta: 0:01:02  lr: 0.000116  loss: 0.6579 (0.6556)  time: 0.5678  data: 0.0002  max mem: 6820\n[05:25:18.917726] Epoch: [50]  [880/969]  eta: 0:00:50  lr: 0.000116  loss: 0.6494 (0.6557)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:25:30.290066] Epoch: [50]  [900/969]  eta: 0:00:39  lr: 0.000116  loss: 0.6470 (0.6561)  time: 0.5686  data: 0.0002  max mem: 6820\n[05:25:41.644085] Epoch: [50]  [920/969]  eta: 0:00:27  lr: 0.000116  loss: 0.6180 (0.6558)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:25:53.023470] Epoch: [50]  [940/969]  eta: 0:00:16  lr: 0.000116  loss: 0.6516 (0.6559)  time: 0.5689  data: 0.0002  max mem: 6820\n[05:26:04.404470] Epoch: [50]  [960/969]  eta: 0:00:05  lr: 0.000115  loss: 0.6470 (0.6560)  time: 0.5690  data: 0.0002  max mem: 6820\n[05:26:08.946585] Epoch: [50]  [968/969]  eta: 0:00:00  lr: 0.000115  loss: 0.6383 (0.6563)  time: 0.5683  data: 0.0001  max mem: 6820\n[05:26:09.071782] Epoch: [50] Total time: 0:09:12 (0.5704 s / it)\n[05:26:09.071900] Averaged stats: lr: 0.000115  loss: 0.6383 (0.6563)\n[05:26:10.031692] val:  [  0/139]  eta: 0:02:12  loss: 0.7270 (0.7270)  time: 0.9551  data: 0.8115  max mem: 6820\n[05:26:11.530344] val:  [ 10/139]  eta: 0:00:28  loss: 0.6748 (0.6173)  time: 0.2230  data: 0.0745  max mem: 6820\n[05:26:13.036844] val:  [ 20/139]  eta: 0:00:22  loss: 0.5640 (0.5845)  time: 0.1502  data: 0.0005  max mem: 6820\n[05:26:14.542816] val:  [ 30/139]  eta: 0:00:19  loss: 0.5324 (0.5758)  time: 0.1505  data: 0.0002  max mem: 6820\n[05:26:16.054744] val:  [ 40/139]  eta: 0:00:16  loss: 0.6224 (0.6053)  time: 0.1508  data: 0.0002  max mem: 6820\n[05:26:17.564952] val:  [ 50/139]  eta: 0:00:14  loss: 0.6705 (0.6160)  time: 0.1510  data: 0.0002  max mem: 6820\n[05:26:19.083656] val:  [ 60/139]  eta: 0:00:12  loss: 0.6253 (0.6162)  time: 0.1514  data: 0.0002  max mem: 6820\n[05:26:20.602159] val:  [ 70/139]  eta: 0:00:11  loss: 0.6192 (0.6174)  time: 0.1518  data: 0.0002  max mem: 6820\n[05:26:22.118523] val:  [ 80/139]  eta: 0:00:09  loss: 0.6192 (0.6232)  time: 0.1517  data: 0.0002  max mem: 6820\n[05:26:23.636588] val:  [ 90/139]  eta: 0:00:07  loss: 0.7211 (0.6380)  time: 0.1516  data: 0.0002  max mem: 6820\n[05:26:25.165410] val:  [100/139]  eta: 0:00:06  loss: 0.7517 (0.6495)  time: 0.1523  data: 0.0002  max mem: 6820\n[05:26:26.688025] val:  [110/139]  eta: 0:00:04  loss: 0.7459 (0.6544)  time: 0.1525  data: 0.0002  max mem: 6820\n[05:26:28.205451] val:  [120/139]  eta: 0:00:03  loss: 0.5840 (0.6454)  time: 0.1519  data: 0.0002  max mem: 6820\n[05:26:29.726817] val:  [130/139]  eta: 0:00:01  loss: 0.5234 (0.6281)  time: 0.1519  data: 0.0001  max mem: 6820\n[05:26:30.846522] val:  [138/139]  eta: 0:00:00  loss: 0.3897 (0.6161)  time: 0.1472  data: 0.0001  max mem: 6820\n[05:26:30.950282] val: Total time: 0:00:21 (0.1574 s / it)\n[05:26:31.006013] val loss: 0.6160579276599473\n[05:26:31.006072] Accuracy: 0.6501, F1 Score: 0.6494, ROC AUC: 0.7141, Hamming Loss: 0.3499,\n Jaccard Score: 0.4810, Precision: 0.6512, Recall: 0.6501,\n Average Precision: 0.7124, Kappa: 0.3002, Score: 0.5546\n[05:26:31.007936] Best epoch = 49, Best score = 0.5641\n[05:26:31.010422] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[05:26:32.376109] Epoch: [51]  [  0/969]  eta: 0:22:01  lr: 0.000115  loss: 0.6472 (0.6472)  time: 1.3642  data: 0.7948  max mem: 6820\n[05:26:43.739483] Epoch: [51]  [ 20/969]  eta: 0:09:35  lr: 0.000115  loss: 0.6388 (0.6343)  time: 0.5681  data: 0.0002  max mem: 6820\n[05:26:55.133968] Epoch: [51]  [ 40/969]  eta: 0:09:06  lr: 0.000115  loss: 0.6634 (0.6481)  time: 0.5697  data: 0.0002  max mem: 6820\n[05:27:06.569232] Epoch: [51]  [ 60/969]  eta: 0:08:49  lr: 0.000115  loss: 0.6610 (0.6508)  time: 0.5717  data: 0.0002  max mem: 6820\n[05:27:17.994381] Epoch: [51]  [ 80/969]  eta: 0:08:35  lr: 0.000115  loss: 0.6273 (0.6479)  time: 0.5712  data: 0.0002  max mem: 6820\n[05:27:29.421814] Epoch: [51]  [100/969]  eta: 0:08:22  lr: 0.000115  loss: 0.6557 (0.6545)  time: 0.5713  data: 0.0002  max mem: 6820\n[05:27:40.848450] Epoch: [51]  [120/969]  eta: 0:08:09  lr: 0.000114  loss: 0.6556 (0.6571)  time: 0.5713  data: 0.0002  max mem: 6820\n[05:27:52.262720] Epoch: [51]  [140/969]  eta: 0:07:57  lr: 0.000114  loss: 0.6602 (0.6591)  time: 0.5707  data: 0.0002  max mem: 6820\n[05:28:03.627748] Epoch: [51]  [160/969]  eta: 0:07:45  lr: 0.000114  loss: 0.6569 (0.6587)  time: 0.5682  data: 0.0002  max mem: 6820\n[05:28:15.021777] Epoch: [51]  [180/969]  eta: 0:07:33  lr: 0.000114  loss: 0.6468 (0.6595)  time: 0.5696  data: 0.0002  max mem: 6820\n[05:28:26.392909] Epoch: [51]  [200/969]  eta: 0:07:21  lr: 0.000114  loss: 0.6655 (0.6615)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:28:37.753441] Epoch: [51]  [220/969]  eta: 0:07:09  lr: 0.000114  loss: 0.6512 (0.6618)  time: 0.5680  data: 0.0002  max mem: 6820\n[05:28:49.126091] Epoch: [51]  [240/969]  eta: 0:06:57  lr: 0.000114  loss: 0.6698 (0.6629)  time: 0.5686  data: 0.0002  max mem: 6820\n[05:29:00.488416] Epoch: [51]  [260/969]  eta: 0:06:46  lr: 0.000114  loss: 0.6561 (0.6623)  time: 0.5681  data: 0.0002  max mem: 6820\n[05:29:11.837417] Epoch: [51]  [280/969]  eta: 0:06:34  lr: 0.000113  loss: 0.6687 (0.6619)  time: 0.5674  data: 0.0002  max mem: 6820\n[05:29:23.191362] Epoch: [51]  [300/969]  eta: 0:06:22  lr: 0.000113  loss: 0.6495 (0.6622)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:29:34.544357] Epoch: [51]  [320/969]  eta: 0:06:11  lr: 0.000113  loss: 0.6398 (0.6611)  time: 0.5676  data: 0.0002  max mem: 6820\n[05:29:45.909446] Epoch: [51]  [340/969]  eta: 0:05:59  lr: 0.000113  loss: 0.6567 (0.6611)  time: 0.5682  data: 0.0002  max mem: 6820\n[05:29:57.315170] Epoch: [51]  [360/969]  eta: 0:05:47  lr: 0.000113  loss: 0.6497 (0.6605)  time: 0.5702  data: 0.0002  max mem: 6820\n[05:30:08.727993] Epoch: [51]  [380/969]  eta: 0:05:36  lr: 0.000113  loss: 0.6680 (0.6608)  time: 0.5706  data: 0.0002  max mem: 6820\n[05:30:20.129788] Epoch: [51]  [400/969]  eta: 0:05:25  lr: 0.000113  loss: 0.6787 (0.6616)  time: 0.5700  data: 0.0002  max mem: 6820\n[05:30:31.540006] Epoch: [51]  [420/969]  eta: 0:05:13  lr: 0.000112  loss: 0.6552 (0.6614)  time: 0.5705  data: 0.0002  max mem: 6820\n[05:30:42.977289] Epoch: [51]  [440/969]  eta: 0:05:02  lr: 0.000112  loss: 0.6439 (0.6611)  time: 0.5718  data: 0.0002  max mem: 6820\n[05:30:54.384563] Epoch: [51]  [460/969]  eta: 0:04:50  lr: 0.000112  loss: 0.6558 (0.6612)  time: 0.5703  data: 0.0002  max mem: 6820\n[05:31:05.785773] Epoch: [51]  [480/969]  eta: 0:04:39  lr: 0.000112  loss: 0.6553 (0.6613)  time: 0.5700  data: 0.0002  max mem: 6820\n[05:31:17.167939] Epoch: [51]  [500/969]  eta: 0:04:27  lr: 0.000112  loss: 0.6510 (0.6618)  time: 0.5690  data: 0.0002  max mem: 6820\n[05:31:28.534787] Epoch: [51]  [520/969]  eta: 0:04:16  lr: 0.000112  loss: 0.6450 (0.6606)  time: 0.5683  data: 0.0002  max mem: 6820\n[05:31:39.912822] Epoch: [51]  [540/969]  eta: 0:04:04  lr: 0.000112  loss: 0.6275 (0.6597)  time: 0.5688  data: 0.0002  max mem: 6820\n[05:31:51.282676] Epoch: [51]  [560/969]  eta: 0:03:53  lr: 0.000111  loss: 0.6151 (0.6594)  time: 0.5684  data: 0.0002  max mem: 6820\n[05:32:02.654075] Epoch: [51]  [580/969]  eta: 0:03:42  lr: 0.000111  loss: 0.6455 (0.6587)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:32:13.999873] Epoch: [51]  [600/969]  eta: 0:03:30  lr: 0.000111  loss: 0.6848 (0.6598)  time: 0.5672  data: 0.0002  max mem: 6820\n[05:32:25.363182] Epoch: [51]  [620/969]  eta: 0:03:19  lr: 0.000111  loss: 0.6671 (0.6605)  time: 0.5681  data: 0.0002  max mem: 6820\n[05:32:36.722579] Epoch: [51]  [640/969]  eta: 0:03:07  lr: 0.000111  loss: 0.6512 (0.6605)  time: 0.5679  data: 0.0002  max mem: 6820\n[05:32:48.090795] Epoch: [51]  [660/969]  eta: 0:02:56  lr: 0.000111  loss: 0.6400 (0.6600)  time: 0.5684  data: 0.0002  max mem: 6820\n[05:32:59.444847] Epoch: [51]  [680/969]  eta: 0:02:44  lr: 0.000111  loss: 0.6798 (0.6606)  time: 0.5677  data: 0.0002  max mem: 6820\n[05:33:10.778937] Epoch: [51]  [700/969]  eta: 0:02:33  lr: 0.000110  loss: 0.6362 (0.6601)  time: 0.5667  data: 0.0002  max mem: 6820\n[05:33:22.107344] Epoch: [51]  [720/969]  eta: 0:02:21  lr: 0.000110  loss: 0.6292 (0.6599)  time: 0.5664  data: 0.0002  max mem: 6820\n[05:33:33.469341] Epoch: [51]  [740/969]  eta: 0:02:10  lr: 0.000110  loss: 0.6485 (0.6597)  time: 0.5681  data: 0.0002  max mem: 6820\n[05:33:44.816117] Epoch: [51]  [760/969]  eta: 0:01:59  lr: 0.000110  loss: 0.6559 (0.6598)  time: 0.5673  data: 0.0002  max mem: 6820\n[05:33:56.160155] Epoch: [51]  [780/969]  eta: 0:01:47  lr: 0.000110  loss: 0.6577 (0.6598)  time: 0.5672  data: 0.0002  max mem: 6820\n[05:34:07.928550] Epoch: [51]  [800/969]  eta: 0:01:36  lr: 0.000110  loss: 0.6522 (0.6594)  time: 0.5884  data: 0.0002  max mem: 6820\n[05:34:19.271797] Epoch: [51]  [820/969]  eta: 0:01:24  lr: 0.000110  loss: 0.6317 (0.6592)  time: 0.5671  data: 0.0002  max mem: 6820\n[05:34:30.647233] Epoch: [51]  [840/969]  eta: 0:01:13  lr: 0.000110  loss: 0.6400 (0.6590)  time: 0.5687  data: 0.0002  max mem: 6820\n[05:34:42.047899] Epoch: [51]  [860/969]  eta: 0:01:02  lr: 0.000109  loss: 0.6567 (0.6590)  time: 0.5700  data: 0.0002  max mem: 6820\n[05:34:53.453630] Epoch: [51]  [880/969]  eta: 0:00:50  lr: 0.000109  loss: 0.6616 (0.6589)  time: 0.5702  data: 0.0002  max mem: 6820\n[05:35:04.885342] Epoch: [51]  [900/969]  eta: 0:00:39  lr: 0.000109  loss: 0.6315 (0.6591)  time: 0.5715  data: 0.0002  max mem: 6820\n[05:35:16.285255] Epoch: [51]  [920/969]  eta: 0:00:27  lr: 0.000109  loss: 0.6200 (0.6583)  time: 0.5699  data: 0.0002  max mem: 6820\n[05:35:27.723989] Epoch: [51]  [940/969]  eta: 0:00:16  lr: 0.000109  loss: 0.6653 (0.6584)  time: 0.5718  data: 0.0002  max mem: 6820\n[05:35:39.162596] Epoch: [51]  [960/969]  eta: 0:00:05  lr: 0.000109  loss: 0.6720 (0.6586)  time: 0.5719  data: 0.0002  max mem: 6820\n[05:35:43.751210] Epoch: [51]  [968/969]  eta: 0:00:00  lr: 0.000109  loss: 0.6343 (0.6586)  time: 0.5722  data: 0.0002  max mem: 6820\n[05:35:43.902390] Epoch: [51] Total time: 0:09:12 (0.5706 s / it)\n[05:35:43.902506] Averaged stats: lr: 0.000109  loss: 0.6343 (0.6586)\n[05:35:44.685722] val:  [  0/139]  eta: 0:01:48  loss: 0.6411 (0.6411)  time: 0.7787  data: 0.6284  max mem: 6820\n[05:35:46.205023] val:  [ 10/139]  eta: 0:00:26  loss: 0.5942 (0.5522)  time: 0.2088  data: 0.0576  max mem: 6820\n[05:35:47.719088] val:  [ 20/139]  eta: 0:00:21  loss: 0.5236 (0.5273)  time: 0.1516  data: 0.0004  max mem: 6820\n[05:35:49.238856] val:  [ 30/139]  eta: 0:00:18  loss: 0.4804 (0.5202)  time: 0.1516  data: 0.0002  max mem: 6820\n[05:35:50.760602] val:  [ 40/139]  eta: 0:00:16  loss: 0.5706 (0.5474)  time: 0.1520  data: 0.0002  max mem: 6820\n[05:35:52.280502] val:  [ 50/139]  eta: 0:00:14  loss: 0.5751 (0.5573)  time: 0.1520  data: 0.0002  max mem: 6820\n[05:35:53.802699] val:  [ 60/139]  eta: 0:00:12  loss: 0.5587 (0.5584)  time: 0.1520  data: 0.0002  max mem: 6820\n[05:35:55.321627] val:  [ 70/139]  eta: 0:00:11  loss: 0.5580 (0.5647)  time: 0.1520  data: 0.0002  max mem: 6820\n[05:35:56.836708] val:  [ 80/139]  eta: 0:00:09  loss: 0.6231 (0.5851)  time: 0.1516  data: 0.0002  max mem: 6820\n[05:35:58.359251] val:  [ 90/139]  eta: 0:00:07  loss: 0.7932 (0.6112)  time: 0.1518  data: 0.0002  max mem: 6820\n[05:35:59.880162] val:  [100/139]  eta: 0:00:06  loss: 0.8113 (0.6306)  time: 0.1521  data: 0.0002  max mem: 6820\n[05:36:01.404295] val:  [110/139]  eta: 0:00:04  loss: 0.7998 (0.6422)  time: 0.1522  data: 0.0002  max mem: 6820\n[05:36:02.927940] val:  [120/139]  eta: 0:00:02  loss: 0.6242 (0.6380)  time: 0.1523  data: 0.0002  max mem: 6820\n[05:36:04.450882] val:  [130/139]  eta: 0:00:01  loss: 0.5512 (0.6250)  time: 0.1522  data: 0.0002  max mem: 6820\n[05:36:05.571480] val:  [138/139]  eta: 0:00:00  loss: 0.4533 (0.6163)  time: 0.1473  data: 0.0002  max mem: 6820\n[05:36:05.676168] val: Total time: 0:00:21 (0.1566 s / it)\n[05:36:05.732667] val loss: 0.6162876636861897\n[05:36:05.732738] Accuracy: 0.6501, F1 Score: 0.6460, ROC AUC: 0.7181, Hamming Loss: 0.3499,\n Jaccard Score: 0.4783, Precision: 0.6574, Recall: 0.6501,\n Average Precision: 0.7147, Kappa: 0.3002, Score: 0.5548\n[05:36:05.734766] Best epoch = 49, Best score = 0.5641\n[05:36:05.737481] log_dir: ./output_logs/\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[05:36:07.115554] Epoch: [52]  [  0/969]  eta: 0:22:14  lr: 0.000109  loss: 0.5810 (0.5810)  time: 1.3768  data: 0.7997  max mem: 6820\n[05:36:18.501550] Epoch: [52]  [ 20/969]  eta: 0:09:36  lr: 0.000108  loss: 0.6490 (0.6481)  time: 0.5693  data: 0.0002  max mem: 6820\n[05:36:29.898856] Epoch: [52]  [ 40/969]  eta: 0:09:07  lr: 0.000108  loss: 0.6407 (0.6618)  time: 0.5698  data: 0.0002  max mem: 6820\n[05:36:41.312871] Epoch: [52]  [ 60/969]  eta: 0:08:50  lr: 0.000108  loss: 0.6587 (0.6619)  time: 0.5706  data: 0.0002  max mem: 6820\n[05:36:52.732143] Epoch: [52]  [ 80/969]  eta: 0:08:35  lr: 0.000108  loss: 0.6577 (0.6612)  time: 0.5709  data: 0.0002  max mem: 6820\n[05:37:04.152103] Epoch: [52]  [100/969]  eta: 0:08:22  lr: 0.000108  loss: 0.6551 (0.6592)  time: 0.5709  data: 0.0002  max mem: 6820\n[05:37:15.557670] Epoch: [52]  [120/969]  eta: 0:08:09  lr: 0.000108  loss: 0.6593 (0.6604)  time: 0.5702  data: 0.0002  max mem: 6820\n[05:37:26.928016] Epoch: [52]  [140/969]  eta: 0:07:57  lr: 0.000108  loss: 0.6676 (0.6611)  time: 0.5685  data: 0.0002  max mem: 6820\n[05:37:38.306531] Epoch: [52]  [160/969]  eta: 0:07:45  lr: 0.000108  loss: 0.6495 (0.6621)  time: 0.5689  data: 0.0002  max mem: 6820\n[05:37:49.685111] Epoch: [52]  [180/969]  eta: 0:07:33  lr: 0.000107  loss: 0.6459 (0.6605)  time: 0.5689  data: 0.0002  max mem: 6820\n[05:38:01.064624] Epoch: [52]  [200/969]  eta: 0:07:21  lr: 0.000107  loss: 0.6496 (0.6606)  time: 0.5689  data: 0.0002  max mem: 6820\n[05:38:12.425324] Epoch: [52]  [220/969]  eta: 0:07:09  lr: 0.000107  loss: 0.6407 (0.6605)  time: 0.5680  data: 0.0002  max mem: 6820\n[05:38:23.786291] Epoch: [52]  [240/969]  eta: 0:06:57  lr: 0.000107  loss: 0.6543 (0.6605)  time: 0.5680  data: 0.0002  max mem: 6820\n[05:38:35.131479] Epoch: [52]  [260/969]  eta: 0:06:45  lr: 0.000107  loss: 0.6632 (0.6597)  time: 0.5672  data: 0.0002  max mem: 6820\n[05:38:46.501339] Epoch: [52]  [280/969]  eta: 0:06:34  lr: 0.000107  loss: 0.6812 (0.6623)  time: 0.5684  data: 0.0002  max mem: 6820\n[05:38:57.843496] Epoch: [52]  [300/969]  eta: 0:06:22  lr: 0.000107  loss: 0.6665 (0.6627)  time: 0.5671  data: 0.0002  max mem: 6820\n[05:39:09.173436] Epoch: [52]  [320/969]  eta: 0:06:10  lr: 0.000106  loss: 0.6441 (0.6627)  time: 0.5664  data: 0.0002  max mem: 6820\n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/RETFound_MAE/main_finetune.py\", line 405, in <module>\n    main(args, criterion)\n  File \"/kaggle/working/RETFound_MAE/main_finetune.py\", line 351, in main\n    train_stats = train_one_epoch(\n                  ^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/RETFound_MAE/engine_finetune.py\", line 61, in train_one_epoch\n    torch.cuda.synchronize()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\", line 1040, in synchronize\n    return torch._C._cuda_synchronize()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":37}]}