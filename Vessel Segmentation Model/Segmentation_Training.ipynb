{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1PaQbuTg7C9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jInWT3Ptg1C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FundusVesselDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.filenames = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.filenames[idx])\n",
        "        label_path = os.path.join(self.label_dir, self.filenames[idx])\n",
        "\n",
        "        image = cv2.imread(img_path).astype(np.float32)\n",
        "        label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "\n",
        "        # Z-score normalize\n",
        "        mean, std = image.mean(), image.std()\n",
        "        image = (image - mean) / (std + 1e-8)\n",
        "\n",
        "        image = np.transpose(image, (2, 0, 1))  # HWC → CHW\n",
        "        label = label / 255.0  # binarize [0, 1]\n",
        "        label = np.expand_dims(label, axis=0)\n",
        "\n",
        "        return torch.tensor(image), torch.tensor(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTQL3wrvPJEr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# === Paths ===\n",
        "train_image_dir = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Preprocessed_All_Train_Images\"\n",
        "train_label_dir = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Preprocessed_All_Train_Labels\"\n",
        "\n",
        "# === Dataset + DataLoader ===\n",
        "train_dataset = FundusVesselDataset(train_image_dir, train_label_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3x-q4I2weL7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# === DropBlock2D ===\n",
        "class DropBlock2D(nn.Module):\n",
        "    def __init__(self, block_size, drop_prob):\n",
        "        super(DropBlock2D, self).__init__()\n",
        "        self.block_size = block_size\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.drop_prob == 0.0:\n",
        "            return x\n",
        "\n",
        "        gamma = self._compute_gamma(x)\n",
        "        B, C, H, W = x.shape\n",
        "        mask = (torch.rand(B, 1, H, W, device=x.device) < gamma).float()\n",
        "        block_mask = F.max_pool2d(mask, self.block_size, stride=1, padding=self.block_size // 2)\n",
        "        block_mask = 1 - block_mask\n",
        "        out = x * block_mask\n",
        "        out = out * (block_mask.numel() / block_mask.sum().clamp(min=1.0))\n",
        "        return out\n",
        "\n",
        "    def _compute_gamma(self, x):\n",
        "        _, _, h, w = x.size()\n",
        "        return self.drop_prob / (self.block_size ** 2) * (h * w) / ((h - self.block_size + 1) * (w - self.block_size + 1))\n",
        "\n",
        "# === Res2Block ===\n",
        "class Res2Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, scale=4):\n",
        "        super(Res2Block, self).__init__()\n",
        "        self.scale = scale\n",
        "        width = out_channels // scale\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(width, width, 3, padding=1, bias=False) for _ in range(scale - 1)\n",
        "        ])\n",
        "        self.bns = nn.ModuleList([\n",
        "            nn.BatchNorm2d(width) for _ in range(scale - 1)\n",
        "        ])\n",
        "\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        spx = list(torch.chunk(out, self.scale, 1))\n",
        "        for i in range(1, self.scale):\n",
        "            spx[i] = self.relu(self.bns[i - 1](self.convs[i - 1](spx[i] + spx[i - 1])))\n",
        "        out = torch.cat(spx, 1)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        return self.relu(out)\n",
        "\n",
        "# === Spatial Attention ===\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        A = torch.sum(x, dim=1, keepdim=True)\n",
        "        flat = A.view(A.size(0), -1)\n",
        "        softmax = F.softmax(flat, dim=-1)\n",
        "        P = softmax.view_as(A)\n",
        "        return x * P\n",
        "\n",
        "# === PAM ===\n",
        "class PAM(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(PAM, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        proj_query = self.query_conv(x).view(B, -1, H * W).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(B, -1, H * W)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "        proj_value = self.value_conv(x).view(B, -1, H * W)\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(B, C, H, W)\n",
        "        return self.gamma * out + x\n",
        "\n",
        "# === CAM ===\n",
        "class CAM(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(CAM, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        proj_query = x.view(B, C, -1)\n",
        "        proj_key = x.view(B, C, -1).permute(0, 2, 1)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "        proj_value = x.view(B, C, -1)\n",
        "        out = torch.bmm(attention, proj_value).view(B, C, H, W)\n",
        "        return self.gamma * out + x\n",
        "\n",
        "# === DA-Res2UNet ===\n",
        "class DARes2UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DARes2UNet, self).__init__()\n",
        "        self.enc1 = nn.Sequential(Res2Block(3, 16), DropBlock2D(7, 0.1))\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc2 = nn.Sequential(Res2Block(16, 32), DropBlock2D(7, 0.1))\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc3 = nn.Sequential(Res2Block(32, 64), DropBlock2D(7, 0.1))\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc4 = nn.Sequential(Res2Block(64, 128), DropBlock2D(7, 0.1))\n",
        "\n",
        "        self.pam = PAM(128)\n",
        "        self.cam = CAM(128)\n",
        "        self.sa = SpatialAttention()\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec3 = Res2Block(128, 64)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
        "        self.dec2 = Res2Block(64, 32)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
        "        self.dec1 = Res2Block(32, 16)\n",
        "\n",
        "        self.final = nn.Conv2d(16, 1, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        e3 = self.enc3(self.pool2(e2))\n",
        "        e4 = self.enc4(self.pool3(e3))\n",
        "\n",
        "        att = self.sa(self.pam(self.cam(e4)))\n",
        "\n",
        "        d3 = self.up3(att)\n",
        "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
        "\n",
        "        return self.sigmoid(self.final(d1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjza6pzQthVD"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# === Initialize model, optimizer, scheduler ===\n",
        "model = DARes2UNet()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=40)\n",
        "\n",
        "# === Load checkpoint (handle both old and new format) ===\n",
        "#checkpoint_path = \"/content/drive/MyDrive/EECE 490 Project/SegTraining3/best_model.pth\"\n",
        "#checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "#if 'model_state_dict' in checkpoint:  # New format (full checkpoint)\n",
        "#    print(\"🔄 Loading model, optimizer, scheduler states...\")\n",
        "#    start_epoch = checkpoint.get('epoch', 0)\n",
        "#    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "#else:  # Old format (model weights only)\n",
        "#    print(\"🔄 Loading model weights only...\")\n",
        "#    model.load_state_dict(checkpoint)\n",
        "\n",
        "#model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvgj2Zogtlu-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# === Ensure Save Directory Exists ===\n",
        "log_dir = \"/content/drive/MyDrive/EECE 490 Project/SegTraining\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_path = os.path.join(log_dir, \"training_log.txt\")\n",
        "model_path = os.path.join(log_dir, \"best_model.pth\")\n",
        "\n",
        "# === Setup ===\n",
        "EPOCHS = 40\n",
        "best_loss = float('inf')\n",
        "\n",
        "def dice_coef(preds, targets, threshold=0.5, eps=1e-6):\n",
        "    preds = (preds > threshold).float()\n",
        "    intersection = (preds * targets).sum()\n",
        "    return (2. * intersection + eps) / (preds.sum() + targets.sum() + eps)\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    dice_total = 0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        dice_total += dice_coef(outputs, masks).item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_dice = dice_total / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # === Print ===\n",
        "    print(\"=\"*60)\n",
        "    print(f\"🧠 EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"📉 Loss: {epoch_loss:.4f} | 🎯 Dice: {avg_dice:.4f} | ⏱ Time: {epoch_time:.2f}s\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # === Save metrics to log file ===\n",
        "    with open(log_path, \"a\") as f:\n",
        "        f.write(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss:.4f}, Dice: {avg_dice:.4f}\\n\")\n",
        "\n",
        "    # === Save best model checkpoint ===\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(\"✅ Best model saved to Google Drive!\")\n",
        "\n",
        "    # === Save snapshot every 5 epochs ===\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            img, mask = next(iter(train_loader))\n",
        "            pred = model(img.to(device)).cpu().squeeze().numpy()\n",
        "\n",
        "            pred_img = (pred * 255).astype(np.uint8)\n",
        "            snapshot_path = os.path.join(log_dir, f\"pred_epoch_{epoch+1}.png\")\n",
        "            cv2.imwrite(snapshot_path, pred_img)\n",
        "            print(f\"📸 Snapshot saved: pred_epoch_{epoch+1}.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWdcBQ5pYxrf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# === Ensure Save Directory Exists ===\n",
        "log_dir = \"/content/drive/MyDrive/EECE 490 Project/SegTraining2\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_path = os.path.join(log_dir, \"training_log.txt\")\n",
        "model_path = os.path.join(log_dir, \"best_model.pth\")\n",
        "\n",
        "# === Setup ===\n",
        "EPOCHS = 100\n",
        "best_loss = 28.4986  # Resume from last best val loss\n",
        "\n",
        "def dice_coef(preds, targets, threshold=0.5, eps=1e-6):\n",
        "    preds = (preds > threshold).float()\n",
        "    intersection = (preds * targets).sum()\n",
        "    return (2. * intersection + eps) / (preds.sum() + targets.sum() + eps)\n",
        "\n",
        "# === Training Loop ===\n",
        "start_epoch = 37\n",
        "for _ in range(start_epoch):\n",
        "    scheduler.step()\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    dice_total = 0\n",
        "\n",
        "    # === TRAINING LOOP ===\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        dice_total += dice_coef(outputs, masks).item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    avg_train_dice = dice_total / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "\n",
        "    # === PRINT METRICS ===\n",
        "    print(\"=\"*60)\n",
        "    print(f\"🧠 EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"📉 Train Loss: {avg_train_loss:.4f} | 🎯 Train Dice: {avg_train_dice:.4f}\")\n",
        "    print(f\"⏱ Time: {epoch_time:.2f}s\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # === SAVE LOG ===\n",
        "    with open(log_path, \"a\") as f:\n",
        "        f.write(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f}, Train Dice: {avg_train_dice:.4f}\")\n",
        "\n",
        "    # === SAVE BEST MODEL ===\n",
        "    if avg_train_loss < best_loss:\n",
        "        best_loss = avg_train_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "        }, model_path)\n",
        "        print(\"✅ Best model + optimizer + scheduler saved to Google Drive!\")\n",
        "\n",
        "    # === SNAPSHOT EVERY 5 EPOCHS ===\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            img, mask = next(iter(train_loader))\n",
        "            pred = model(img.to(device)).cpu().numpy()\n",
        "            pred_img = pred[0, 0]  # First image, first channel (H x W)\n",
        "            pred_img = (pred_img * 255).astype(np.uint8)\n",
        "\n",
        "            snapshot_path = os.path.join(log_dir, f\"pred_epoch_{epoch+1}.png\")\n",
        "            cv2.imwrite(snapshot_path, pred_img)\n",
        "            print(f\"📸 Snapshot saved: pred_epoch_{epoch+1}.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJKozZS0K1Pq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# === Ensure Save Directory Exists ===\n",
        "log_dir = \"/content/drive/MyDrive/EECE 490 Project/SegTraining3\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_path = os.path.join(log_dir, \"training_log.txt\")\n",
        "model_path = os.path.join(log_dir, \"best_model.pth\")\n",
        "\n",
        "# === Setup ===\n",
        "EPOCHS = 100\n",
        "best_loss = float('inf')\n",
        "\n",
        "def dice_coef(preds, targets, threshold=0.5, eps=1e-6):\n",
        "    preds = (preds > threshold).float()\n",
        "    intersection = (preds * targets).sum()\n",
        "    return (2. * intersection + eps) / (preds.sum() + targets.sum() + eps)\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    dice_total = 0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        dice_total += dice_coef(outputs, masks).item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_dice = dice_total / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # === Print ===\n",
        "    print(\"=\"*60)\n",
        "    print(f\"🧠 EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"📉 Loss: {epoch_loss:.4f} | 🎯 Dice: {avg_dice:.4f} | ⏱ Time: {epoch_time:.2f}s\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # === Save metrics to log file ===\n",
        "    with open(log_path, \"a\") as f:\n",
        "        f.write(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss:.4f}, Dice: {avg_dice:.4f}\\n\")\n",
        "\n",
        "    # === Save best model checkpoint ===\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(\"✅ Best model saved to Google Drive!\")\n",
        "\n",
        "    # === Save snapshot every 5 epochs ===\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            img, mask = next(iter(train_loader))\n",
        "            pred = model(img.to(device)).cpu().numpy()\n",
        "\n",
        "            # Save only the first image in the batch\n",
        "            pred_img = pred[0, 0]  # shape: [256, 256]\n",
        "            pred_img = (pred_img * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "            snapshot_path = os.path.join(log_dir, f\"pred_epoch_{epoch+1}.png\")\n",
        "            cv2.imwrite(snapshot_path, pred_img)\n",
        "            print(f\"📸 Snapshot saved: pred_epoch_{epoch+1}.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Reload model for inference ===\n",
        "model = DARes2UNet()\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/EECE 490 Project/SegTraining3/best_model.pth\"))\n",
        "model = model.to(device)\n",
        "model.eval()  # Set to evaluation mode\n"
      ],
      "metadata": {
        "id": "qHrdfbmFPZ1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Paths ===\n",
        "test_image_dir = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Preprocessed_All_Test_Images\"\n",
        "test_label_dir = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Preprocessed_All_Test_Labels\"\n",
        "\n",
        "# === Dataset + DataLoader ===\n",
        "test_dataset = FundusVesselDataset(test_image_dir, test_label_dir)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "YwDyXmzHPaVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# === Inference Save Directory ===\n",
        "save_dir = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Test_Predictions\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# === Inference Loop ===\n",
        "for idx, (img, mask) in enumerate(test_loader):\n",
        "    img = img.to(device)\n",
        "\n",
        "    # 🔥 Get exact original filename 🔥\n",
        "    filename = test_loader.dataset.filenames[idx]  # EXACT input image name\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(img)\n",
        "        pred = pred.cpu().numpy()[0, 0]  # [batch, channel, H, W] → [H, W]\n",
        "\n",
        "    # === Save prediction ===\n",
        "    pred_img = (pred * 255).clip(0, 255).astype(np.uint8)\n",
        "    save_path = os.path.join(save_dir, filename)  # SAME name, SAME extension!\n",
        "    cv2.imwrite(save_path, pred_img)\n",
        "\n",
        "    # Optional: Show first image\n",
        "    if idx == 0:\n",
        "        plt.figure(figsize=(12,4))\n",
        "        plt.subplot(1,3,1); plt.imshow(img.cpu().numpy()[0].transpose(1,2,0)); plt.title(\"Input Image\")\n",
        "        plt.subplot(1,3,2); plt.imshow(mask.cpu().numpy()[0,0], cmap='gray'); plt.title(\"Ground Truth\")\n",
        "        plt.subplot(1,3,3); plt.imshow(pred, cmap='gray'); plt.title(\"Prediction\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "rVVDg9VzPkM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# === Directories ===\n",
        "pred_dir = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Test_Predictions\"\n",
        "gt_dir   = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Preprocessed_All_Test_Labels\"\n",
        "\n",
        "# === Initialize metrics ===\n",
        "dice_scores = []\n",
        "accuracies  = []\n",
        "sensitivities = []\n",
        "specificities = []\n",
        "\n",
        "# === Metric Calculation Function ===\n",
        "def compute_metrics(pred, gt):\n",
        "    pred = (pred > 0.5).astype(np.uint8)\n",
        "    gt   = (gt > 0.5).astype(np.uint8)\n",
        "\n",
        "    TP = np.sum((pred == 1) & (gt == 1))\n",
        "    TN = np.sum((pred == 0) & (gt == 0))\n",
        "    FP = np.sum((pred == 1) & (gt == 0))\n",
        "    FN = np.sum((pred == 0) & (gt == 1))\n",
        "\n",
        "    dice = (2 * TP) / (2 * TP + FP + FN + 1e-8)\n",
        "    acc  = (TP + TN) / (TP + TN + FP + FN + 1e-8)\n",
        "    sen  = TP / (TP + FN + 1e-8)\n",
        "    spe  = TN / (TN + FP + 1e-8)\n",
        "\n",
        "    return dice, acc, sen, spe\n",
        "\n",
        "# === Loop over predictions ===\n",
        "for filename in os.listdir(pred_dir):\n",
        "    pred_path = os.path.join(pred_dir, filename)\n",
        "    gt_path   = os.path.join(gt_dir, filename)\n",
        "\n",
        "    # === Ensure both files exist ===\n",
        "    if not os.path.exists(gt_path):\n",
        "        print(f\"🚨 Ground truth missing for: {filename}\")\n",
        "        continue\n",
        "\n",
        "    # === Load images ===\n",
        "    pred = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
        "    gt   = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # === Skip unreadable images ===\n",
        "    if pred is None or gt is None:\n",
        "        print(f\"🚨 Problem loading {filename}\")\n",
        "        continue\n",
        "\n",
        "    # === Normalize to [0, 1] ===\n",
        "    pred = pred / 255.0\n",
        "    gt   = gt / 255.0\n",
        "\n",
        "    # === Compute metrics ===\n",
        "    dice, acc, sen, spe = compute_metrics(pred, gt)\n",
        "    dice_scores.append(dice)\n",
        "    accuracies.append(acc)\n",
        "    sensitivities.append(sen)\n",
        "    specificities.append(spe)\n",
        "\n",
        "# === Report average metrics ===\n",
        "print(f\"🎯 Average Dice Coefficient: {np.mean(dice_scores):.4f}\")\n",
        "print(f\"✅ Average Accuracy: {np.mean(accuracies):.4f}\")\n",
        "print(f\"🔥 Average Sensitivity (Recall): {np.mean(sensitivities):.4f}\")\n",
        "print(f\"🛡️ Average Specificity: {np.mean(specificities):.4f}\")\n"
      ],
      "metadata": {
        "id": "PGGgIG9qWmVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# === Directories ===\n",
        "test_image_dir = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Preprocessed_All_Test_Images\"\n",
        "test_label_dir = \"/content/drive/MyDrive/EECE 490 Project/Segmentation Set/Preprocessed_All_Test_Labels\"\n",
        "\n",
        "# === Filter valid image files ===\n",
        "image_extensions = {\".png\", \".jpg\", \".jpeg\", \".tif\"}\n",
        "\n",
        "image_files = sorted([\n",
        "    f for f in os.listdir(test_image_dir)\n",
        "    if os.path.splitext(f)[1].lower() in image_extensions\n",
        "])\n",
        "\n",
        "label_files = sorted([\n",
        "    f for f in os.listdir(test_label_dir)\n",
        "    if os.path.splitext(f)[1].lower() in image_extensions\n",
        "])\n",
        "\n",
        "# === Counts ===\n",
        "print(f\"🖼️ Total Test Images: {len(image_files)}\")\n",
        "print(f\"🎯 Total Test Labels: {len(label_files)}\")\n",
        "\n",
        "# === Check for mismatches ===\n",
        "missing_labels = set(image_files) - set(label_files)\n",
        "missing_images = set(label_files) - set(image_files)\n",
        "\n",
        "if not missing_labels and not missing_images:\n",
        "    print(\"✅ All image and label filenames MATCH perfectly!\")\n",
        "else:\n",
        "    if missing_labels:\n",
        "        print(f\"🚨 Images without labels: {missing_labels}\")\n",
        "    if missing_images:\n",
        "        print(f\"🚨 Labels without images: {missing_images}\")\n"
      ],
      "metadata": {
        "id": "ZU_7noEKXjTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load your custom image ===\n",
        "img_path = \"/content/drive/MyDrive/EECE 490 Project/Generated_Images_DR/seed0002.png\"\n",
        "image = cv2.imread(img_path).astype(np.float32)\n",
        "\n",
        "# === Preprocess (match training) ===\n",
        "mean, std = image.mean(), image.std()\n",
        "image = (image - mean) / (std + 1e-8)\n",
        "image = np.transpose(image, (2, 0, 1))  # HWC → CHW\n",
        "image = torch.tensor(image).unsqueeze(0).to(device)  # Add batch dim\n",
        "\n",
        "# === Inference ===\n",
        "with torch.no_grad():\n",
        "    pred = model(image)\n",
        "    pred = pred.cpu().numpy()[0, 0]  # Remove batch & channel dims\n",
        "\n",
        "# === Save or display prediction ===\n",
        "pred_img = (pred * 255).clip(0, 255).astype(np.uint8)\n",
        "cv2.imwrite(\"/content/drive/MyDrive/pred_custom.png\", pred_img)\n"
      ],
      "metadata": {
        "id": "7fJuRGbOseD_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}