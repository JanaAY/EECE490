{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9UKygqnGYJy",
        "outputId": "5b8a6c01-250f-48d8-eebe-019bb8b4ec9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/EECE 490 Project/Classification_Set/augmented_final_split_fully_balanced/train\"\n",
        "val_dir   = \"/content/drive/MyDrive/EECE 490 Project/Classification_Set/augmented_final_split_fully_balanced/val\"\n",
        "test_dir  = \"/content/drive/MyDrive/EECE 490 Project/Classification_Set/augmented_final_split_fully_balanced/test\"\n",
        "\n",
        "img_size = (200, 200)\n",
        "batch_size = 32\n",
        "\n",
        "# No augmentation here, images are already augmented\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "test_gen = datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WkolVB0WH3L",
        "outputId": "459980fa-5a4d-49e1-9124-79f6e77f7ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 35000 images belonging to 5 classes.\n",
            "Found 5000 images belonging to 5 classes.\n",
            "Found 10000 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "def build_advanced_rsg_net(input_shape=(200, 200, 3), num_classes=5):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # === RSG Block 1 ===\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # === RSG Block 2 ===\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # === Global Average Pooling + Dense Layers ===\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # === Output Layer ===\n",
        "    if num_classes == 2:\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "    else:\n",
        "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "LQKAp1pjUHwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "\n",
        "# === ENSURE CHECKPOINT DIR EXISTS\n",
        "os.makedirs(\"/content/drive/MyDrive/EECE 490 Project/rsgnet_checkpoints\", exist_ok=True)\n",
        "\n",
        "# === OUTPUT PATHS\n",
        "checkpoint_path = \"/content/drive/MyDrive/EECE 490 Project/rsgnet_checkpoints/best_model.h5\"\n",
        "log_path = \"/content/drive/MyDrive/EECE 490 Project/rsgnet_checkpoints/training_log.csv\"\n",
        "\n",
        "# === CALLBACKS\n",
        "callbacks = [\n",
        "    ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),\n",
        "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1),\n",
        "    CSVLogger(log_path)\n",
        "]\n",
        "\n",
        "# === BUILD + COMPILE\n",
        "model = build_advanced_rsg_net()\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# === TRAIN\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=30,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylribQeBWLnU",
        "outputId": "0afbbc0a-1541-4217-8ccc-a21d1f6ffa46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        }
      ]
    }
  ]
}